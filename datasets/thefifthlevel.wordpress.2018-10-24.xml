<?xml version="1.0" encoding="UTF-8" ?>
<!-- This is a WordPress eXtended RSS file generated by WordPress as an export of your site. -->
<!-- It contains information about your site's posts, pages, comments, categories, and other content. -->
<!-- You may use this file to transfer that content from one site to another. -->
<!-- This file is not intended to serve as a complete backup of your site. -->

<!-- To import this information into a WordPress site follow these steps: -->
<!-- 1. Log in to that site as an administrator. -->
<!-- 2. Go to Tools: Import in the WordPress admin panel. -->
<!-- 3. Install the "WordPress" importer from the list. -->
<!-- 4. Activate & Run Importer. -->
<!-- 5. Upload this file using the form provided on that page. -->
<!-- 6. You will first be asked to map the authors in this export file to users -->
<!--    on the site. For each author, you may choose to map to an -->
<!--    existing user on the site or to create a new user. -->
<!-- 7. WordPress will then import each of the posts, pages, comments, categories, etc. -->
<!--    contained in this file into your site. -->


<rss version="2.0"
	xmlns:excerpt="http://wordpress.org/export/1.2/excerpt/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:wp="http://wordpress.org/export/1.2/"
>

<channel>
	<title>The Fifth Level</title>
	<link>https://fifthlevel.ai</link>
	<description>Steering wheel optional</description>
	<pubDate>Wed, 24 Oct 2018 06:59:03 +0000</pubDate>
	<language>en-US</language>
	<wp:wxr_version>1.2</wp:wxr_version>
	<wp:base_site_url>https://fifthlevel.ai</wp:base_site_url>
	<wp:base_blog_url>https://fifthlevel.ai</wp:base_blog_url>

	<wp:author><wp:author_id>2</wp:author_id><wp:author_login><![CDATA[editor]]></wp:author_login><wp:author_email><![CDATA[sriharisrinivasan@gmail.com]]></wp:author_email><wp:author_display_name><![CDATA[editor]]></wp:author_display_name><wp:author_first_name><![CDATA[]]></wp:author_first_name><wp:author_last_name><![CDATA[]]></wp:author_last_name></wp:author>

	<wp:category>
		<wp:term_id>1412</wp:term_id>
		<wp:category_nicename><![CDATA[autonomous-vehicles]]></wp:category_nicename>
		<wp:category_parent><![CDATA[]]></wp:category_parent>
		<wp:cat_name><![CDATA[Autonomous Vehicles]]></wp:cat_name>
	</wp:category>

	

<image>
	<url>https://fifthlevel.ai/wp-content/uploads/2018/10/cropped-wheel-1596877_640-3-32x32.png</url>
	<title>The Fifth Level</title>
	<link>https://fifthlevel.ai</link>
	<width>32</width>
	<height>32</height>
</image> 
<site xmlns="com-wordpress:feed-additions:1">152311745</site>
	<item>
		<title>Hello, Pittsburgh!</title>
		<link>https://fifthlevel.ai/archives/441</link>
		<pubDate>Thu, 23 Feb 2017 14:02:02 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/e7c11ab64b12</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Pittsburgh has been called many things — The Steel City, City of Champions, City of Bridges, The Capital of Work. A newer name, one we’ve become particularly fond of, is Roboburgh. Yet, no matter what you call it, Pittsburgh has always been an epicenter for innovation, with an incredible workforce possessing the talent, grit and determination to take an idea and turn it into reality.</p><p>For decades, Pittsburgh supplied the vast majority of our nation’s steel — an industry that grew from 380,000 tons annually in 1875 to 60 million tons by 1920. In building this empire, Andrew Carnegie asserted an innate ability to identify a new technology and make it work in the real world — achieving mass production at an unprecedented scale. And the enterprising newcomers who arrived to this area looking for good-paying jobs played an integral role in creating that success.</p><p>People in Pittsburgh have always known how to build things, things that helped to build a nation — bridges, railroads, highways, buildings. The very infrastructure that enables us to move around, which we tend to take for granted, would not be possible if it weren’t for the tremendous efforts, vision and talent the people of Pittsburgh have always brought to their work.</p><p>When de-industrialization hit cities across the United States and the steel industry went into decline, Pittsburgh had to go in search of a new identity. Fortunately, the seeds of innovation from early philanthropic efforts meant it didn’t have to go far. Founded in 1900, Carnegie Technical Schools is now Carnegie Mellon University, home to the world-class Robotics Institute. This region now boasts more than 68 institutions of higher learning, including the University of Pittsburgh and its renowned medical center.</p><p>Western Pennsylvania recruits talent from all over the world, as the education community here provides a haven for entrepreneurs to test out new ideas in the hope one might someday change the world. Pittsburgh is the place to grow ideas into working prototypes and proofs of concept. Once again, what’s driving this innovation is people. Our ecosystem supports an established base of talent and a soaring reputation that continues to attract new talent. People come here and want to stay, underpinning our economic development in the process.</p><p>Just as Carnegie’s steel empire transformed a country’s infrastructure, a new generation in Pittsburgh is working to transform traditional industries such as transportation, automotive, medicine and other fields. We are software engineers, electrical engineers, mechanical engineers, data scientists, computer scientists, roboticists — all working with our hands and with the knowledge necessary to apply new science, like artificial intelligence, to solve really hard problems, particularly in the realm of mobility.</p><p>I came to Pittsburgh 20 years ago to attend the University of Pittsburgh and fell in love with this place. So when we founded Argo AI, we knew our headquarters would be based here, and we’re embracing this new generation of engineers and scientists who are both discovering the city’s energy and fueling it. Retaining this pool of top-notch talent is vitally important as we collaborate with world-class universities to ensure our great ideas and innovations will support our mission to provide affordable mobility for all.</p><p>I’m excited to announce the headquarters for Argo AI will be based in Pittsburgh’s Strip District. We’re quickly growing our team and expect to be moving into our new engineering center in the coming month. We are also <a href="https://www.argo.ai/careers/">aggressively recruiting</a> for teams to be based in Southeast Michigan and the Bay Area of California and we look forward to embracing those communities as well.</p><p>Just as Pittsburgh, and this neighborhood in particular, played a significant role in rolling out the infrastructure of this country, I expect it will again play a major role in the automation of the automotive industry and beyond.</p><p>Pittsburgh has been voted the most livable city six times since 2000, and it’s no secret why. Living, working and playing here is incredible. We’re energized by the passion and commitment of this community — especially that of city officials and the current administration — and we’re truly excited for what we will build together.</p><p>Bryan Salesky,</p><p>CEO, Argo AI</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e7c11ab64b12" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/hello-pittsburgh-e7c11ab64b12">Hello, Pittsburgh!</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/hello-pittsburgh-e7c11ab64b12?source=rss-ba51b2aa6fa1------2" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>441</wp:post_id>
		<wp:post_date><![CDATA[2017-02-23 14:02:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-02-23 14:02:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[hello-pittsburgh]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@ArgoAI]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/hello-pittsburgh-e7c11ab64b12?source=rss-ba51b2aa6fa1------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[799]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Why We Created Argo AI</title>
		<link>https://fifthlevel.ai/archives/442</link>
		<pubDate>Fri, 10 Feb 2017 19:30:02 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/aa3f43ebefb6</guid>
		<description></description>
		<content:encoded><![CDATA[<p>We founded Argo AI to tackle one of the most challenging applications in computer science, robotics and artificial intelligence — self-driving vehicles. While technology exists today to augment the human driver and automate the driving task up to a certain level of capability, replacing the human driver remains an extremely complex challenge. Advances in artificial intelligence, machine learning and computer vision are required to solve it. These technologies will eventually lead us to a new generation of the automobile — a vehicle that is connected, intelligent, and able to safely operate itself alone or as part of a shared fleet. The potential of these shared fleets of self-driving vehicles will be one of the most transformative advancements in this century.</p><p>Autonomous vehicles have the potential to save thousands of lives, to extend personal mobility to many who might not have it, and to transform the landscape of an urban setting. On the safety front, the always-attentive capability of a self-driving vehicle can remove concerns over driver distraction. Allowing a vehicle to do the actual driving could enable the peace of mind that comes from having a chauffeur, meaning people wouldn’t have to concentrate on driving when they don’t want to do so or have the capability to handle the task.</p><p><a href="https://medium.com/@ford/a-look-into-fords-self-driving-future-5aae38ee2059">A Look into Ford’s Self-Driving Future</a></p><p>On the mobility front, autonomous vehicles will provide freedom and independence for the millions of people who might lack the ability to safely or legally drive themselves. On the urban front, networks of self-driving vehicles have the potential to reduce much of the traffic congestion that is choking cities today.</p><p>Consider this: Most personally owned vehicles sit idle about 95 percent of the time. To allow for this, as much as 30 percent of the real estate in a city center may be devoted solely to parking. Think about how autonomous vehicle fleets can address these challenges. They can provide convenient, reliable, affordable transportation shared among multiple users to reduce congestion and parking lot usage, enabling communities to replace parking lots with more economic and socially beneficial spaces.</p><p>Argo AI will develop and deploy the latest advancements in artificial intelligence, machine learning and computer vision to help build safe and efficient self-driving vehicles that enable these transformations and more. The challenges are significant, but we are a team that believes in tackling hard, meaningful problems to improve the world. Our ambitions can only be realized if we are willing to partner with others and keep an open mind about how to solve problems.</p><p>We’re excited about Ford’s commitment to invest $1 billion over the next five years in Argo AI to develop a virtual driver system for its fully autonomous vehicle coming in 2021. We’re joining forces with Ford’s autonomous vehicle development team to strengthen the commercialization of self-driving vehicles. Our agility combined with Ford’s scale uniquely unites the benefits of a technology startup with the experience and discipline of an automaker’s industry-leading autonomous vehicle development program.</p><p><a href="https://shift.newco.co/fords-road-to-full-autonomy-36cb9cca330">Ford’s Road to Full Autonomy</a></p><p>We’re working with Ford because of their enduring commitment to mobility dating back more than 100 years. <a href="https://medium.com/u/db92c082f24a">Ford Motor Company</a> has built its business around human-driven vehicles, but its visionaries see the societal benefits that self-driving vehicles enable for the future. We respect Ford’s extensive autonomous vehicle research and development efforts and are thrilled to be partnering with them.</p><p>Ford is the majority stakeholder in Argo AI, but we are structured to operate with substantial independence. Our initial focus will be to support Ford’s autonomous vehicle development and production, but in the future, we may license our technology to other companies and sectors looking for self-driving capability.</p><p>Our employees will enjoy significant equity participation in the company, enabling them to share in our success. We’re actively recruiting for our Pittsburgh headquarters, and at our engineering centers in Southeastern Michigan and the Bay Area of California so that we can quickly grow our team.</p><p>At Argo AI, we will execute with urgency and commitment, ensuring this important technology is responsibly made available to millions.</p><p>– The Argo AI team</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=aa3f43ebefb6" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/why-we-created-argo-ai-aa3f43ebefb6">Why We Created Argo AI</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/why-we-created-argo-ai-aa3f43ebefb6?source=rss-ba51b2aa6fa1------2" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>442</wp:post_id>
		<wp:post_date><![CDATA[2017-02-10 19:30:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-02-10 19:30:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[why-we-created-argo-ai]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@ArgoAI]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/why-we-created-argo-ai-aa3f43ebefb6?source=rss-ba51b2aa6fa1------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[798]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Microsoft, Toyota announce new licensing agreement for connected car IP</title>
		<link>https://fifthlevel.ai/archives/593</link>
		<pubDate>Sat, 01 Apr 2017 11:15:23 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.ipwatchdog.com/?p=80786</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The licensing agreement, which covers patents directed at connected car technologies, is the latest partnership between these two companies seeking to increase entertainment and autonomous tech platforms within vehicles.,, In recent months, Microsoft has been ramping up licensing programs seeking to encourage the use of its patented technologies by auto manufacturers. At this year’s Consumer Electronics Show in Las Vegas, the company announced the Microsoft Connected Car Platform, a development platform for connected car technologies using the Azure cloud.</p>
<p>The post <a rel="nofollow" href="http://www.ipwatchdog.com/2017/04/01/microsoft-toyota-announce-new-licensing-agreement-connected-car-ip/id=80786/">Microsoft, Toyota announce new licensing agreement for connected car IP</a> appeared first on <a rel="nofollow" href="http://www.ipwatchdog.com">IPWatchdog.com | Patents &amp; Patent Law</a>.</p> <p><b><a href="http://www.ipwatchdog.com/2017/04/01/microsoft-toyota-announce-new-licensing-agreement-connected-car-ip/id=80786/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>593</wp:post_id>
		<wp:post_date><![CDATA[2017-04-01 11:15:23]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-04-01 11:15:23]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[microsoft-toyota-announce-new-licensing-agreement-for-connected-car-ip]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/tag/autonomous-vehicles/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/2017/04/01/microsoft-toyota-announce-new-licensing-agreement-connected-car-ip/id=80786/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Alphabet’s Waymo files patent and trade secret lawsuit against Uber</title>
		<link>https://fifthlevel.ai/archives/2546</link>
		<pubDate>Sun, 19 Mar 2017 10:15:53 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.ipwatchdog.com/?p=79448</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Waymo’s suit includes counts of infringement for each of the four patents asserted in the case. The suit also includes counts for violations of the Defend Trade Secrets Act and state claims for violations of the California Uniform Trade Secret Act. Waymo is seeking preliminary and permanent injunctions, damages for patent infringement including trebled damages for infringement of the ‘922, ‘464 and ‘273 patents and punitive damages among other forms of relief.</p>
<p>The post <a rel="nofollow" href="http://www.ipwatchdog.com/2017/03/19/alphabets-waymo-patent-trade-secret-lawsuit-uber/id=79448/">Alphabet&#8217;s Waymo files patent and trade secret lawsuit against Uber</a> appeared first on <a rel="nofollow" href="http://www.ipwatchdog.com">IPWatchdog.com | Patents &amp; Patent Law</a>.</p> <p><b><a href="http://www.ipwatchdog.com/2017/03/19/alphabets-waymo-patent-trade-secret-lawsuit-uber/id=79448/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2546</wp:post_id>
		<wp:post_date><![CDATA[2017-03-19 10:15:53]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-03-19 10:15:53]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[alphabets-waymo-files-patent-and-trade-secret-lawsuit-against-uber]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="patents"><![CDATA[Patents]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/tag/autonomous-vehicles/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/2017/03/19/alphabets-waymo-patent-trade-secret-lawsuit-uber/id=79448/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>What Do Cloud Robotics Mean for Driverless Cars?</title>
		<link>https://fifthlevel.ai/archives/2547</link>
		<pubDate>Sat, 21 Jan 2017 12:00:41 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.ipwatchdog.com/?p=77173</guid>
		<description></description>
		<content:encoded><![CDATA[<p>When you think of autonomous cars or driverless vehicles, you probably don’t associate them with cloud computing and data analytics. However, that’s exactly the technology that makes autonomy possible, at least when it comes to modern mechanics... A vehicle that has to wait for commands from a cloud system is especially vulnerable.</p>
<p>The post <a rel="nofollow" href="http://www.ipwatchdog.com/2017/01/21/cloud-robotics-mean-driverless-cars/id=77173/">What Do Cloud Robotics Mean for Driverless Cars?</a> appeared first on <a rel="nofollow" href="http://www.ipwatchdog.com">IPWatchdog.com | Patents &amp; Patent Law</a>.</p> <p><b><a href="http://www.ipwatchdog.com/2017/01/21/cloud-robotics-mean-driverless-cars/id=77173/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2547</wp:post_id>
		<wp:post_date><![CDATA[2017-01-21 12:00:41]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-01-21 12:00:41]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[what-do-cloud-robotics-mean-for-driverless-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="patents"><![CDATA[Patents]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/tag/autonomous-vehicles/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/2017/01/21/cloud-robotics-mean-driverless-cars/id=77173/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>AgeLab researching autonomous vehicle systems in ongoing collaboration with Toyota</title>
		<link>https://fifthlevel.ai/archives/2647</link>
		<pubDate>Thu, 08 Jun 2017 15:10:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/mit-agelab-researching-autonomous-vehicle-system-in-ongoing-collaboration-with-toyota-0608</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The <a href="http://agelab.mit.edu/?utm_source=MITNews&amp;utm_medium=Article&amp;utm_campaign=ALToyota" target="_blank">MIT AgeLab</a>&nbsp;will build and analyze new deep-learning-based perception and motion planning technologies for automated vehicles in partnership with the Toyota Collaborative Safety Research Center (CSRC). The new&nbsp;research initiative, called CSRC Next, is part of&nbsp;a five-year-old ongoing relationship with Toyota.</p> <p>The first phase of projects with Toyota CSRC has been&nbsp;led by <a href="http://web.mit.edu/reimer/www/?utm_source=MITNews&amp;utm_medium=Article&amp;utm_campaign=ALToyota" target="_blank">Bryan Reimer</a>, a research scientist at MIT AgeLab, which is part of the&nbsp;<a href="http://ctl.mit.edu/?utm_source=MITNews&amp;utm_medium=Article&amp;utm_campaign=ALToyota" target="_blank">MIT Center for Transportation and Logistics</a>. Reimer&nbsp;manages a multidisciplinary team of researchers, and students focused on understanding how drivers respond to the increasing complexity of the modern operating environment. He and his team studied the demands of modern in-vehicle voice interfaces and found that they draw drivers’ eyes away from the road to a greater degree than expected, and that the demands of these interfaces need to be considered in the time course optimization of systems. Reimer’s study eventually contributed to the redesign of the instrumentation of the current Toyota Corolla and the forthcoming&nbsp;2018 Toyota Camry.&nbsp;(Read more in the <a href="http://pressroom.toyota.com/article_display.cfm?article_id=6052&amp;utm_source=MITNews&amp;utm_medium=Article&amp;utm_campaign=ALToyota" target="_blank">2017 Toyota CSRC report</a>.)</p> <p>Reimer and his team are also building and developing prototypes of hardware and software systems that can be integrated into cars in order to detect everything about the state of the driver and the external environment. These&nbsp;prototypes are designed to work both with cars with minimal levels of autonomy and with cars that are fully autonomous.&nbsp;</p> <p>Computer scientist and team member&nbsp;<a href="http://agelab.mit.edu/people/lex-fridman?utm_source=MITNews&amp;utm_medium=Article&amp;utm_campaign=ALToyota" target="_blank">Lex Fridman</a>&nbsp;is leading&nbsp;a group of seven computer engineers who are working on computer vision, deep learning, and planning algorithms for semi-autonomous vehicles. The application of deep learning is being used for understanding both the world around the car and human behavior inside it.</p> <p>“The vehicle must first gain awareness of all entities in the driving scene, including pedestrians, cyclists, cars, traffic signals, and road markings,”&nbsp;Fridman says. “We use a learning-based approach for this perception task and also for the subsequent task of planning a safe trajectory around those entities.”</p> <p>Fridman and his team, now firmly entrenched in the next phase of the project with Toyota CRSC, set up a stationary camera at a busy intersection on the MIT campus to automatically detect the micro-movements of pedestrians as they make decisions about crossing the street. Using deep learning and computer vision methods, the system automatically converts the raw video footage into millisecond-level estimations of each pedestrian’s body position.&nbsp;The program has analyzed&nbsp;the head, arm, feet and full-body movement of more than&nbsp;100,000 pedestrians.&nbsp;</p> <p>Fridman’s research also focuses on the world inside the car.&nbsp;</p> <p>“Just as interesting and complex is the integration of data inside the car to improve our understanding of automated systems and enhance their capability to support the driver,”&nbsp;he says.&nbsp;“This includes everything about the driver’s face, head position, emotion, drowsiness, attentiveness, and body language.”&nbsp;</p> <p>With Toyota and other partners, the team is exploring&nbsp;the use of cameras positioned to monitor&nbsp;the driver, as well as methods to&nbsp;extract all those driver state factors from the raw video and turn&nbsp;them into useable data which can to support future automotive industry needs.&nbsp;</p> <p>“What’s innovative about Lex’s work is that it uses state-of-the-art methods in computer science and artificial intelligence to study the complexities of human intent grounded in large-scale real-world data,” Reimer says.</p> <p>Toyota CSRC Director&nbsp;Chuck Gulash says the research&nbsp;“leverages the AgeLab’s expertise in computer vision, state detection, naturalistic data collection and deep learning to focus on the challenges and opportunities of autonomous vehicle technologies.”</p> <p>When asked how the research collaboration would affect the future of automotive technology, Gulash says it will&nbsp;“contribute to better computer-based perception of a vehicle’s environment as well as social interactions with other road users.”</p> <p>“What is unique about the AgeLab’s work is that it brings together advanced computer science with a human centered perspective on driver behavior,”&nbsp;he says. “As with all CSRC projects, output from the AgeLab’s effort will be openly shared with industry, academia and government to contribute to future safe mobility.”</p> <p>MIT AgeLab Director&nbsp;<a href="http://agelab.mit.edu/people/joseph-f-coughlin?utm_source=MITNews&amp;utm_medium=Article&amp;utm_campaign=ALToyota" target="_blank">Joe Coughlin</a>&nbsp;says the&nbsp;AgeLab “is using all of these technologies to do two things: understand human behavior in the driving context, and to design future systems that result in greater safety and expansion of mobility options for all ages.”</p>
<p><b><a href="http://news.mit.edu/2017/mit-agelab-researching-autonomous-vehicle-system-in-ongoing-collaboration-with-toyota-0608" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2647</wp:post_id>
		<wp:post_date><![CDATA[2017-06-08 15:10:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-06-08 15:10:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[agelab-researching-autonomous-vehicle-systems-in-ongoing-collaboration-with-toyota]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/mit-agelab-researching-autonomous-vehicle-system-in-ongoing-collaboration-with-toyota-0608]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Engineers design drones that can stay aloft for five days</title>
		<link>https://fifthlevel.ai/archives/2648</link>
		<pubDate>Wed, 07 Jun 2017 03:59:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/drones-stay-aloft-five-days-0607</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In the event of a natural disaster that disrupts phone and Internet systems over a wide area, autonomous aircraft could potentially hover over affected regions, carrying communications payloads that provide temporary telecommunications coverage to those in need.</p> <p>However, such unpiloted aerial vehicles, or UAVs, are often expensive to operate, and can only remain in the air for a day or two, as is the case with most autonomous surveillance aircraft operated by the U.S. Air Force. Providing adequate and persistent coverage would require a relay of multiple aircraft, landing and refueling around the clock, with operational costs of thousands of dollars per hour, per vehicle.&nbsp;</p> <p>Now a team of MIT engineers has come up with a much less expensive UAV design that can hover for longer durations to provide wide-ranging communications support. The researchers designed, built, and tested a UAV resembling a thin glider with a 24-foot wingspan. The vehicle can carry 10 to 20 pounds of communications equipment while flying at an altitude of 15,000 feet. Weighing in at just under 150 pounds, the vehicle is powered by a 5-horsepower gasoline engine and can keep itself aloft for more than five days — longer than any gasoline-powered autonomous aircraft has remained in flight, the researchers say.</p> <p>The team is presenting its results this week at the American Institute of Aeronautics and Astronautics Conference in Denver, Colorado. The team was led by R. John Hansman, the T. Wilson Professor of Aeronautics and Astronautics; and Warren Hoburg, the Boeing Assistant Professor of Aeronautics and Astronautics. Hansman and Hoburg are co-instructors for MIT’s Beaver Works project, a student research collaboration between MIT and the MIT Lincoln Laboratory.</p> <p><strong>A solar no-go</strong></p> <p>Hansman and Hoburg worked with MIT students to design a long-duration UAV as part of a Beaver Works capstone project — typically a two- or three-semester course that allows MIT students to design a vehicle that meets certain mission specifications, and to build and test their design.</p> <p>In the spring of 2016, the U.S. Air Force approached the Beaver Works collaboration with an idea for designing a long-duration UAV powered by solar energy. The thought at the time was that an aircraft, fueled by the sun, could potentially remain in flight indefinitely. Others, including Google, have experimented with this concept,&nbsp; designing solar-powered, high-altitude aircraft to deliver continuous internet access to rural and remote parts of Africa.</p> <p>But when the team looked into the idea and analyzed the problem from multiple engineering angles, they found that solar power — at least for long-duration emergency response — was not the way to go.</p> <p>“[A solar vehicle] would work fine in the summer season, but in winter, particularly if you’re far from the equator, nights are longer, and there’s not as much sunlight &nbsp;during the day. So you have to carry more batteries, which adds weight and makes the plane bigger,” Hansman says. “For the mission of disaster relief, this could only respond to disasters that occur in summer, at low latitude. That just doesn’t work.”</p> <p>The researchers came to their conclusions after modeling the problem using GPkit, a software tool developed by Hoburg that allows engineers to determine the optimal design decisions or dimensions for a vehicle, given certain constraints or mission requirements.</p> <p>This method is not unique among initial aircraft design tools, but unlike these tools, which take into account only several main constraints, Hoburg’s method allowed the team to consider around 200 constraints and physical models simultaneously, and to fit them all together to create an optimal aircraft design.</p> <p>“This gives you all the information you need to draw up the airplane,” Hansman says. “It also says that for every one of these hundreds of parameters, if you changed one of them, how much would that influence the plane’s performance? If you change the engine a bit, it will make a big difference. And if you change wingspan, will it show an effect?”</p> <p><strong>Framing for takeoff</strong></p> <p>After determining, through their software estimations, that a solar-powered UAV would not be feasible, at least for long-duration use in any part of the world, the team performed the same modeling for a gasoline-powered aircraft. They came up with a design that was predicted to stay in flight for more than five days, at altitudes of 15,000 feet, in up to 94th-percentile winds, at any latitude.</p> <p>In the fall of 2016, the team built a prototype UAV, following the dimensions determined by students using Hoburg’s software tool. To keep the vehicle lightweight, they used materials such as carbon fiber for its wings and fuselage, and Kevlar for the tail and nosecone, which houses the payload. The researchers designed the UAV to be easily taken apart and stored in a FedEx box, to be shipped to any disaster region and quickly reassembled.</p> <p>This spring, the students refined the prototype and developed a launch system, fashioning a simple metal frame to fit on a typical car roof rack. The UAV sits atop the frame as a driver accelerates the launch vehicle (a car or truck) up to rotation speed — the UAV’s optimal takeoff speed. At that point, the remote pilot would angle the UAV toward the sky, automatically releasing a fastener and allowing the UAV to lift off.</p> <p>In early May, the team put the UAV to the test, conducting flight tests at Plum Island Airport in Newburyport, Massachusetts. For initial flight testing, the students modified the vehicle to comply with FAA regulations for small unpiloted aircraft, which allow drones flying at low altitude and weighing less than 55 pounds. To reduce the UAV’s weight from 150 to under 55 pounds, the researchers simply loaded it with a smaller ballast payload and less gasoline.</p> <p>In their initial tests, the UAV successfully took off, flew around, and landed safely. Hoburg says there are special considerations that have to be made to test the vehicle over multiple days, such as having enough people to monitor the aircraft over a long period of time.</p> <p>“There are a few aspects to flying for five straight days,” Hoburg says. “But we’re pretty confident that we have the right fuel burn rate and right engine that we could fly it for five days.”</p> <p>“These vehicles could be used not only for disaster relief but also other missions, such as environmental monitoring. You might want to keep watch on wildfires or the outflow of a river,” Hansman adds. “I think it’s pretty clear that someone within a few years will manufacture a vehicle that will be a knockoff of this.”</p> <p>This research was supported, in part, by MIT Lincoln Laboratory.</p>
<p><b><a href="http://news.mit.edu/2017/drones-stay-aloft-five-days-0607" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2648</wp:post_id>
		<wp:post_date><![CDATA[2017-06-07 03:59:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-06-07 03:59:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[engineers-design-drones-that-can-stay-aloft-for-five-days]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/drones-stay-aloft-five-days-0607]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>LIDS Smart Urban Infrastructures Workshop highlights emerging research</title>
		<link>https://fifthlevel.ai/archives/2649</link>
		<pubDate>Wed, 31 May 2017 21:35:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/mit-lids-smart-urban-infrastructures-workshop-highlights-emerging-research-0531</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Data can reveal valuable insights about the ways humans interact with their urban surroundings, helping to determine the types of services and systems they need&nbsp;and how those services and systems should work.&nbsp;Transportation, the electric power grid, and other&nbsp;services people rely upon&nbsp;can become more automated and more responsive — and&nbsp;ultimately&nbsp;smarter — through data science approaches.</p> <p><br />
Those ideas were at the center of the <a href="https://lidssmart2017.mit.edu/" target="_blank">Smart Urban Infrastructures Workshop</a>, which was held May 11-12 at the Media Lab. The event was&nbsp;hosted by the <a href="http://lids.mit.edu" target="_blank">Laboratory for Information and Decisions Systems (LIDS)</a>, which is both the longest-running research laboratory at MIT and the major research lab of the <a href="https://idss.mit.edu" target="_blank">MIT Institute for Data, Systems, and Society</a>&nbsp;(IDSS).&nbsp;<br />
<br />
“Thanks to advances in technology, we see more and more smart services,” said Asuman Ozdaglar, the Joseph F. and Nancy P. Keithley Professor in Electrical Engineering and head of the Department of Electrical Engineering and Computer Science. “These smart services take many forms and include increasingly more platforms for sharing resources.”<br />
<br />
The conference was organized around six central themes related to smart services: security and privacy;&nbsp;smart cities and communities;&nbsp;communications and the internet of things;&nbsp;transportation services and platforms;&nbsp;autonomous transportation; and smart grid and energy services. The first day included a <a href="https://lidssmart2017.mit.edu/poster-session/" target="_blank">student poster session</a>, while the second day featured&nbsp;a keynote talk from GE Digital Vice President Peter Marx, who also drew from some of his past experiences in the public sector as chief technology officer for the City of&nbsp;Los Angeles.<br />
<br />
Speakers on the&nbsp;Security and Privacy in Smart Services panel shared a variety of perspectives on current privacy challenges. Daniel Weitzner, founding director of the MIT Internet Policy Research Initiative and principal research scientist at the Computer Science and Artificial Intelligence Laboratory (CSAIL), discussed the need to understand the significance and meaning of privacy in people’s lives in order to create effective policies. He explored&nbsp;the complexity that emerges when people&nbsp;try&nbsp;to define exactly what privacy is in terms of how people perceive and value it.</p> <p>“It’s tempting to have a single, formal definition of ‘privacy,” he said, “but we can’t do that. Privacy means different things to different people.”<br />
<br />
Lalitha Sankar, assistant professor in the School of Electrical, Computer&nbsp;and Energy Engineering at Arizona State University, addressed privacy and security in the context of power systems — including looking at the importance of cybersecurity in maintaining the operations of smart cities. She cited&nbsp;the example of a major cyber attack that disabled a third of the Ukrainian power grid to illustrate a case in which “the control was bypassed from the human in the loop,” and where the cyber system itself was unable to identify the problem.<br />
<br />
During the panel session on Smart Cities and Communities, Mark Gorenberg, the founder and managing director of Zetta Partners, talked about strategies for making communities more energy-efficient and more connected by using data related to local preferences and needs. Fellow panelist&nbsp;Glenn Ricart of US Ignite emphasized the importance of “civic partnerships,” including work with volunteers and universities.<br />
<br />
Amy Glasmeier, professor in the MIT Department of Urban Studies and Planning, focused on the ongoing challenge of providing more access to smart services for wider segments of the population.<br />
<br />
“How do we change and broaden the demographics [of people using smart systems]?” Glasmeier asked. In designing smart systems, she said&nbsp;the key question needs to be: “What is the problem we are trying to solve, and for whom?”<br />
<br />
In the panel on Communications and the Internet of Things in Smart Cities, Veniam CEO and founder João Barros&nbsp;discussed some communications applications that can improve cities, such as having vehicles share software updates.<br />
<br />
“Many applications allow you to reduce traffic if vehicles can communicate with each other,” he said.<br />
<br />
Iyad Rahwan, a professor at the Media Lab and IDSS affiliate faculty member, presented some of his research on the complex ethical dilemmas of autonomous vehicles. Human drivers make quick, intuitive, and often high-stakes decisions to assess relative risk and act accordingly. Although efforts toward building these capabilities in cars are well underway, designing such complex systems presents great challenges, he said.<br />
<br />
The Smart Grid and Energy Services panel also highlighted the human component of smart systems. Marija Ilic, an IDSS&nbsp;visiting professor, talked about the need to think about power grids as “data-enabled, socio-ecological systems.” Ilic and other panelists discussed the importance of making it easy for consumers to participate in decisions about energy usage — particularly by using data to identify certain tasks and decisions and then automate them.</p>
<p><b><a href="http://news.mit.edu/2017/mit-lids-smart-urban-infrastructures-workshop-highlights-emerging-research-0531" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2649</wp:post_id>
		<wp:post_date><![CDATA[2017-05-31 21:35:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-05-31 21:35:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[lids-smart-urban-infrastructures-workshop-highlights-emerging-research]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/mit-lids-smart-urban-infrastructures-workshop-highlights-emerging-research-0531]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Cinematography on the fly</title>
		<link>https://fifthlevel.ai/archives/2650</link>
		<pubDate>Thu, 18 May 2017 03:59:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/camera-equipped-drones-cinematography-0518</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In recent years, a host of Hollywood blockbusters — including “The Fast and the Furious 7,” “Jurassic World,” and “The Wolf of Wall Street” — have included aerial tracking shots provided by drone helicopters outfitted with cameras.</p> <p>Those shots required separate operators for the drones and the cameras, and careful planning to avoid collisions. But a team of researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and ETH Zurich hope to make drone cinematography more accessible, simple, and reliable.</p> <p>At the International Conference on Robotics and Automation later this month, the researchers will present a system that allows a director to specify a shot’s framing — which figures or faces appear where, at what distance. Then, on the fly, it generates control signals for a camera-equipped autonomous drone, which preserve that framing as the actors move.</p> <p>As long as the drone’s information about its environment is accurate, the system also guarantees that it won’t collide with either stationary or moving obstacles.</p> <p>“There are other efforts to do autonomous filming with one drone,” says Daniela Rus, an Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science at MIT and a senior author on the new paper. "They can follow someone, but if the subject turns, say 180 degrees, the drone will end up showing the back of the subject. With our solution, if the subject turns 180 degrees, our drones are able to circle around and keep focus on the face. We are able to specify richer higher-level constraints for the drones. The drones then map the high-level specifications into control and we end up with greater levels of interaction between the drones and the subjects."</p> <p>Joining Rus on the paper are Javier Alonso-Mora, who was a postdoc in her group when the work was done and is now an assistant professor of robotics at the Delft University of Technology; Tobias Nägeli, a graduate student at ETH Zurich and his advisor Otmar Hilliges, an assistant professor of computer science; and Alexander Domahidi, CTO of Embotech, an autonomous-systems company that spun out of ETH.</p> <div class="cms-placeholder-content-video"></div> <p><strong>In the picture</strong></p> <p>With the new system, the user can specify how much of the screen a face or figure should occupy, what part of the screen it should occupy, and what the subject’s orientation toward the camera should be — straight on, profile, three-quarter view from either side, or over the shoulder. Those parameters can be set separately for any number of subjects; in tests at MIT, the researchers used compositions involving up to three subjects.</p> <p>Usually, the maintenance of the framing will be approximate. Unless the actors are extremely well-choreographed, the distances between them, the orientations of their bodies, and their distance from obstacles will vary, making it impossible to meet all constraints simultaneously. But the user can specify how the different factors should be weighed against each other. Preserving the actors’ relative locations onscreen, for instance, might be more important than maintaining a precise distance, or vice versa. The user can also assign a weight to minimize occlusion, ensuring that one actor doesn’t end up blocking another from the camera.</p> <p>The key to the system, Alonso-Mora explains, is that it continuously estimates the velocities of all of the moving objects in the drone’s environment and projects their locations a second or two into the future. This buys it a little time to compute optimal flight trajectories and also ensures that it can get recover smoothly if the drone needs to take evasive action to avoid collision.</p> <p>The system updates its position projections about 50 times a second. Usually, the updates will have little effect on the drone’s trajectory, but the frequent updates ensure that the system can handle sudden changes of velocity.</p> <p>The researchers tested the system at CSAIL’s motion-capture studio, using a quadrotor (four-propeller) drone. The motion-capture system provided highly accurate position data about the subjects, the studio walls, and the drone itself.</p> <p>In one set of experiments, the subjects actively tried to collide with the drone, marching briskly toward it as it attempted to keep them framed within the shot. In all such cases, it avoided collision and immediately tried to resume the prescribed framing.</p>
<p><b><a href="http://news.mit.edu/2017/camera-equipped-drones-cinematography-0518" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2650</wp:post_id>
		<wp:post_date><![CDATA[2017-05-18 03:59:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-05-18 03:59:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[cinematography-on-the-fly]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/camera-equipped-drones-cinematography-0518]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Q&#038;A: On the future of human-centered robotics</title>
		<link>https://fifthlevel.ai/archives/2651</link>
		<pubDate>Mon, 15 May 2017 19:10:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/mit-professor-david-mindell-on-the-future-of-human-centered-robotics-0515</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>Science and technology are essential tools for innovation, and to reap their full potential, we also need to articulate and solve the many aspects of today’s global issues that are rooted in the political, cultural, and economic realities of the human world.&nbsp;With that mission in mind, MIT's School of Humanities, Arts, and Social Sciences has launched <a href="http://shass.mit.edu/news/news-2015-the-human-factor" target="_blank">The Human Factor</a> — an ongoing series of stories and interviews that highlight research on the human dimensions of global challenges. Contributors to this series also share ideas for cultivating the multidisciplinary collaborations needed to solve the major civilizational issues of our time.</em></p> <p><em>David Mindell, the Frances and David Dibner Professor of the History of Engineering and Manufacturing and Professor of Aeronautics and Astronautics at MIT, researches the intersections of human behavior, technological innovation, and automation. Mindell&nbsp;is the author of five acclaimed books, most recently "Our Robots, Ourselves: Robotics and the Myths of Autonomy" (Viking, 2015).&nbsp;He is also&nbsp;the co-founder of Humatics Corporation, which develops technologies for human-centered automation. SHASS Communications recently asked him to share his thoughts on the relationship of robotics to human activities, and the role of multidisciplinary research in solving complex global issues. </em></p> <p><strong>Q:</strong> A major theme in recent political discourse has been the perceived impact of robots and automation on the United States labor economy. In your research into the relationship between human activity and robotics, what insights have you gained that inform the future of human jobs, and the direction of technological innovation?</p> <p><strong>A:</strong>&nbsp;In looking at how people have designed, used, and adopted robotics in extreme environments like the deep ocean, aviation, or space, my most recent work shows how robotics and automation carry with them human assumptions about how work gets done, and how technology alters those assumptions. For example, the U.S. Air Force’s Predator drones were originally envisioned as fully autonomous — able to fly without any human assistance. In the end, these drones require hundreds of people to operate.</p> <p>The new success of robots will depend on how well they situate into human environments. As in chess, the strongest players are often the combinations of human and machine. I increasingly see that the three critical elements are people, robots, and infrastructure — all interdependent.</p> <p><strong>Q:</strong> In your recent book "Our Robots, Ourselves," you describe the success of a human-centered robotics,&nbsp;and explain why it is the more promising research direction — rather than research that aims for total robotic autonomy. How is your perspective being received by robotic engineers and other technologists, and do you see examples of research projects that are aiming at human-centered robotics?</p> <p><strong>A:</strong> One still hears researchers describe full autonomy as the only way to go; often they overlook the multitude of human intentions built into even the most autonomous systems, and the infrastructure that surrounds them. My work describes situated autonomy,&nbsp;where autonomous systems can be highly functional within human environments such as factories or cities. Autonomy as a means of moving through physical environments has made enormous strides in the past ten years. As a means of moving through human environments, we are only just beginning. The new frontier is learning how to design the relationships between people, robots, and infrastructure. We need new sensors, new software, new ways of architecting systems.</p> <p><strong>Q:</strong> What can the study of the history of technology teach us about the future of robotics?</p> <p><strong>A:</strong> The history of technology does not predict the future, but it does offer rich examples of how people build and interact with technology, and how it evolves over time. Some problems just keep coming up over and over again, in new forms in each generation. When the historian notices such patterns, he can begin to ask: Is there some fundamental phenomenon here? If it is fundamental, how is it likely to appear in the next generation? Might the dynamics be altered in unexpected ways by human or technical innovations?</p> <p>One such pattern is how autonomous systems have been rendered less autonomous when they make their way into real world human environments. Like the Predator&nbsp;drone, future military robots will likely be linked to human commanders and analysts in some ways as well. Rather than eliding those links, designing them to be as robust and effective as possible is a worthy focus for researchers’ attention.</p> <p><strong>Q</strong>: MIT President L. Rafael Reif has said that the solutions to today’s challenges depend on marrying advanced technical and scientific capabilities with a deep understanding of the world’s political, cultural, and economic realities. What barriers do you see to multidisciplinary, sociotechnical collaborations, and how can we overcome them?</p> <p><strong>A</strong>:&nbsp;I fear that as our technical education and research continues to excel, we are building human perspectives into technologies in ways not visible to our students. All data, for example, is socially inflected, and we are building systems that learn from those data and act in the world. As a colleague from Stanford recently observed, go to Google image search and type in “Grandma” and you’ll see the social bias that can leak into data sets —&nbsp;the top results all appear white and middle class.</p> <p>Now think of those data sets as bases of decision making for vehicles like cars or trucks, and we become aware of the social and political dimensions that we need to build into systems to serve human needs. For example, should driverless cars adjust their expectations for pedestrian behavior according to the neighborhoods they’re in?</p> <p>Meanwhile, too much of the humanities has developed islands of specialized discourse that is inaccessible to outsiders. I used to be more optimistic about multidisciplinary collaborations to address these problems. Departments and schools are great for organizing undergraduate majors and graduate education, but the old two-cultures&nbsp;divides remain deeply embedded in the daily practices of how we do our work. I’ve long believed MIT needs a new school to address these synthetic, far-reaching questions and train students to think in entirely new ways.</p> <p><em>Interview prepared by MIT SHASS Communications<br />
Editorial team: Emily Hiestand (series editor), Daniel Evans Pritchard</em><br />
&nbsp;</p>
<p><b><a href="http://news.mit.edu/2017/mit-professor-david-mindell-on-the-future-of-human-centered-robotics-0515" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2651</wp:post_id>
		<wp:post_date><![CDATA[2017-05-15 19:10:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-05-15 19:10:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[qa-on-the-future-of-human-centered-robotics]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/mit-professor-david-mindell-on-the-future-of-human-centered-robotics-0515]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Eric Schmidt visits MIT to discuss computing, artificial intelligence, and the future of technology</title>
		<link>https://fifthlevel.ai/archives/2652</link>
		<pubDate>Thu, 04 May 2017 21:10:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/eric-schmidt-visits-mit-to-discuss-computing-ai-future-of-technology-0504</guid>
		<description></description>
		<content:encoded><![CDATA[<p>When Alphabet executive chairman Eric Schmidt started programming in 1969 at the age of 14, there was no explicit title for what he was doing. “I was just a nerd,” he says.</p> <p>But now computer science has fundamentally transformed fields like transportation, health care and education, and also provoked many new questions. What will artificial intelligence (AI) be like in 10 years? How will it impact tomorrow’s jobs? What’s next for autonomous cars?</p> <p>These topics were all on the table on May 3, when the <a href="http://csail.mit.edu" target="_blank">Computer Science and Artificial Intelligence Laboratory</a> (CSAIL) hosted Schmidt for a conversation with CSAIL Director Daniela Rus at the Kirsch Auditorium in the Stata Center.</p> <div class="cms-placeholder-content-video"></div> <p>Schmidt discussed his early days as a computer science PhD at the University of California at Berkeley, where he looked up to MIT researchers like Michael Dertouzos. At Bell Labs he coded UNIX’s lexical-analysis program Lex before moving on to executive roles at Sun Microsystems, Novell, and finally Google, where he served as CEO from 2001 to 2011. In his current role as executive chairman of Google’s parent company, Schmidt focuses on Alphabet’s external matters, advising Google CEO Sundar Pichai and other senior leadership on business and policy.</p> <p>Speaking with Rus on the topic of health care, Schmidt said that doing a better job of leveraging data will enable doctors to improve how they make decisions.</p> <p>“Hospitals have enormous amounts of data, which is inaccessible to anyone except for themselves,” he said. “These [machine learning] techniques allow you to take all of that information, sum it all together, and actually produce outcomes.”</p> <p>Schmidt also cited Google’s ongoing work in self-driving vehicles, including last week’s launch of 500 cars in Arizona, and addressed the issue of how technology will impact jobs in different fields.</p> <p>“The economic folks would say that you can see the job that’s lost, but you very seldom can see the job that’s created,” said Schmidt. “While there will be a tremendous dislocation of jobs — and I’m not denying that — I think that, in aggregate, there will be more jobs.” &nbsp;</p> <p>Rus also asked Schmidt about his opposition to the Trump administration’s efforts to limit the number of H1B visas that U.S. tech companies can offer to high-skilled foreign workers.</p> <p>“At Google we want the best people in the world, regardless of sex, race, country, or what-have-you,” said Schmidt. “Stupid government policies that restrict us from giving us a fair chance of getting those people are antithetical to our mission [and] the things we serve.”</p> <p>Schmidt ended the conversation by imploring students to take the skills they’ve learned and use them to work on the world’s toughest problems.</p> <p>“There’s nothing more exciting than that feeling of inventing something new,” he said. “You as scientists should identify those areas and run at them as hard as you can.”</p> <p>In his introduction of Schmidt, MIT President L. Rafael Reif applauded him for his leadership on issues like innovation and sustainability, including his support of MIT’s <a href="https://www.mitinclusiveinnovation.com/" target="_blank">Inclusive Innovation Competition</a>, which awards prizes to organizations that focus on improving economic opportunity for low-income communities.</p> <p>“Eric embodies what we at MIT call ‘making a better world,’” said Reif. “As AI and machine learning become more sophisticated and increase the potential for automation, the concept of ‘inclusive innovation’ has never been more critical. I am grateful to Eric for his support of the competition and for his partnership on an issue that matters deeply to us at MIT.”</p> <p>The talk was co-sponsored by CSAIL and the Department of Electrical Engineering and Computer Science.</p>
<p><b><a href="http://news.mit.edu/2017/eric-schmidt-visits-mit-to-discuss-computing-ai-future-of-technology-0504" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2652</wp:post_id>
		<wp:post_date><![CDATA[2017-05-04 21:10:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-05-04 21:10:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[eric-schmidt-visits-mit-to-discuss-computing-artificial-intelligence-and-the-future-of-technology]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/eric-schmidt-visits-mit-to-discuss-computing-ai-future-of-technology-0504]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Governor Charlie Baker visits AgeLab at MIT Center for Transportation and Logistics</title>
		<link>https://fifthlevel.ai/archives/2653</link>
		<pubDate>Wed, 19 Apr 2017 19:50:53 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/governor-charlie-baker-visits-agelab-mit-center-transportation-logistics-0419</guid>
		<description></description>
		<content:encoded><![CDATA[<p>On April 12, Massachusetts Governor Charlie Baker visited the <a href="http://agelab.mit.edu/" target="_blank">MIT AgeLab</a> at the <a href="http://ctl.mit.edu/" target="_blank">MIT Center for Transportation and Logistics</a>, where he signed an executive order establishing the state’s first Governor’s Council to Address Aging in Massachusetts.</p> <p>The council will develop a plan to improve public and private efforts to support healthy aging in Massachusetts, to achieve the goal of making the Commonwealth the most age-friendly state in the nation. Older adults are the largest and fastest-growing segment of the U.S. population, and they will make up 23 percent of the Commonwealth’s population by 2035.&nbsp;&nbsp;&nbsp;&nbsp;</p> <p>“The notion that people are fully retired at the age of 65 is inconsistent with what I see around Massachusetts every day,” said Governor Baker. “Many of our older adults still have ample time, energy, and talent available to start a second or third career, volunteer in their community, become a mentor, or pursue an unfulfilled passion. I look forward to the council’s work considering ways for the state to improve public and private means for supporting and engaging with older adults.”</p> <p>“Since CTL invested in AgeLab 15 years ago, it has become a major center for research on aging,” said Yossi Sheffi, director of the Center for Transportation and Logistics. Initially, AgeLab focused on improving transportation for the elderly, but since then has expanded its research portfolio to look at technological innovation and improvements in lifestyle, home, and health. The implications for product supply chains as the demand for products tailored to elderly consumers increases, is another area of interest. “The fact that the governor chose to sign the executive order that brings this groundbreaking council into existence here at MIT reflects AgeLab’s leadership position in the field,” said Sheffi, who is the Elisha Gray II Professor of Engineering Systems and a professor in civil and environmental engineering at MIT.</p> <p>A multidisciplinary research program, the MIT AgeLab is focused on improving the experience of life in old age. Its current directions of inquiry include research into the rapidly changing behavior of older consumers and their caregivers, older drivers, and even younger people as they face the prospect of a longer — and perhaps more uncertain — future than ever before. The ultimate goal, says Joseph F. Coughlin, the AgeLab's founder and director, is to expand how aging is framed in the eyes of researchers, product developers, marketers, startup founders, venture capitalists — and yes, government.</p> <p>“Increased longevity is among humankind's greatest achievements,” Coughlin said. “The challenge we now face is to live not just longer, but also better. Innovations being developed here at MIT and throughout the Commonwealth promise to improve life for older adults and their families. Moreover, these new technologies, services, and related businesses are fast positioning Massachusetts as the global leader in the rapidly growing longevity economy.” Coughlin is a member of the new council.</p> <p>The council will be supported by the Executive Office of Elder Affairs, and is expected to deliver a preliminary report to Governor Baker by the end of the year.</p>
<p><b><a href="http://news.mit.edu/2017/governor-charlie-baker-visits-agelab-mit-center-transportation-logistics-0419" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2653</wp:post_id>
		<wp:post_date><![CDATA[2017-04-19 19:50:53]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-04-19 19:50:53]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[governor-charlie-baker-visits-agelab-at-mit-center-for-transportation-and-logistics]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/governor-charlie-baker-visits-agelab-mit-center-transportation-logistics-0419]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Looking ahead to the future of computer-driven cars</title>
		<link>https://fifthlevel.ai/archives/2654</link>
		<pubDate>Thu, 13 Apr 2017 21:05:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/amnon-shashua-mobileye-discusses-future-of-computer-driven-cars-0413</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The field of transportation is undergoing a seismic shift with the introduction of autonomous driving — or computer-driven cars. Computer vision scientist and <a href="http://www.mobileye.com/en-us/" target="_blank">Mobileye</a> co-founder Amnon Shashua PhD ’93 described the challenges associated with this technology in a talk last month hosted by MIT’s <a href="http://cbmm.mit.edu/" target="_blank">Center for Brains, Minds and Machines</a> (CBMM).<br />
&nbsp;<br />
The technology behind computer driven cars, Shashua <a href="http://cbmm.mit.edu/video/convergence-machine-learning-and-artificial-intelligence-towards-enabling-autonomous-driving" target="_blank">explained</a>, involves machine learning and the latest cutting-edge artificial intelligence algorithms in three major areas: sensing, planning, and mapping.<br />
&nbsp;<br />
Shashua earned his PhD in the Artificial Intelligence Laboratory at MIT’s Department of Brain and Cognitive Sciences in 1993. He received postdoctoral training under Professor Tomaso Poggio at MIT’s Center for Biological and Computational Learning (now CBMM) and currently holds the Sachs Chair in computer science at the Hebrew University of Jerusalem.<br />
&nbsp;<br />
In 1999, Shashua co-founded Mobileye, an Israeli technology company that makes sensors and cameras for driverless vehicles. In March, chip maker Intel purchased Mobileye for $15.3 billion — the largest-ever acquisition of an Israeli tech company.</p>
<p><b><a href="http://news.mit.edu/2017/amnon-shashua-mobileye-discusses-future-of-computer-driven-cars-0413" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2654</wp:post_id>
		<wp:post_date><![CDATA[2017-04-13 21:05:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-04-13 21:05:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[looking-ahead-to-the-future-of-computer-driven-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/amnon-shashua-mobileye-discusses-future-of-computer-driven-cars-0413]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Multi-university effort will advance materials, define the future of mobility</title>
		<link>https://fifthlevel.ai/archives/2655</link>
		<pubDate>Mon, 03 Apr 2017 21:55:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/multi-university-effort-will-advance-materials-define-future-of-mobility-0403</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Three MIT-affiliated research teams will receive about $10M in funding as part of a $35M materials science discovery program launched by the <a href="http://www.tri.global" target="_blank">Toyota Research Institute (TRI)</a>. Provided over four years, the support to MIT researchers will be primarily directed at scientific discoveries and advancing a technology that underpins the future of mobility and autonomous systems: energy storage.</p> <p>MIT’s <a href="http://www.mit.edu/~bazant/" target="_blank">Martin Bazant</a>, joined by colleagues at Stanford University and Purdue University, will lead an effort to develop a novel, data-driven design of lithium-ion (Li-ion) batteries. These energy storage workhorses, used in cellphones and hybrid cars, are practical, but complicated due to the fundamental complexity of their electrochemistry. Leveraging a <a href="http://news.stanford.edu/2016/08/04/stanford-probes-secrets-rechargeable-batteries/" target="_blank">nanoscale visualization technique</a> that revealed, for the first time, how Li-ion particles charge and discharge in real time, in <a href="http://news.mit.edu/2012/lithium-battery-decoded-0208" target="_self">good agreement with his theoretical predictions</a>, Bazant will use machine learning to develop a scalable predictive modeling framework for rechargeable batteries.</p> <p>“By applying machine learning methods to these videos of the inner workings of rechargeable batteries — using each pixel and each frame as a measurement — we can tease out models that better fit the experimental data,” says Bazant, the E. G. Roos (1944) Professor of Chemical Engineering and a professor of mathematics. “The approach has the potential to unify energy materials design by connecting atomistic with macroscopic properties and advance electrochemical materials more generally.”</p> <p>In addition to Bazant’s endeavor, which also includes collaborator <a href="http://web.mit.edu/cheme/people/profile.html?id=48" target="_blank">Richard Braatz</a>, the Edwin R. Gilliland Professor, two other MIT-affiliated projects will receive support from TRI. <a href="http://www.rle.mit.edu/gg/" target="_blank">Jeffrey Grossman</a>, the Morton and Claire Goulder and Family Professor in Environmental Systems, and <a href="http://web.mit.edu/eel/people.html" target="_blank">Yang Shao-Horn</a>, the W.M. Keck Professor of Energy, will lead the largest funded project focused on the design principles of polymer stability and conductivity for lithium batteries. The team also includes <a href="http://web.mit.edu/johnsongroup/" target="_blank">Jeremiah A. Johnson</a>, the Firmenich Career Development Associate Professor in the Department of Chemistry, and <a href="http://willardgroup.mit.edu" target="_blank">Adam Willard</a>, assistant professor in chemistry, as well as machine learning and optimization expert <a href="http://www.mit.edu/~suvrit" target="_blank">Suvrit Sra</a>, principal research scientist in the Laboratory for Information and Decision Systems (LIDS) in the Department of Electrical Engineering and Computer Science.</p> <p>Sra is excited about the research because it “brings together diverse expertise and offers a remarkable opportunity to develop machine learning models tuned to the problem, as well as large-scale discrete probability and optimization algorithms, topics that lie at the heart of my research.” The long-term impact that machine learning, and more, broadly artificial intelligence techniques, will have on materials discovery, he adds, extends well beyond this one project. Sra expects that in addition to accelerating materials discovery the methods he develops will lead to fundamental progress in machine learning too.</p> <p>In addition to these lithium battery projects, <a href="https://www.romangroup.mit.edu" target="_blank">Yuriy Román</a>, associate professor of chemical engineering, will serve as co-lead investigator with Shao-Horn to explore the design principles of nanostructured, non-precious-metal-containing catalysts for oxygen reduction and evolution. Leveraging a novel synthesis route to create nanostructured catalysts with minute precious metals developed in the Roman lab, Roman and Shao-Horn will develop a predictive framework for catalytic activity. The researchers aim to identify new classes of stable, highly active electrocatalysts — essential components in renewable energy technologies like fuel cells, metal-air batteries and solar fuels — that are less expensive to produce and commercialize.</p> <p>While backed by a company known primarily for its cars, TRI’s priorities are expansive, including artificial intelligence and computer science, home robotics and assistive technologies, and materials design and discovery.</p> <p>Bazant has been impressed by the flexibility TRI provides and by their comfort with backing fundamental science, practical application, as well as blue-sky ideas. “It’s an unusual institute in terms of funding, unlike most government and industry avenues. We can set up teams that are not too big and more nimble, and each year we can revise our plan rather than be focused on a specific technology,” he says.</p> <p>Not bound to the typical “trial and error approach to product development and commercialization,” Bazant and other faculty can focus on theory and simulation using data or explore the basic design principles of materials. In his case, that means the possibility of contributing to the design of a future hybrid car as well as advancing machine learning techniques for materials that go well beyond batteries.</p> <p>“I’m confident we will push boundaries in basic scientific discoveries, nanomaterials, catalysis, and energy systems that go beyond just new innovation a few years down the road,” adds Shao-Horn. All of the research findings supported by TRI will remain open and publishable in scientific journals.</p> <p>"Accelerating the pace of materials discovery will help lay the groundwork for the future of clean energy and bring us even closer to achieving Toyota’s vision of reducing global average new-vehicle CO<sub>2</sub> emissions by 90 percent by 2050,” said TRI Chief Science Officer Eric Krotkov in a <a href="http://pressroom.toyota.com/releases/tri+artificial+intelligence+new+materials+march30.htm" target="_blank">prior press release</a>.</p> <p>These grants in materials discovery build upon earlier support provided to MIT researchers. In the fall of 2015, TRI announced <a href="http://news.mit.edu/2015/csail-toyota-25-million-research-center-autonomous-cars-0904" target="_self">$50 million in research funding</a>, half of which went to MIT’s Computer Science and Artificial intelligence Laboratory (CSAIL) to fund a center dedicated to developing autonomous vehicles technologies to improve safety. Moreover, the Institute’s presence is felt deeply throughout TRI. <a href="http://meche.mit.edu/people/faculty/jleonard@mit.edu" target="_blank">John Leonard</a>, the Samuel C. Collins Professor of Mechanical and Ocean Engineering, heads up their autonomy effort; <a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>, associate professor in the Department of Electrical Engineering and Computer Science, leads simulation and control; and Gill Pratt ’89, CEO of TRI, was former director of the MIT Leg Lab.&nbsp;</p>
<p><b><a href="http://news.mit.edu/2017/multi-university-effort-will-advance-materials-define-future-of-mobility-0403" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2655</wp:post_id>
		<wp:post_date><![CDATA[2017-04-03 21:55:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-04-03 21:55:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[multi-university-effort-will-advance-materials-define-the-future-of-mobility]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/multi-university-effort-will-advance-materials-define-future-of-mobility-0403]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>At the intersection of industry and academia</title>
		<link>https://fifthlevel.ai/archives/2692</link>
		<pubDate>Wed, 22 Mar 2017 21:35:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/intersection-of-industry-and-academia-0322</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In an effort to solve pressing issues faced by the engineering industry and by the world more broadly, researchers from across MIT joined forces with representatives from <a href="https://www.parsons.com/pages/default.aspx" target="_blank">Parsons Corporation</a>, a technology-driven engineering services firm, for a day of collaboration.</p> <p>The Infrastructure, Smart Cities and Transportation Workshop, co-hosted by the Department of Civil and Environmental Engineering (CEE) and Parsons, was held on March 8. The event opened doors to future opportunities for MIT researchers and members of the engineering industry to work together.</p> <p>“As students we can easily get caught up in the weeds of our technical research, so we find it rewarding to hear from experts who can help us connect the theory of our research with applications within and outside of academia. Today is the perfect example of that connection,” said graduate student Adam Rosenfield on behalf of the MIT Transportation Student Group, a student society of the Interdepartmental Program in Transportation.</p> <p>The day-long event brought together faculty members and leaders from cross-cutting initiatives and programs from across MIT, leadership representatives from Parsons, and students and postdocs from many corners of MIT to explore parallels between ongoing research and current industry needs. The event highlighted how combining resources from academia and the engineering industry can solve major infrastructure and transportation problems and ultimately create a more sustainable world.</p> <p>Overlaps between Parsons and CEE run deeper than research topics. Head of CEE <a href="http://web.mit.edu/mbuehler/www/" target="_blank">Markus Buehler</a>, the McAfee Professor of Engineering and organizer of the workshop, explained that Ralph M. Parsons, Parsons’ founder, donated the funds needed to double the size of the old CEE hydrology laboratory. In 1970, the renovated and remodeled lab was renamed the Ralph M. Parsons Laboratory for Environmental Science and Engineering.&nbsp;</p> <p>“MIT’s commitment to address the most challenging issues in infrastructure and environment and the focus of Parsons to solve the toughest problems are an incredibly exciting mix that can lead to new paradigms of innovation and impact,” Buehler said. “Many of the ideas discussed at the workshop could define the future of civil and environmental engineering professionals. Changing the world to become a better place has never been more urgent and tangible.”</p> <p>Biff Lyons, executive vice president of Parsons’ Security and Intelligence Division, furthered this idea by commenting on the innovative culture at MIT. “The unique nature of what you do here, to create an environment where students from all over the world can come and work together to tackle big problems, that kind of culture is also important to us at Parsons.”&nbsp;</p> <p><strong>Empowering with data </strong></p> <p>“We can wake up, check a few apps, and decide in real-time at what time we are going to leave for work, what mode we will take, and what route we will take,” said <a href="https://cee.mit.edu/people_individual/carolina-osorio/" target="_blank">Carolina Osorio</a>, assistant professor in CEE. This is just one example of smart mobility.</p> <p>The concepts of smart mobility and smart cities were central to the discussions at the workshop. By deeming something “smart,” one is referring the use of data and networked systems to create more efficient and sustainable alternatives to established norms. These types of “smart” innovations are already embedded into the world; smart mobility could refer to apps like Uber and Lyft, which allow users to track in real-time where their taxi service is located, such as the situation that Osorio referenced.</p> <p>Likewise, a smart city might utilize an app that shows the exact route of a snow plow, suggested Lester Yoshida, senior vice president of Intelligent Transportation Systems at Parsons. The value of smart cities and improved infrastructure became increasingly apparent during numerous talks throughout the day.</p> <p>Transportation is central to the design and function of smart cities, especially in urban settings. Bringing together scholars from across multiple departments resulted in an interdisciplinary approach to an oft-studied field. Combined with industry expertise from Parsons, intelligent transportation was another major topic of interest throughout the workshop.</p> <p><a href="https://cee.mit.edu/people_individual/moshe-e-ben-akiva/" target="_blank">Moshe Ben-Akiva</a>, the Edmund K. Turner Professor of CEE, began the dialogue about smart mobility, speaking specifically about the use of optimization and behavioral modeling techniques to design efficient and personalized smart mobility solutions. Ben-Akiva’s group has developed an app-based behavior laboratory, Future Mobility Sensing, and a computer simulation laboratory, SimMobility, to make&nbsp;inferences about user preferences and to&nbsp;evaluate&nbsp;smart mobility solutions&nbsp;using real data and under various scenarios. Ben-Akiva gave an overview of his numerous research projects, including real-time toll optimization in Texas, Autonomous Mobility On-Demand in Singapore, Flexible Mobility On-Demand in Japan,&nbsp;and Tripod, a system of sustainable travel incentives with prediction, optimization and personalization capabilities&nbsp;in Boston. He concluded his talk by discussing the impacts of smart mobility and new technologies on the future of urban mobility. &nbsp;</p> <p>Data are extremely valuable in transportation research. Yoshida stressed the importance of origin and destination pairs, data points that show the routes autonomous vehicles (AVs) use on their trips. By looking at this information, Yoshida suggests the developers of AVs can more effectively understand the impact of AVs on traffic and infrastructure. The insight can allow for further consideration of how to create more efficient systems to take advantage of the existing infrastructure and to not congest certain areas.</p> <p>“The key feature in transportation that’s really changing the field is all this data that we have available,” said Osorio, who develops methods to inform the design and operations of large-scale mobility systems, while accounting for the intricate real-time interactions between the system and its users. “We now have travelers that are better equipped with data and have a better understanding of the system and what their options are.”</p> <p>While this is empowering to travelers, it puts increasing demand on the operators, designers and stakeholders that collect the information, since they now need to predict how individual users will react in real-time and how that will impact traffic patterns throughout the city, Osorio noted. &nbsp;</p> <p><strong>Hype and hacks of autonomous transportation and smart infrastructures </strong></p> <p><a href="http://dusp.mit.edu/faculty/jinhua-zhao" target="_blank">Jinhua Zhao</a>, the Edward H. and Joyce Linde Assistant Professor of City and Transportation Planning in the Department of Urban Studies and Planning (DUSP), spoke about the social and emotional aspects of transportation that are often ignored in transportation research, and how behavioral science can be integrated with transportation technology to shape travel behavior and design mobility systems.</p> <p>Zhao also pointed out the social function of the sharing economy, particularly ride-sharing. He explained that with public transit, “even though it is public, it does not induce social behavior. People look down and avoid making eye contact with each other. But in the back of a car, it’s different. That’s where conversation happens.” The nature of shared car rides is impromptu, captive for a considerable duration, and remarkably more intimate, representing a unique juxtaposition of spontaneity and intensity. He also noted that the opportunity for social interaction in ride-sharing can reduce the anxiety during commuting, enable passengers to use the time more productively, and fertilize innovative mobility system design. But Zhao also warned the potential for social prejudice to be reflected in the ride sharing context and called for regulatory innovations.</p> <p>While there is lots of excitement surrounding autonomous vehicles, Ali Jadbabaie, the JR East Professor of CEE and Institute for Data, Systems and Society (IDSS) and associate director of IDSS, pointed out the vulnerability of self-driving cars to cyberphysical attacks. Jadbabaie described this as “hype and asymmetry,” an imbalance between the excitement about AVs versus the ease of attack and difficulty of defense. He furthermore pointed out the importance of understanding the social behavior of drivers and how the self-driving cars and regular vehicles can coexist in the near future.</p> <p>Assistant Professor of CEE <a href="https://cee.mit.edu/people_individual/saurabh-amin/" target="_blank">Saurabh Amin</a> similarly presented on cyber-physical infrastructure security. He studies network control and resilient infrastructures, and spoke about the use of algorithms to detect and respond to both random and adversarial incidents in a network, such as in electrical power distribution and urban water systems. He and researchers from the Resilient Infrastructure Networks Lab create algorithms that anticipate and plan for malicious attacks on systems, such as the hacks alluded to by Jadbabaie.</p> <p>Cybersecurity is also one service Parsons provides. Jay Williams, vice president of critical infrastructure protection at Parsons, spoke briefly about when cyber-attacks converge with physical spaces and he described the multi-faceted approach Parsons uses to ensure security.</p> <p><strong>What’s next?</strong></p> <p>Transportation is constantly evolving, from the rise of Uber and ride-sharing, to self-driving cars taking to the roads. Smart cities will also continue to evolve as transportation and other new technologies change.</p> <p>Even though smart cities and autonomous vehicles are at the top of many peoples’ minds, Gibran Hadj-Chikh, director of innovative transport at Parsons, reminded the audience that “we never know what’s coming,” and that it’s vital to think outside of the box at what other alternatives there are that could enter the market.</p> <p>Osorio echoed this uncertainty of the future; “When we think about the next generation of systems, there’s a lot of uncertainty about what it’s going to look like. Very disruptive technologies like Uber and autonomous vehicles are going to keep presenting themselves, and they’re going to change how users interact with the system and what types of new mobility services need to be provided,” she said.</p> <p>Fortunately, the innovation and successful entrepreneurship that stems from MIT is well-positioned to address the future of transportation and infrastructures as they progress and advance over time.</p>
<p><b><a href="http://news.mit.edu/2017/intersection-of-industry-and-academia-0322" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2692</wp:post_id>
		<wp:post_date><![CDATA[2017-03-22 21:35:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-03-22 21:35:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[at-the-intersection-of-industry-and-academia]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/intersection-of-industry-and-academia-0322]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Security for multirobot systems</title>
		<link>https://fifthlevel.ai/archives/2693</link>
		<pubDate>Fri, 17 Mar 2017 03:59:57 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/security-multirobot-systems-hackers-0317</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Distributed <a href="http://news.mit.edu/2011/distributed-computing-0608">planning</a>, <a href="http://news.mit.edu/2016/human-robot-rescue-teams-0217">communication</a>, and <a href="http://news.mit.edu/2016/algorithm-robot-teams-moving-obstacles-0421">control</a> algorithms for autonomous robots make up a <a href="http://news.mit.edu/2015/algorithm-helps-robots-handle-uncertainty-0602">major</a> <a href="http://news.mit.edu/2014/collaborative-learning-for-robots-0625">area</a> of <a href="http://news.mit.edu/2011/robot-algorithm-0503">research</a> in computer science. But in the literature on multirobot systems, security has gotten relatively short shrift.</p> <p>In the latest issue of the journal <em>Autonomous Robots</em>, researchers from MIT’s Computer Science and Artificial Intelligence Laboratory and their colleagues present a new technique for preventing malicious hackers from commandeering robot teams’ communication networks. The technique could provide an added layer of security in systems that encrypt communications, or an alternative in circumstances in which encryption is impractical.</p> <p>“The robotics community has focused on making multirobot systems autonomous and increasingly more capable by developing the science of autonomy. In some sense we have not done enough about systems-level issues like cybersecurity and privacy,” says Daniela Rus, an Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science at MIT and senior author on the new paper.</p> <p>“But when we deploy multirobot systems in real applications, we expose them to all the issues that current computer systems are exposed to,” she adds. “If you take over a computer system, you can make it release private data — and you can do a lot of other bad things. A cybersecurity attack on a robot has all the perils of attacks on computer systems, plus the robot could be controlled to take potentially damaging action in the physical world. So in some sense there is even more urgency that we think about this problem.”</p> <p><strong>Identity theft</strong></p> <p>Most planning algorithms in multirobot systems rely on some kind of voting procedure to determine a course of action. Each robot makes a recommendation based on its own limited, local observations, and the recommendations are aggregated to yield a final decision.</p> <p>A natural way for a hacker to infiltrate a multirobot system would be to impersonate a large number of robots on the network and cast enough spurious votes to tip the collective decision, a technique called “spoofing.” The researchers’ new system analyzes the distinctive ways in which robots’ wireless transmissions interact with the environment, to assign each of them its own radio “fingerprint.” If the system identifies multiple votes as coming from the same transmitter, it can discount them as probably fraudulent.</p> <p>“There are two ways to think of it,” says Stephanie Gil, a research scientist in Rus’ Distributed Robotics Lab and a co-author on the new paper. “In some cases cryptography is too difficult to implement in a decentralized form. Perhaps you just don’t have that central key authority that you can secure, and you have agents continually entering or exiting the network, so that a key-passing scheme becomes much more challenging to implement. In that case, we can still provide protection.</p> <p>“And in case you can implement a cryptographic scheme, then if one of the agents with the key gets compromised, we can still provide &nbsp;protection by mitigating and even quantifying the maximum amount of damage that can be done by the adversary.”</p> <p><strong>Hold your ground</strong></p> <p>In their paper, the researchers consider a problem known as “coverage,” in which robots position themselves to distribute some service across a geographic area — communication links, monitoring, or the like. In this case, each robot’s “vote” is simply its report of its position, which the other robots use to determine their own.</p> <p>The paper includes a theoretical analysis that compares the results of a common coverage algorithm under normal circumstances and the results produced when the new system is actively thwarting a spoofing attack. Even when 75 percent of the robots in the system have been infiltrated by such an attack, the robots’ positions are within 3 centimeters of what they should be. To verify the theoretical predictions, the researchers also implemented their system using a battery of distributed Wi-Fi transmitters and an autonomous helicopter.</p> <p>“This generalizes naturally to other types of algorithms beyond coverage,” Rus says.</p> <p>The new system grew out of an earlier project involving Rus, Gil, Dina Katabi — who is the other Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science at MIT — and Swarun Kumar, who earned master’s and doctoral degrees at MIT before moving to Carnegie Mellon University. That project sought to use Wi-Fi signals to determine transmitters’ locations and to <a href="https://groups.csail.mit.edu/drl/wiki/images/8/88/Ijrr_2014_final.pdf">repair</a> <a href="http://news.mit.edu/2011/exp-ad-hoc-0310">ad hoc</a> communication networks. On the new paper, the same quartet of researchers is joined by MIT Lincoln Laboratory’s Mark Mazumder.</p> <p>Typically, radio-based location determination requires an array of receiving antennas. A radio signal traveling through the air reaches each of the antennas at a slightly different time, a difference that shows up in the phase of the received signals, or the alignment of the crests and troughs of their electromagnetic waves. From this phase information, it’s possible to determine the direction from which the signal arrived.</p> <p><strong>Space vs. time</strong></p> <p>A bank of antennas, however, is too bulky for an autonomous helicopter to ferry around. The MIT researchers found a way to make accurate location measurements using only <a href="http://groups.csail.mit.edu/drl/wiki/images/6/6a/sgil_mobicom2014.pdf">two antennas</a>, spaced about 8 inches apart. Those antennas must move through space in order to simulate measurements from multiple antennas. That’s a requirement that autonomous robots meet easily. In the experiments reported in the new paper, for instance, the autonomous helicopter hovered in place and rotated around its axis in order to make its measurements.</p> <p>When a Wi-Fi transmitter broadcasts a signal, some of it travels in a direct path toward the receiver, but much of it bounces off of obstacles in the environment, arriving at the receiver from different directions. For location determination, that’s a problem, but for radio fingerprinting, it’s an advantage: The different energies of signals arriving from different directions give each transmitter a distinctive profile.</p> <p>There’s still some room for error in the receiver’s measurements, however, so the researchers’ new system doesn’t completely ignore probably fraudulent transmissions. Instead, it discounts them in proportion to its certainty that they have the same source. The new paper’s theoretical analysis shows that, for a range of reasonable assumptions about measurement ambiguities, the system will thwart spoofing attacks without unduly punishing valid transmissions that happen to have similar fingerprints.</p> <p>“The work has important implications, as many systems of this type are on the horizon — networked autonomous driving cars, Amazon delivery drones, et cetera,” says David Hsu, a professor of computer science at the National University of Singapore. “Security would be a major issue for such systems, even more so than today’s networked computers. This solution is creative and departs completely from traditional defense mechanisms.”</p>
<p><b><a href="http://news.mit.edu/2017/security-multirobot-systems-hackers-0317" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2693</wp:post_id>
		<wp:post_date><![CDATA[2017-03-17 03:59:57]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-03-17 03:59:57]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[security-for-multirobot-systems]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/security-multirobot-systems-hackers-0317]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>SMART automation</title>
		<link>https://fifthlevel.ai/archives/2694</link>
		<pubDate>Thu, 19 Jan 2017 19:20:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/smart-automation-daniela-rus-0119</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Daniela Rus loves Singapore. As the MIT professor sits down in her Frank Gehry-designed office in Cambridge, Massachusetts, to talk about her research conducted in Singapore, her face starts to relax in a big smile.</p> <p>Her story with Singapore started in the summer of 2010, when she made her first visit to one of the most futuristic and forward-looking cities in the world. “It was love at first sight,” says the Andrew (1956) and Erna Viterbi Professor of Electrical Engineering and Computer Science and the director of MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL). That summer, she came to Singapore to join the Singapore-MIT Alliance for Research and Technology (SMART) as the first principal investigator in residence for the Future of Urban Mobility Research Program.</p> <p>“In 2010, nobody was talking about autonomous driving. We were pioneers in developing and deploying the first mobility on demand for people with self-driving golf buggies,” says Rus. “And look where we stand today! Every single car maker is investing millions of dollars to advance autonomous driving.&nbsp;Singapore did not hesitate to provide us, at an early stage, with all the financial, logistical, and transportation resources to facilitate our work.”</p> <p>Since her first visit, Rus has returned each year to follow up on the research, and has been involved in leading revolutionary projects&nbsp;for the future of urban mobility.&nbsp;“Our team worked tremendously hard on self-driving technologies, and we are now presenting a wide range of different devices that allow autonomous and secure mobility,” she says. “Our objective today is to make taking a driverless car for a spin as easy as programming a smartphone. A simple interaction between the human and machine will provide a transportation butler.”</p> <p>The first mobility devices her team worked on were self-driving golf buggies. Two years ago, these buggies advanced to a point where the group decided to open them to the public in a trial that lasted one week at the Chinese Gardens, an idea facilitated by Singapore’s Land and Transportation Agency (LTA). Over the course of a week, more than 500 people booked rides from the comfort of their homes, and came to the Chinese Gardens at the designated time and spot to experience mobility-on-demand with robots.</p> <p>The test was conducted around winding paths trafficked by pedestrians, bicyclists, and the occasional monitor lizard. The experiments also tested an online booking system that enabled visitors to schedule pickups and drop-offs&nbsp;around the garden, automatically routing and redeploying the vehicles to accommodate all the requests. The public’s response was joyful and positive, and this brought the team renewed enthusiasm to take the technology to the next level.</p> <p>Since the Chinese Gardens public trial, the autonomous car group has introduced a few other self-driving vehicles: a self-driving city car, and two personal mobility robots, a self-driving scooter and a self-driving wheelchair. Each of these vehicles was created in three phases: In the first phase, the vehicle was converted to drive-by-wire control, which allows a computer to control acceleration, braking, and steering of the car. In the second phase, the vehicle drives on each of the pathways in its operation environment and makes a map using features detected by the sensors. In the third phase, the vehicle uses the map to compute a path from the customer’s pick-up point to the customer’s drop-off point and proceeds to drive along the path, localizing continuously and avoiding any other cars, people, and unexpected obstacles. The devices also used traffic data from LTA to model traffic patterns and to study the benefits of ride sharing systems.</p> <p>Last April, the team conducted a new test with the public at MIT. This time, they deployed a self-driving scooter that allowed users to use the same autonomy system indoors as well as outdoors. The trial included autonomous rides in MIT’s Infinite Corridor. A significant challenge in this type of space is localization, or accurately knowing the location of the robot in a long and plain corridor that does not have many distinctive features. The system proved to work very well in this type of environment, and the trial completed the demonstration of a comprehensive uniform autonomous mobility system.</p> <p>“One can easily see the usefulness of such a system between self-driving city cars, golf buggies, and scooters,” Rus says. “A mobility-impaired user could, for example, use a self-driving scooter to get down the hall and through the lobby of an apartment building, take a self-driving golf buggy across the building's parking lot, and pick up an autonomous car on the public roads to go to a similarly equipped amusement park or shopping centre.”</p> <p>Daniela Rus, a Class of 2002 MacArthur Fellow and member of the USA National Academy of Engineering, knows that each successful step into urban mobility will bring a positive contribution of artificial intelligence to the public. According to the World Health Organization, 3,400 people die each day in the world from traffic-related accidents. “It is a new space race,” she says, convinced that autonomy is part of the solution to safe transportation. Daniela Rus will continue visiting her beloved Singapore, where she particularly enjoys the food, the beautiful flowers, the kindness of its people, and the smartness of its youth. “Singapore is definitely a model in many fields,” she concludes.</p>
<p><b><a href="http://news.mit.edu/2017/smart-automation-daniela-rus-0119" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2694</wp:post_id>
		<wp:post_date><![CDATA[2017-01-19 19:20:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-01-19 19:20:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[smart-automation]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/smart-automation-daniela-rus-0119]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>MIT Media Lab to participate in $27 million initiative on AI ethics and governance</title>
		<link>https://fifthlevel.ai/archives/2695</link>
		<pubDate>Tue, 10 Jan 2017 11:05:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/mit-media-lab-to-participate-in-ai-ethics-and-governance-initiative-0110</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The <a href="http://www.media.mit.edu/" target="_blank">MIT Media Lab</a> and the <a href="http://cyber.harvard.edu/" target="_blank">Berkman Klein Center for Internet and Society</a> at Harvard University will serve as the founding anchor institutions for a new initiative aimed at bridging the gap between the humanities, the social sciences, and computing by addressing the global challenges of artificial intelligence (AI) from a multidisciplinary perspective.</p> <p>“Artificial intelligence agents will impact every part of our lives in every society on Earth. Technology and commerce will see to that,” says Alberto Ibargüen, president and CEO of the <a href="http://www.knightfoundation.org/" target="_blank">John S. and James L. Knight Foundation</a>, which is among those <a href="http://kng.ht/2iWugNp" target="_blank">supporting the initiative</a>.</p> <p>Initially funded with $27 million from the Knight Foundation; LinkedIn co-founder Reid Hoffman; the <a href="http://www.omidyar.com/" target="_blank">Omidyar Network</a>; the William and Flora <a href="http://www.hewlett.org/" target="_blank">Hewlett Foundation</a>; and Jim Pallotta, founder of the Raptor Group, the Ethics and Governance of Artificial Intelligence Fund’s mission is to catalyze global research that advances AI for the public interest, with an emphasis on applied research and education. The fund will also seek to advance public understanding of AI.</p> <p>“AI’s rapid development brings along a lot of tough challenges,” explains <a href="http://www.media.mit.edu/people/joi/overview/" target="_blank">Joi Ito</a>, director of the MIT Media Lab. “For example, one of the most critical challenges is how do we make sure that the machines we ‘train’ don’t perpetuate and amplify the same human biases that plague society? How can we best initiate a broader, in-depth discussion about how society will co-evolve with this technology, and connect computer science and social sciences to develop intelligent machines that are not only ‘smart,’ but also socially responsible?”</p> <div class="cms-placeholder-content-video"></div> <p>What makes this new initiative different and necessary is that it’s aimed at transcending barriers and breaking down silos among disciplines. As founding academic institutions, the Media Lab and Berkman Klein Center, along with other potential collaborators from the public and private sectors, will act as a mechanism to reinforce cross-disciplinary work and encourage intersectional peer dialogue and collaboration.</p> <p>The fund — projected to operate with a phased approach over the next several years — will complement and collaborate with existing efforts and communities, such as the upcoming public symposium “<a href="http://artificialintelligencenow.com/schedule/conference" target="_blank">AI Now</a>,” which is scheduled for July 10 at the MIT Media Lab. The fund will also oversee an AI fellowship program, identify and provide support for collaborative projects, build networks out of the people and organizations currently working to steer AI in directions that help society, and also convene a “brain trust” of experts in the field.</p> <p><strong>A collaborative network</strong></p> <p>The Media Lab and the Berkman Klein Center for Internet and Society will leverage a network of faculty, fellows, staff, and affiliates who will collaborate on unbiased, sustained, evidenced-based, solution-oriented work that cuts across disciplines and sectors. This research will include questions that address society’s ethical expectations of AI, using machine learning to learn ethical and legal norms from data, and using data-driven techniques to quantify the potential impact of AI, for example, on the labor market.</p> <p>Work of this nature is already being undertaken at both institutions. The Media Lab has been exploring some of the moral complexities associated with autonomous vehicles in the Scalable Cooperation group, led by <a href="http://www.media.mit.edu/people/irahwan/overview/" target="_blank">Iyad Rahwan</a>. And the Personal Robots group, led by <a href="http://www.media.mit.edu/people/cynthiab/overview/" target="_blank">Cynthia Breazeal</a>, is investigating the ethics of human-robot interaction.</p> <p>“AI could be as big a disruptor to the world as the Industrial Revolution was in the 18th and 19th centuries,” Rahwan says. He cites transportation systems and employment as among the areas that will likely be affected by automation and AI. “What we need is something that puts our entire society in the control loop of these systems ... technologists, engineers, the public, ethicists, cognitive scientists, economists, legal scholars, anthropologists, faith leaders, government regulators — everyone crucial to protecting the public interest.”</p> <p>"Artificial Intelligence provides the potential for deeply personalized learning experiences for people of all ages and stages," says Breazeal, who emphasizes the need for AI to reach people in developing nations and underserved populations. But she adds that it is also “a kind of double-edged sword. What should it be learning and adapting to benefit you? And what should it do to protect your privacy and your security?”</p> <p><strong>Shared goals and governance</strong></p> <p>The Berkman Klein Center has been working to develop public interest-oriented solutions to many of the challenges of the digital age, incubating programs such as <a href="http://creativecommons.org/" target="_blank">Creative Commons</a> and the <a href="http://dp.la/" target="_blank">Digital Public Library of America</a>. And, it is currently collaborating with the Media Lab on the “<a href="http://berkmankleinassembly.org/" target="_blank">Assembly</a>” program, which gathers high-level developers and tech industry professionals for a rigorous three-week course at Harvard University, followed by a 12-week collaborative development period to explore difficult problems in cybersecurity.</p> <p>“The thread running through these otherwise disparate phenomena is a shift of reasoning and judgment away from people," says Jonathan Zittrain, co-founder of the Berkman Klein Center and professor of law and computer science at Harvard University. “Sometimes that's good, as it can free us up for other pursuits and for deeper undertakings. And sometimes it’s profoundly worrisome, as it decouples big decisions from human understanding and accountability. A lot of our work in this area will be to identify and cultivate technologies and practices that promote human autonomy and dignity rather than diminish it.”</p> <p>The Ethics and Governance of Artificial Intelligence Fund will be governed by a small board, consisting of leadership from each participating foundation and institution. In addition, the board is in the process of selecting a group of expert advisors, which would include Daniela Rus, Andrew and Erna Viterbi Professor Electrical Engineering and director of MIT's <a href="http://csail.mit.edu" target="_blank">Computer Science and Artificial Intelligence Laboratory</a> (CSAIL) along with Max Tegmark, professor of physics at MIT and co-founder of the <a href="http://futureoflife.org/" target="_blank">Future of Life Institute</a>, as well as a number of other individuals from a wide range of disciplines and organizations.</p> <p>Ito says that convening all perspectives now<em> </em>is essential. “Instead of setting up an institution, we've decided to create a dynamic network. There are a number of areas where the deployment of machine learning is already starting to pose questions that are best answered in an interdisciplinary way. This project that we're all embarking on is just the beginning.”</p>
<p><b><a href="http://news.mit.edu/2017/mit-media-lab-to-participate-in-ai-ethics-and-governance-initiative-0110" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2695</wp:post_id>
		<wp:post_date><![CDATA[2017-01-10 11:05:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-01-10 11:05:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[mit-media-lab-to-participate-in-27-million-initiative-on-ai-ethics-and-governance]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/mit-media-lab-to-participate-in-ai-ethics-and-governance-initiative-0110]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>How ride-sharing can improve traffic, save money, and help the environment</title>
		<link>https://fifthlevel.ai/archives/2696</link>
		<pubDate>Wed, 04 Jan 2017 21:00:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2016/how-ride-sharing-can-improve-traffic-save-money-and-help-environment-0104</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Traffic is not just a nuisance for drivers: It’s also a public&nbsp;health hazard and bad news for the economy.</p> <p>Transportation studies put the&nbsp;<a href="https://mobility.tamu.edu/ums/media-information/press-release/" target="_blank">annual cost of congestion at $160 billion</a>, which includes 7 billion hours of time lost to sitting in traffic and an extra 3 billion gallons of fuel burned.</p> <p>One way to improve traffic is through ride-sharing — and a <a href="http://www.pnas.org/content/early/2017/01/01/1611675114" target="_blank">new MIT study</a> suggests that using carpooling options from companies like Uber and Lyft could reduce the number of vehicles on the road by a factor of three without significantly impacting travel time.</p> <p>Led by Professor Daniela Rus, director of MIT’s <a href="http://csail.mit.edu" target="_blank">Computer Science and Artificial Intelligence Laboratory</a> (CSAIL), researchers developed an algorithm that found 3,000 four-passenger cars could serve 98 percent of taxi demand in New York City, with an average wait-time of only 2.7 minutes.</p> <p>“Instead of transporting people one at a time, drivers could transport two to four people at once, resulting&nbsp;in fewer trips, in less time, to make the same amount of money,” says Rus. “A system like this could allow drivers to work shorter shifts, while also creating less traffic, cleaner air, and shorter, less stressful commutes.”</p> <p>The team also found that 95 percent of demand would be covered by just 2,000 10-person vehicles, compared to the nearly 14,000 taxis that currently operate in New York City.</p> <p>Using data from 3 million taxi rides, the new algorithm works in real-time to reroute cars based on incoming requests, and can also proactively send idle cars to areas with high demand — a step that speeds up service 20 percent, according to Rus.</p> <p>“To our knowledge, this is the first time that scientists have been able to experimentally quantify the trade-off between fleet size, capacity, waiting time, travel delay, and operational costs for a range of vehicles, from taxis to vans and shuttles,” says Rus. “What’s more, the system is particularly suited to autonomous cars, since it can continuously reroute vehicles based on real-time requests.”</p> <p>Rus wrote an article about the work with former CSAIL postdoc Javier Alonso-Mora, Cornell University Assistant Professor Samitha Samaranayake, PhD student Alex Wallar, and MIT Professor Emilio Frazzoli. The article was published in this week’s issue of the <em>Proceedings of the National Academy of the Sciences.</em></p> <p>While the concept of carpooling has been around for decades, it’s only in the last two years that services such as Uber and Lyft have leveraged smartphone data in a way that has made ride-sharing a cheap, convenient option. (In 2015, Lyft reported that <a href="http://www.geekwire.com/2015/lyfts-carpooling-service-now-makes-up-50-of-rides-in-san-francisco-30-in-nyc/" target="_blank">half of its San Francisco trips are carpools</a>.)</p> <p>However, existing approaches are still limited in their complexity. For example, some ride-sharing systems require that user B be on the way for user A, and need to have all the requests submitted before they can create a route.</p> <p>In contrast, the new system allows requests to be rematched to different vehicles. It can also analyze a range of different types of vehicles to determine, say, where or when a 10-person van would be of the greatest benefit.</p> <p>The system works by first creating a graph of all of the requests and all of the vehicles. It then creates a second graph of all possible trip combinations, and uses a method called “integer linear programming” to compute the best assignment of vehicles to trips.</p> <p>After cars are assigned, the algorithm can then rebalance the remaining idle vehicles by sending them to higher-demand areas.</p> <p>“A key challenge was to develop a real-time solution that considers the thousands of vehicles and requests at once,” says Rus. “We can do this in our method because that first step enables us to understand and abstract the road network at a fine level of detail.”<em>&nbsp;&nbsp; &nbsp;</em></p> <p>The final product is what Rus calls an “anytime optimal algorithm,” which means that it gets better the more times you run it —&nbsp;and she says she’s eager to see how much it can improve with further refinement.</p> <p>“Ride-sharing services have enormous potential for positive societal impact with respect to congestion, pollution, and energy consumption,”&nbsp;Rus says. “It’s important that we as researchers do everything we can to explore ways to make these transportation systems as efficient and reliable as possible.”</p>
<p><b><a href="http://news.mit.edu/2016/how-ride-sharing-can-improve-traffic-save-money-and-help-environment-0104" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2696</wp:post_id>
		<wp:post_date><![CDATA[2017-01-04 21:00:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-01-04 21:00:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[how-ride-sharing-can-improve-traffic-save-money-and-help-the-environment]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2016/how-ride-sharing-can-improve-traffic-save-money-and-help-environment-0104]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Driverless platoons</title>
		<link>https://fifthlevel.ai/archives/2697</link>
		<pubDate>Wed, 21 Dec 2016 04:59:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2016/driverless-truck-platoons-save-time-fuel-1221</guid>
		<description></description>
		<content:encoded><![CDATA[<p>As driverless cars merge into our transportation system in the coming years, some researchers believe autonomous vehicles may save fuel by trailing each other in large platoons. Like birds and fighter jets flying in formation, or bikers and race car drivers drafting in packs, vehicles experience less aerodynamic drag when they drive close together.</p> <p>But assembling a vehicle platoon to deliver packages between distribution centers, or to transport passengers between stations, requires time. The first vehicle to arrive at a station must wait for others to show up before they can all leave as a platoon, creating inevitable delays.</p> <p>Now MIT engineers have studied a simple vehicle-platooning scenario and determined the best ways to deploy vehicles in order to save fuel and minimize delays. Their analysis, presented this week at the <em>International Workshop on the Algorithmic Foundations of Robotics,</em> shows that relatively simple, straightforward schedules may be the optimal approach for saving fuel and minimizing delays for autonomous vehicle fleets. The findings may also apply to conventional long-distance trucking and even ride-sharing services.</p> <p>“Ride-sharing and truck platooning, and even flocking birds and formation flight, are similar problems from a systems point of view,” says Sertac Karaman, the Class of 1948 Career Development Associate Professor of Aeronautics and Astronautics</p> <p>&nbsp;at MIT. “People who study these systems only look at efficiency metrics like delay and throughput. We look at those same metrics, versus sustainability such as cost, energy, and environmental impact. This line of research might really turn transportation on its head.”</p> <p>Karaman is a co-author of the paper, along with Aviv Adler, a graduate student in the Department of Electrical Engineering and Computer Science, and David Miculescu, a graduate student in the Department of Aeronautics and Astronautics.</p> <p><strong>Pushing through drag</strong></p> <p>Karaman says that for truck-driving — particularly over long distances — most of a truck’s fuel is spent on trying to overcome aerodynamic drag, that is, to push the truck through the surrounding air. Scientists have previously calculated that if several trucks were to drive just a few meters apart, one behind the other, those in the middle should experience less drag, saving fuel by as much as 20 percent, while the last truck should save 15 percent — slightly less, due to air currents that drag behind.</p> <p>If more vehicles are added to a platoon, more energy can collectively be saved. But there is a cost in terms of the time it takes to assemble a platoon.</p> <p>Karaman and his colleagues developed a mathematical model to study the effects of different scheduling policies on fuel consumption and delays. They modeled a simple scenario in which multiple trucks travel between two stations, arriving at each station at random times. The model includes two main components: a formula to represent vehicle arrival times, and another to predict the energy consumption of a vehicle platoon.&nbsp;</p> <p>The group looked at how arrival times and energy consumption changed under two general scheduling policies: a time-table policy, in which vehicles assemble and leave as a platoon at set times; and a feedback policy, in which vehicles assemble and leave as a platoon only when a certain number of vehicles are present — a policy that Karaman first experienced in Turkey.</p> <p>“I grew up in Turkey, where there are two types of public transportation buses: normal buses that go out at certain time units, and another set where the driver will sit there until the bus is full, and then will go,” Karaman says.</p> <p><strong>When to stay, when to go</strong></p> <p>In their modeling of vehicle platooning, the researchers analyzed many different scenarios under the two main scheduling policies. For example, to evaluate the effects of time-table scheduling, they modeled scenarios in which platoons were sent out at regular intervals — for instance, every five minutes — versus over more staggered intervals, such as every three and seven minutes. Under the feedback policy, they compared scenarios in which platoons were deployed once a certain number of trucks reached a station, versus sending three trucks out one time, then five trucks out the next time.</p> <p>Ultimately, the team found the simplest policies incurred the least delays while saving the most fuel. That is, time tables set to deploy platoons at regular intervals were more sustainable and efficient than those that deployed at more staggered times. Similarly, feedback scenarios that waited for the same number of trucks before deploying every time were more optimal than those that varied the number of trucks in a platoon.</p> <p>Overall, feedback policies were just slightly more sustainable than time-table policies, saving only 5 percent more fuel.</p> <p>“You’d think a more complicated scheme would save more energy and time,” Karaman says. “But we show in a formal proof that in the long run, it’s the simpler policies that help you.”</p> <p><strong>Ahead of the game</strong></p> <p>Karaman is currently working with trucking companies in Brazil that are interested in using the group’s model to determine how to deploy truck platoons to save fuel. He hopes to use data from these companies on when trucks enter highways to compute delay and energy tradeoffs with his mathematical model.</p> <p>Eventually, he says, the model may suggest that trucks &nbsp;follow each other at very close range, within 3 to 4 meters, which is difficult for a driver to maintain. Ultimately, Karaman says, truck platoons may require autonomous driving systems to kick in during long stretches of driving, to keep the platoon close enough together to save the most fuel.</p> <p>“There are already experimental trials testing autonomous trucks [in Europe],” Karaman says. “I imagine truck platooning is something we might see early in the [autonomous transportation] game.”</p> <p>The researchers are also applying their simulations to autonomous ride-sharing services. Karaman envisions a system of driverless shuttles that transport passengers between stations, at rates and times that depend on the overall system’s energy capacity and schedule requirements. The team’s simulations could determine, for instance, the optimal number of passengers per shuttle in order to save fuel or prevent gridlock.</p> <p>“We believe that ultimately this thinking will allow us to build new transportation systems in which the cost of transportation will be reduced substantially,” Karaman says.</p> <p>This research was funded, in part, by the National Science Foundation.</p>
<p><b><a href="http://news.mit.edu/2016/driverless-truck-platoons-save-time-fuel-1221" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2697</wp:post_id>
		<wp:post_date><![CDATA[2016-12-21 04:59:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-12-21 04:59:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[driverless-platoons]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2016/driverless-truck-platoons-save-time-fuel-1221]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Industry Leaders to Form Consortium for Network and Computing Infrastructure of Automotive Big Data</title>
		<link>https://fifthlevel.ai/archives/551</link>
		<pubDate>Thu, 10 Aug 2017 23:12:11 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://aecc.wpengine.com/?p=99</guid>
		<description></description>
		<content:encoded><![CDATA[<p>DENSO Corporation, Ericsson, Intel Corporation, Nippon Telegraph and Telephone Corporation (NTT), NTT DOCOMO, INC., Toyota InfoTechnology Center Co., Ltd. and Toyota Motor Corporation today announced that they have initiated the formation of the Automotive Edge Computing Consortium. The objective of the consortium is to develop an ecosystem for connected cars to support emerging services such &#91;...&#93;</p>
<p>The post <a rel="nofollow" href="https://aecc.org/industry-leaders-to-form-consortium-for-network-and-computing-infrastructure-of-automotive-big-data/">Industry Leaders to Form Consortium for Network and Computing Infrastructure of Automotive Big Data</a> appeared first on <a rel="nofollow" href="https://aecc.org">Automotive Edge Computing Consortium</a>.</p> <p><b><a href="https://aecc.org/industry-leaders-to-form-consortium-for-network-and-computing-infrastructure-of-automotive-big-data/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>551</wp:post_id>
		<wp:post_date><![CDATA[2017-08-10 23:12:11]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-08-10 23:12:11]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[industry-leaders-to-form-consortium-for-network-and-computing-infrastructure-of-automotive-big-data]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://aecc.org/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://aecc.org/industry-leaders-to-form-consortium-for-network-and-computing-infrastructure-of-automotive-big-data/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Autonomous vehicle development keeps creeping forward to a self-driving future</title>
		<link>https://fifthlevel.ai/archives/590</link>
		<pubDate>Mon, 17 Jul 2017 21:14:39 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.ipwatchdog.com/?p=85790</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Autonomous, or self-driving, vehicles are coming, even if it’s going to take some time for the technology to become fully operational on U.S. highways. When the technology catches up with the commercial demand, however, there’s little doubt that the market for autonomous vehicles will be huge. News reports from last June indicate that market research firm IHS Automotive published a report forecasting that almost 21 million driverless cars will be driven on roads across the world by the year 2035... Despite forays into the automotive world by Silicon Valley contenders like Tesla, Google’s Waymo and Uber, it seems that the coming generation of American research and development for self-driving cars will be centered in Detroit, long the center of the American automotive world.</p>
<p>The post <a rel="nofollow" href="http://www.ipwatchdog.com/2017/07/17/autonomous-vehicle-development-self-driving-future/id=85790/">Autonomous vehicle development keeps creeping forward to a self-driving future</a> appeared first on <a rel="nofollow" href="http://www.ipwatchdog.com">IPWatchdog.com | Patents &amp; Patent Law</a>.</p> <p><b><a href="http://www.ipwatchdog.com/2017/07/17/autonomous-vehicle-development-self-driving-future/id=85790/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>590</wp:post_id>
		<wp:post_date><![CDATA[2017-07-17 21:14:39]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-07-17 21:14:39]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[autonomous-vehicle-development-keeps-creeping-forward-to-a-self-driving-future]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/tag/autonomous-vehicles/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/2017/07/17/autonomous-vehicle-development-self-driving-future/id=85790/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Waymo drops three of four patent claims in its case against Uber</title>
		<link>https://fifthlevel.ai/archives/591</link>
		<pubDate>Tue, 11 Jul 2017 09:15:06 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.ipwatchdog.com/?p=85564</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In a joint stipulation and order entered three claims of patent infringement were dropped in the intellectual property case being fought between San Francisco, CA-based transportation company Uber Technologies and Waymo, one of the subsidiaries of Google-owner Alphabet Inc. The order is one of the most recent filings in a case which has seen hundreds of documents filed since the case began this February. The case is filed in the U.S. District Court for the Northern District of California (N.D. Cal.).</p>
<p>The post <a rel="nofollow" href="http://www.ipwatchdog.com/2017/07/11/waymo-drops-patent-claims-uber/id=85564/">Waymo drops three of four patent claims in its case against Uber</a> appeared first on <a rel="nofollow" href="http://www.ipwatchdog.com">IPWatchdog.com | Patents &amp; Patent Law</a>.</p> <p><b><a href="http://www.ipwatchdog.com/2017/07/11/waymo-drops-patent-claims-uber/id=85564/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>591</wp:post_id>
		<wp:post_date><![CDATA[2017-07-11 09:15:06]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-07-11 09:15:06]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[waymo-drops-three-of-four-patent-claims-in-its-case-against-uber]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/tag/autonomous-vehicles/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/2017/07/11/waymo-drops-patent-claims-uber/id=85564/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Waymo v. Uber: a Gordian Knot Gets Tighter</title>
		<link>https://fifthlevel.ai/archives/592</link>
		<pubDate>Thu, 15 Jun 2017 11:15:26 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.ipwatchdog.com/?p=84511</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In the annals of U.S. innovators, there are many infamous disputes between technology companies from Shockley and Fairchild in semiconductors to Microsoft and Apple in operating systems to today's high-profile lawsuit of Waymo vs. Uber in driverless car technology. What initially started as a trade secrets litigation has mushroomed into a high stakes game involving patent infringement, unfair competition, private arbitration, unlawful termination and the Fifth Amendment right against self-incrimination. It's a virtual Gordian Knot of legal entanglements.</p>
<p>The post <a rel="nofollow" href="http://www.ipwatchdog.com/2017/06/15/waymo-v-uber-gordian-knot-gets-tighter/id=84511/">Waymo v. Uber: a Gordian Knot Gets Tighter</a> appeared first on <a rel="nofollow" href="http://www.ipwatchdog.com">IPWatchdog.com | Patents &amp; Patent Law</a>.</p> <p><b><a href="http://www.ipwatchdog.com/2017/06/15/waymo-v-uber-gordian-knot-gets-tighter/id=84511/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>592</wp:post_id>
		<wp:post_date><![CDATA[2017-06-15 11:15:26]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-06-15 11:15:26]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[waymo-v-uber-a-gordian-knot-gets-tighter]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/tag/autonomous-vehicles/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/2017/06/15/waymo-v-uber-gordian-knot-gets-tighter/id=84511/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>US House unanimously approves measures to speed up autonomous car testing</title>
		<link>https://fifthlevel.ai/archives/757</link>
		<pubDate>Wed, 06 Sep 2017 20:03:44 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/17/09/06/us-house-unanimously-approves-measures-to-speed-up-autonomous-car-testing</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/22688-27746-smartcar-applecar-l.jpg" alt="Article Image" border="0" /> <br><br> The U.S. House of Representatives voted unanimously on Wednesday to approve a proposal that could accelerate the testing and deployment of self-driving cars, though not without safety concerns from some parties. <p><b><a href="https://appleinsider.com/articles/17/09/06/us-house-unanimously-approves-measures-to-speed-up-autonomous-car-testing" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>757</wp:post_id>
		<wp:post_date><![CDATA[2017-09-06 20:03:44]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-09-06 20:03:44]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[us-house-unanimously-approves-measures-to-speed-up-autonomous-car-testing]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/17/09/06/us-house-unanimously-approves-measures-to-speed-up-autonomous-car-testing]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[803]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Squad of Apple car staffers jump ship to self-driving startup Zoox</title>
		<link>https://fifthlevel.ai/archives/758</link>
		<pubDate>Wed, 30 Aug 2017 19:27:57 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/17/08/30/squad-of-apple-car-staffers-jump-ship-to-self-driving-startup-zoox</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/22596-27528-zoox3-l.jpg" alt="Article Image" border="0" /> <br><br> A group of 17 Apple engineers said to be working on Apple's automotive ambitions has reportedly left the company for self-driving company Zoox after Apple scaled back plans. <p><b><a href="https://appleinsider.com/articles/17/08/30/squad-of-apple-car-staffers-jump-ship-to-self-driving-startup-zoox" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>758</wp:post_id>
		<wp:post_date><![CDATA[2017-08-30 19:27:57]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-08-30 19:27:57]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[squad-of-apple-car-staffers-jump-ship-to-self-driving-startup-zoox]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/17/08/30/squad-of-apple-car-staffers-jump-ship-to-self-driving-startup-zoox]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[802]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple reportedly upgrades self-driving testbed with new LiDAR equipment, more</title>
		<link>https://fifthlevel.ai/archives/759</link>
		<pubDate>Fri, 25 Aug 2017 23:55:05 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/17/08/25/apple-reportedly-upgrades-self-driving-testbed-with-new-lidar-equipment-more</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/22542-27376-170825-Car-l.jpg" alt="Article Image" border="0" /> <br><br> A report on Friday claims an updated version of Apple's self-driving testbed has been seen driving on Silicon Valley roads, topped with a massive LiDAR array with integrated cameras, GPS and other equipment. <p><b><a href="https://appleinsider.com/articles/17/08/25/apple-reportedly-upgrades-self-driving-testbed-with-new-lidar-equipment-more" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>759</wp:post_id>
		<wp:post_date><![CDATA[2017-08-25 23:55:05]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-08-25 23:55:05]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-reportedly-upgrades-self-driving-testbed-with-new-lidar-equipment-more]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/17/08/25/apple-reportedly-upgrades-self-driving-testbed-with-new-lidar-equipment-more]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[801]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple&#039;s self-driving tech said to be aimed at ridehailing as hiring resumes</title>
		<link>https://fifthlevel.ai/archives/760</link>
		<pubDate>Thu, 24 Aug 2017 15:51:35 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/17/08/24/apples-self-driving-tech-said-to-be-aimed-at-ridehailing-as-hiring-resumes</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/22518-27315-didichuxing-iphone-l.jpg" alt="Article Image" border="0" /> <br><br> Apple's long-term goal with its self-driving project is likely the ridesharing/ridehailing market, and the associated team is said to be hiring again despite last year's layoffs and reorganization, according to a report. <p><b><a href="https://appleinsider.com/articles/17/08/24/apples-self-driving-tech-said-to-be-aimed-at-ridehailing-as-hiring-resumes" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>760</wp:post_id>
		<wp:post_date><![CDATA[2017-08-24 15:51:35]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-08-24 15:51:35]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apples-self-driving-tech-said-to-be-aimed-at-ridehailing-as-hiring-resumes]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/17/08/24/apples-self-driving-tech-said-to-be-aimed-at-ridehailing-as-hiring-resumes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[800]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Ford Demonstrates Autonomous Development Progress In Michigan</title>
		<link>https://fifthlevel.ai/archives/1157</link>
		<pubDate>Sat, 24 Jun 2017 20:06:11 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/149388/ford-fusion-autonomous-demonstration/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[The company has used Mcity's facility, which simulates urban environment. <p><b><a href="https://www.motor1.com/news/149388/ford-fusion-autonomous-demonstration/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1157</wp:post_id>
		<wp:post_date><![CDATA[2017-06-24 20:06:11]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-06-24 20:06:11]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[ford-demonstrates-autonomous-development-progress-in-michigan]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/videos/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/149388/ford-fusion-autonomous-demonstration/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Roborace Releases Video Of Devbot&#039;s Flying Lap At Speed</title>
		<link>https://fifthlevel.ai/archives/1158</link>
		<pubDate>Thu, 22 Jun 2017 15:33:43 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/149214/roborace-berlin-demonstration-video/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[The driverless racer hustles around the track impressively. We look forward to seeing a whole grid of them vying for a win. <p><b><a href="https://www.motor1.com/news/149214/roborace-berlin-demonstration-video/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1158</wp:post_id>
		<wp:post_date><![CDATA[2017-06-22 15:33:43]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-06-22 15:33:43]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[roborace-releases-video-of-devbots-flying-lap-at-speed]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/videos/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/149214/roborace-berlin-demonstration-video/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Chevy Launches Fleet Of 130 Self-Driving Bolt EVs</title>
		<link>https://fifthlevel.ai/archives/1159</link>
		<pubDate>Tue, 13 Jun 2017 19:05:11 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/148351/chevy-mass-producing-autonomous-bolt/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[Now nearly 200 autonomous Bolts in total will be hitting roads in the U.S. <p><b><a href="https://www.motor1.com/news/148351/chevy-mass-producing-autonomous-bolt/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1159</wp:post_id>
		<wp:post_date><![CDATA[2017-06-13 19:05:11]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-06-13 19:05:11]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[chevy-launches-fleet-of-130-self-driving-bolt-evs]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/videos/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/148351/chevy-mass-producing-autonomous-bolt/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Workshop: Self-driving cars – strategic implications for the auto industry</title>
		<link>https://fifthlevel.ai/archives/2480</link>
		<pubDate>Wed, 30 Aug 2017 18:58:27 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.driverless-future.com/?p=1089</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Please join us for this 1-day workshop on October 24 in Frankfurt, Germany or on November 2 in Auburn Hills, USA. The <a href="http://www.autelligence.com/event/self-driving-car-workshops-2/">workshop </a>examines the disruptive implications of self-driving car technology and the strategic consequences for the auto industry, its suppliers and related industries. The workshop will be led by Dr. Alexander Hars.</p>
<p><strong>Program highlights</strong></p>
<div id="stcpDiv">
<ul>
<li>The workshop begins with a review of the current state of the global, distributed innovation process related to self-driving cars, and examines the underlying technical, economic, legal and geopolitical factors upon which it depends.</li>
<li>Key implications for the mobility space will be discussed through an in-depth analysis of the many facets of the economics of self-driving mobility services.</li>
<li>We will examine how fully self-driving cars will affect different aspects of personal mobility – the propensity to use self-driving mobility services for local or long distance travel, the decision to purchase a car, buyer preferences for specific car models and features as well as the transition towards electric vehicles.</li>
<li>We will then focus on the various players in the SDC field, including leading OEMs, new entrants such as Google, Uber, key suppliers, including sensor and hardware providers as well as various governments, including the US, UK, Singapore, Japan and China.</li>
<li>We explore four potential strategic responses for the auto industry and discuss business models associated with self-driving vehicles and their suitability for the various players.</li>
<li>We review key implications for model mix, volume, as well as sales and design processes.</li>
</ul>
<p><strong>Who should attend?<br />
</strong>This workshop is intended for executives who need to think through the consequences of self-driving cars on the automotive sector. It offers frameworks and insights to help them develop their understanding and analysis of the threats and opportunities of SDCs for the industry.  It will help them to understand the implications of SDCs and to formulate appropriate strategies for their business.<strong><br />
</strong></p>
<p><strong>More information, event agenda and registration</strong><br />
This event is organized by <a href="http://www.autelligence.com/event/self-driving-car-workshops-2/">Autelligence</a>. Further details are available on <a href="http://www.autelligence.com/event/self-driving-car-workshops-2/">Autelligence site</a>.</p>
</div> <p><b><a href="http://www.driverless-future.com/?p=1089" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2480</wp:post_id>
		<wp:post_date><![CDATA[2017-08-30 18:58:27]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-08-30 18:58:27]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[workshop-self-driving-cars-strategic-implications-for-the-auto-industry]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?feed=rss2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?p=1089]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>New robot rolls with the rules of pedestrian conduct</title>
		<link>https://fifthlevel.ai/archives/2596</link>
		<pubDate>Wed, 30 Aug 2017 03:59:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/new-robot-rolls-rules-pedestrian-conduct-0830</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Just as drivers observe the rules of the road, most pedestrians follow certain social codes when navigating a hallway or a crowded thoroughfare: Keep to the right, pass on the left, maintain a respectable berth, and be ready to weave or change course to avoid oncoming obstacles while keeping up a steady walking pace.</p> <p>Now engineers at MIT have designed an autonomous robot with “socially aware navigation,” that can keep pace with foot traffic while observing these general codes of pedestrian conduct.</p> <p>In drive tests performed inside MIT’s Stata Center, the robot, which resembles a knee-high kiosk on wheels, successfully avoided collisions while keeping up with the average flow of pedestrians. The researchers have detailed their robotic design in a paper that they will present at the IEEE Conference on Intelligent Robots and Systems in September.</p> <p>“Socially aware navigation is a central capability for mobile robots operating in environments that require frequent interactions with pedestrians,” says Yu Fan “Steven” Chen, who led the work as a former MIT graduate student and is the lead author of the study. “For instance, small robots could operate on sidewalks for package and food delivery. Similarly, personal mobility devices could transport people in large, crowded spaces, such as shopping malls, airports, and hospitals.”</p> <p>Chen’s co-authors are graduate student Michael Everett, former postdoc Miao Liu, and Jonathan How, the Richard Cockburn Maclaurin Professor of Aeronautics and Astronautics at MIT.</p> <div class="cms-placeholder-content-video"></div> <p><strong>Social drive</strong></p> <p>In order for a robot to make its way autonomously through a heavily trafficked environment, it must solve four main challenges: localization (knowing where it is in the world), perception (recognizing its surroundings), motion planning (identifying the optimal path to a given destination), and control (physically executing its desired path).</p> <p>Chen and his colleagues used standard approaches to solve the problems of localization and perception. For the latter, they outfitted the robot with off-the-shelf sensors, such as webcams, a depth sensor, and a high-resolution lidar sensor. For the problem of localization, they used open-source algorithms to map the robot’s environment and determine its position. To control the robot, they employed standard methods used to drive autonomous ground vehicles.</p> <p>“The part of the field that we thought we needed to innovate on was motion planning,” Everett says. “Once you figure out where you are in the world, and know how to follow trajectories, which trajectories should you be following?”</p> <p>That’s a tricky problem, particularly in pedestrian-heavy environments, where individual paths are often difficult to predict. As a solution, roboticists sometimes take a trajectory-based approach, in which they program a robot to compute an optimal path that accounts for everyone's desired trajectories. These trajectories must be inferred from sensor data, because people don't explicitly tell the robot where they are trying to go.&nbsp;</p> <p>“But this takes forever to compute. Your robot is just going to be parked, figuring out what to do next, and meanwhile the person’s already moved way past it before it decides ‘I should probably go to the right,’” Everett says. “So that approach is not very realistic, especially if you want to drive faster.”</p> <p>Others have used faster, “reactive-based” approaches, in which a robot is programmed with a simple model, using geometry or physics, to quickly compute a path that avoids collisions.</p> <p>The problem with reactive-based approaches, Everett says, is the unpredictability of human nature — people rarely stick to a straight, geometric path, but rather weave and wander, veering off to greet a friend or grab a coffee. In such an unpredictable environment, such robots tend to collide with people or look like they are being pushed around by avoiding people excessively.</p> <p>&nbsp;“The knock on robots in real situations is that they might be too cautious or aggressive,” Everett says. “People don’t find them to fit into the socially accepted rules, like giving people enough space or driving at acceptable speeds, and they get more in the way than they help.”</p> <p><strong>Training days</strong></p> <p>The team found a way around such limitations, enabling the robot to adapt to unpredictable pedestrian behavior while continuously moving with the flow and following typical social codes of pedestrian conduct.</p> <p>They used reinforcement learning, a type of machine learning approach, in which they performed computer simulations to train a robot to take certain paths, given the speed and trajectory of other objects in the environment. The team also incorporated social norms into this offline training phase, in which they encouraged the robot in simulations to pass on the right, and penalized the robot when it passed on the left.</p> <p>“We want it to be traveling naturally among people and not be intrusive,” Everett says. “We want it to be following the same rules as everyone else.”</p> <p>The advantage to reinforcement learning is that the researchers can perform these training scenarios, which take extensive time and computing power, offline. Once the robot is trained in simulation, the researchers can program it to carry out the optimal paths, identified in the simulations, when the robot recognizes a similar scenario in the real world.</p> <p>The researchers enabled the robot to assess its environment and adjust its path, every one-tenth of a second. In this way, the robot can continue rolling through a hallway at a typical walking speed of 1.2 meters per second, without pausing to reprogram its route.</p> <p>“We’re not planning an entire path to the goal — it doesn’t make sense to do that anymore, especially if you’re assuming the world is changing,” Everett says. “We just look at what we see, choose a velocity, do that for a tenth of a second, then look at the world again, choose another velocity, and go again. This way, we think our robot looks more natural, and is anticipating what people are doing.”</p> <p><strong>Crowd control</strong></p> <p>Everett and his colleagues test-drove the robot in the busy, winding halls of MIT’s Stata Building, where the robot was able to drive autonomously for 20 minutes at a time. It rolled smoothly with the pedestrian flow, generally keeping to the right of hallways, occasionally passing people on the left, and avoiding any collisions.</p> <p>“We wanted to bring it somewhere where people were doing their everyday things, going to class, getting food, and we showed we were pretty robust to all that,” Everett says. “One time there was even a tour group, and it perfectly avoided them.”</p> <p>Everett says going forward, he plans to explore how robots might handle crowds in a pedestrian environment.</p> <p>“Crowds have a different dynamic than individual people, and you may have to learn something totally different if you see five people walking together,” Everett says. “There may be a social rule of, ‘Don’t move through people, don’t split people up, treat them as one mass.’ That’s something we’re looking at in the future.”</p> <p>This research was funded by Ford Motor Company. &nbsp;</p> <p><b><a href="http://news.mit.edu/2017/new-robot-rolls-rules-pedestrian-conduct-0830" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2596</wp:post_id>
		<wp:post_date><![CDATA[2017-08-30 03:59:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-08-30 03:59:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[new-robot-rolls-with-the-rules-of-pedestrian-conduct]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/new-robot-rolls-rules-pedestrian-conduct-0830]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Drones relay RFID signals for inventory control</title>
		<link>https://fifthlevel.ai/archives/2597</link>
		<pubDate>Fri, 25 Aug 2017 04:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/drones-relay-rfid-signals-inventory-control-0825</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Radio frequency ID tags were supposed to revolutionize supply chain management. The dirt-cheap, battery-free tags, which receive power wirelessly from scanners and then broadcast identifying numbers, enable warehouse managers to log inventory much more efficiently than they could by reading box numbers and recording them manually.</p> <p>But the scale of modern retail operations makes even radio frequency ID (RFID) scanning inefficient. Walmart, for instance, reported that in 2013 it lost $3 billion in revenue because of mismatches between its inventory records and its stock. Even with RFID technology, it can take a single large retail store three months to perform a complete inventory review, which means that mismatches often go undiscovered until exposed by a customer request.</p> <p>MIT researchers have now developed a system that enables small, safe, aerial drones to read RFID tags from tens of meters away while identifying the tags’ locations with an average error of about 19 centimeters. The researchers envision that the system could be used in large warehouses for both continuous monitoring, to prevent inventory mismatches, and location of individual items, so that employees can rapidly and reliably meet customer requests.</p> <div class="cms-placeholder-content-video"></div> <p>The central challenge in designing the system was that, with the current state of autonomous navigation, the only drones safe enough to fly within close range of humans are small, lightweight drones with plastic rotors, which wouldn’t cause injuries in the event of a collision. But those drones are too small to carry RFID readers with a range of more than a few centimeters.</p> <p>The researchers met this challenge by using the drones to relay signals emitted by a standard RFID reader. This not only solves the safety problem but also means that drones could be deployed in conjunction with existing RFID inventory systems, without the need for new tags, readers, or reader software.</p> <p>“Between 2003 and 2011, the U.S. Army lost track of $5.8 billion of supplies among its warehouses,” says Fadel Adib, the Sony Corporation Career Development Assistant Professor of Media Arts and Sciences, whose group at the MIT Media Lab developed the new system. “In 2016, the U.S. National Retail Federation reported that shrinkage — loss of items in retail stores — averaged around $45.2 billion annually. By enabling drones to find and localize items and equipment, this research will provide a fundamental technological advancement for solving these problems.”</p> <p>The MIT researchers describe their system, dubbed RFly, in a paper they presented this week at the annual conference of the Association for Computing Machinery's Special Interest Group on Data Communications. Adib is the senior author on the paper, and he’s joined by Yunfei Ma, a postdoc in the Media Lab, and Nicholas Selby, an MIT graduate student in mechanical engineering.</p> <p><strong>Phase shift</strong></p> <p>Relaying RFID signals and using them to determine tags’ locations poses some thorny signal-processing problems. One is that, because the RFID tag is powered wirelessly by the reader, the reader and the tag transmit simultaneously at the same frequency. A relay system adds another pair of simultaneous transmissions: two between the relay and the tag and two between the relay and the reader. That’s four simultaneous transmissions at the same frequency, all interfering with each other.</p> <p>This problem is compounded by the requirement that the system determine the location of the RFID tag. The location-detection — or “localization” — system uses a variation on a device called an antenna array. If several antennas are clustered together, a signal broadcast toward them at an angle will reach each antenna at a slightly different time. That means that the signals detected by the antennas will be slightly out of phase: The troughs and crests of their electromagnetic waves won’t coincide perfectly. From those phase differences, software can deduce the angle of transmission and thus the location of the transmitter.</p> <p>The drone is too small to carry an array of antennas, but it is continuously moving, so readings it takes at different times are also taken at different locations, simulating the multiple antenna elements of an array.</p> <p>Ordinarily, to combat interference, the drone would digitally decode the transmission it receives from the tag and re-encode it for transmission to the reader. But in this case, the delays imposed by the decoding-encoding process would change the signals’ relative phases, making it impossible to accurately gauge location.</p> <p>All radio systems encode information by modulating a base transmission frequency, usually by shifting it slightly up and down. But because an RFID tag has no independent power source, its modulations are detectably smaller than those of the reader. So the MIT researchers devised an analog filter that would subtract the base transmission frequency from the signals that reach the reader and then separate the low-frequency and high-frequency components. The low-frequency component — the signal from the tag — is then added back onto the base frequency.</p> <p><strong>Frame of reference</strong></p> <p>At this point, however, another problem still remains. Because the drone is moving, the phase shift of the signals that reach the reader result from not only the drone’s position relative to the RFID tag but also its position relative to the reader. On the basis of the received signal alone, the reader has no way to tell how much each of those two factors contributed to the total phase shift.</p> <p>So the MIT researchers also equip each of their drones with its own RFID tag. A drone alternates between relaying the reader’s signal to a tagged item and simply letting its own tag reflect the signal back, so that the reader can estimate the drone’s contribution to the total phase shift and remove it.</p> <p>In experiments in the Media Lab that involved tagged objects, many of which were intentionally hidden to approximate the condition of merchandise heaped in piles on warehouse shelves, the system was able to localize the tags with 19-centimeter accuracy while extending the range of the reader tenfold in all directions, or one hundredfold cumulatively. The researchers are currently conducting a second set of experiments in the warehouse of a major Massachusetts retailer.</p> <p>“Relays have been used in communications for a long time, even to bring networks to rural areas,” says Swarun Kumar, an assistant professor of electrical and computer engineering at Carnegie Mellon University. “What changes here is that one of the ends is battery-free, and they want to location-track the battery-free device, which requires phase-consistent measurements. These together make the problem quite challenging. That’s what I think is the conceptual novelty in this work. I anticipate that there might be a lot more applications than the inventory tracking problem — which in and of itself is quite important.”</p>
<p><b><a href="http://news.mit.edu/2017/drones-relay-rfid-signals-inventory-control-0825" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2597</wp:post_id>
		<wp:post_date><![CDATA[2017-08-25 04:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-08-25 04:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[drones-relay-rfid-signals-for-inventory-control]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/drones-relay-rfid-signals-inventory-control-0825]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Hybrid drones carry heavier payloads for greater distances</title>
		<link>https://fifthlevel.ai/archives/2598</link>
		<pubDate>Fri, 04 Aug 2017 18:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/hybrid-drones-carry-heavier-payloads-greater-distances-0804</guid>
		<description></description>
		<content:encoded><![CDATA[<p>MIT alumnus Long Phan SM ’99, PhD ’12 is a technology innovator and entrepreneur with several engineering “firsts” under his belt.</p> <p>In the mid-1990s, Phan helped build the Draper Small Autonomous Aerial Vehicle, the world’s first fully autonomous helicopter. While working on Wall Street in the early 2000s, he became an early pioneer of the high-frequency trading system, which consists of powerful computers that rapidly complete tons of trading transactions.</p> <p>As co-founder, CEO, and chief technology officer of Top Flight Technologies, Phan is now one of the first entrepreneurs to commercialize hybrid gas-to-electric drones. The drones offer an order-of-magnitude increase in range, payload size, and power over battery-powered counterparts.</p> <p>Coming to market this fall, the hybrid drones could help make drone package-delivery a reality, and enhance capabilities for crop imaging, military surveillance, emergency response, and remote infrastructure inspection, among other applications. As the startup continues to develop hybrid drone power sources, the technology could also pave the way for human flight.</p> <p>“The key is having an abundance of power and total energy. That’s what petrol and&nbsp; gasoline gives you,” Phan says. “Using a high-energy-density energy source like gasoline, and converting it to electric power, and doing it efficiently, gives you the equivalent of a ‘super battery.’”</p> <p>Many drones run on batteries, flying for 15 to 30 minutes between charges, with maximum payloads of 5 pounds. Top Flight’s drone can fly for more than 2.5 hours&nbsp;­— enabling ranges of up to 100 miles — while carrying up to 20 pounds.</p> <p>The drone can be customized for any number of industrial-strength applications. The engine weighs about 17 pounds and can generate up to 10 kilowatts of power. It uses gasoline to generate the power that drives the lift motors, keeps backup batteries charged, and powers onboard electronics including computing, sensors, and communications equipment. The onboard batteries never need recharging; users just need to refill the gas tank and fly again. Flight control can operate in fully-or semi-autonomous modes.</p> <p>With the hiring of several MIT alumni, the startup is quietly developing a 100-kilowatt hybrid drone that can lift 100 kilograms — enough to carry a human or two — for up to three hours. NASA, Uber, and many aerospace companies worldwide are currently working on building air taxis, small autonomous planes that will shuttle people around in big cities. But, Phan says, these can stay airborne for only about 10 minutes. Top Flight’s technologies will make them more practical for hauling people from hub to hub.</p> <p>“With a 100-kilowatt hybrid electric engine, concepts like air taxis become viable,” he says. “By 2020, you may see a drone fly a person.”</p> <p><strong>“A Toyota Prius for the sky”</strong></p> <p>Top Flight’s story began in the late 2000s, when Phan was recalled to MIT twice to solve different engineering problems — both times leading to startups.</p> <p>In 2009, Phan’s former advisor Sanjay Sarma, now the Fred Fort Flowers and Daniel Fort Flowers Professor in Mechanical Engineering and vice president for open learning, asked him to enroll in a PhD program to work on wide area thermal imaging. Phan’s research became a core of Phan and Sarma’s startup <a href="http://news.mit.edu/2015/startup-essess-heat-mapping-cars-0105">Essess</a>, which deploys cars with thermal-imaging rooftop rigs that create heat maps of homes and buildings to detect energy leaks.</p> <p>In 2014, Robert Shin, head of the Intelligence, Surveillance, Reconnaissance and Tactical Systems Division at MIT Lincoln Laboratory, approached Phan and asked him to help solve the payload and endurance problems for drones.</p> <p>Phan and other MIT researchers took a shot at the problem by conceptualizing and designing microscale hybrid electric-gas engines for drones. “We said, ‘What if we build a Toyota Prius for the sky?’” Phan says, laughing.</p> <p>Hybrid electric engines are easier to build in cars, because, among other things, there are fewer weight and volume restraints. Engines on drones must be small and lightweight while delivering the same amount of power. This produces major technical challenges with excessive vibration and heat. “Often the engine will literally melt because you’re running it so hot,” Phan says.</p> <p>Using various heat transfer and control techniques — such as strategically incorporating small fans, cooling fins, and rubber vibration dampeners — the team solved those issues and initially slapped a prototype hybrid engine on a generic drone. Their calculations predicted the hybrid drone would fly for an hour — but it flew for nearly 2.5 hours.</p> <p>“The lightbulb went off,” Phan says. “We were like, ‘What else can you do with a drone that can fly for hours?’”</p> <p>Phan founded the startup in 2014, along with Sarma and other MIT engineers, and set up operations in a remote-controlled helicopter hobby shop in Malden, Massachusetts, before opening a separate headquarters in that city in 2016. A couple of funding rounds pushed them past $2 million of early venture funding by 2015.</p> <p>Over the past several years, Top Flight has continued to develop major innovations for the microscale hybrid engine concept, called a “digital gearbox.” Engines for vertical takeoff aircraft, such as helicopters, are complex and difficult to manage, consisting of thousands of mechanical parts. Top Flight’s digital gearbox behaves like those systems but uses electricity to control everything. Gasoline runs to a small generator, creating electric power, which the digital gearbox controls and sends in pulses to the electric motors and electronics. This makes the powering flight much simpler and more efficient, Phan says.</p> <p>“By pulsing the electricity to the motors, we can control the amount of torque and revolutions per minute of the motor,” Phan says. “We can … achieve the same benefits as a traditional mechanical transmission system, but it’s much more efficient, cost-effective, and scalable.”</p> <p><strong>Cruising in agile aerospace</strong></p> <p>Today, Top Flight operates in what it calls “agile aerospace 2.0,” a term representing the valuable vertical range for drones and microsatellites starting from the ground level and rising to 400 feet. Flying closer to the ground means greatly enhanced imaging and sensing resolutions, and other capabilities, such as communications. “If you go outside today, there’s virtually nothing happening in agile aerospace,” Phan says. “But it makes the most sense [for] air taxis or inspecting power lines, or doing logistics or delivery.”</p> <p>Immediate applications for Top Flight’s drone capabilities may include inspecting infrastructure in remote areas. Some U.S. utilities companies are already tasking drones with inspecting power lines and pipelines that go without routine inspection due to their remote locations. Top Flight’s drones could greatly increase the range of those drones while reducing costs and improving worker safety. They could also help pre- and post-disaster recovery efforts by surveying damage to the networks after natural disasters.</p> <p>As for delivery drones, Phan says Top Flight can increase the overall value related to increased range. Amazon, Google, UPS, and other large international firms are developing drone-based solutions that can deliver packages to consumer doorsteps. But they’re restricted to carrying, say, a single textbook and maybe 30 minutes of battery life, limiting their range.</p> <p>“By increasing the range by an order of magnitude, you can capture 100 times more value, due to the increased area coverage, compared to traditional battery drone systems,” Phan says. “[Delivery drones] are not just a gimmick. They’re very feasible soon.”</p> <p>Top Flight’s drones also hold promise for improved military missions, Phan says. A flock of 1,000 small drones could be deployed for longer times to gather reconnaissance data at a cost similar that of a single large military aircraft.</p> <p>When Top Flight completes its 100-kilowatt hybrid electric engine, that same concept could also be used to haul, say, barrels of oil, divided into smaller amounts for military convoys in dangerous zones. Generally, this type of shipping is expensive and hazardous due to transportation costs and various risks on the road. “Instead of carrying really big loads in the tons, you use many drones to carry small loads in the 100-kilogram increments, like a pack of mules,” Phan says.</p> <p>Currently, Top Flight uses an internal combustion engine in its microscale hybrid power systems. Moving forward, the company aims to hybridize gas turbine engines, which are used to power jets and helicopters. “Heat and vibration issues will be magnified, but at the same time they’re much more powerful and almost 100 percent more energy efficient than comparably-sized internal combustion engines,” Phan says. “That’s our next challenge.”</p>
<p><b><a href="http://news.mit.edu/2017/hybrid-drones-carry-heavier-payloads-greater-distances-0804" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2598</wp:post_id>
		<wp:post_date><![CDATA[2017-08-04 18:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-08-04 18:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[hybrid-drones-carry-heavier-payloads-for-greater-distances]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/hybrid-drones-carry-heavier-payloads-greater-distances-0804]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Featured video: A self-driving wheelchair</title>
		<link>https://fifthlevel.ai/archives/2599</link>
		<pubDate>Wed, 26 Jul 2017 19:00:53 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/featured-video-self-driving-wheelchair-0726</guid>
		<description></description>
		<content:encoded><![CDATA[<div class="cms-placeholder-content-video"></div> <p>Singapore and MIT have been at the forefront of autonomous vehicle development. First, there were self-driving <a href="http://news.mit.edu/2013/smart-driverless-golf-cart-provides-a-glimpse-into-a-future-of-autonomous-vehicles" target="_blank">golf buggies</a>. Then, an autonomous <a href="http://news.mit.edu/2016/startup-nutonomy-driverless-taxi-service-singapore-0324" target="_self">electric car</a>. Now, leveraging similar technology, MIT and Singaporean researchers have developed and deployed a self-driving wheelchair at a hospital.&nbsp;</p> <p>Spearheaded by Daniela Rus,&nbsp;the Andrew (1956) and Erna Viterbi Professor of Electrical Engineering and Computer Science and director of MIT’s Computer Science and Artificial Intelligence Laboratory, this autonomous wheelchair is an extension of the&nbsp;<a href="http://news.mit.edu/2016/driverless-scooters-1107" target="_blank">self-driving scooter&nbsp;</a>that launched at MIT last year — and it is a testament to the success of&nbsp;the Singapore-MIT Alliance for Research and Technology, or <a href="https://smart.mit.edu/" target="_blank">SMART</a>, a collaboration between researchers at MIT and in Singapore.</p> <p>Rus, who is also the principal investigator of the SMART Future Urban Mobility research group, says this newest innovation can help nurses focus more on patient care as they can get relief from logistics work which includes searching for wheelchairs and wheeling patients in the complex hospital network.</p> <p>"When we visited several retirement communities, we realized that the quality of life is dependent on mobility. We&nbsp;want to make it really easy for people to move around," Rus says.</p> <p><em>Submitted by: Pauline Teo/SMART | Video by: SMART | 3 min, 3 sec</em></p>
<p><b><a href="http://news.mit.edu/2017/featured-video-self-driving-wheelchair-0726" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2599</wp:post_id>
		<wp:post_date><![CDATA[2017-07-26 19:00:53]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-07-26 19:00:53]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[featured-video-a-self-driving-wheelchair]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/featured-video-self-driving-wheelchair-0726]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Lincoln Laboratory enters licensing agreement to produce its localizing ground-penetrating radar</title>
		<link>https://fifthlevel.ai/archives/2600</link>
		<pubDate>Tue, 18 Jul 2017 18:05:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/lincoln-laboratory-enters-licensing-agreement-to-produce-localizing-ground-penetrating-radar-0718</guid>
		<description></description>
		<content:encoded><![CDATA[<p>MIT has reached agreement with Geophysical Survey Systems, Inc. (GSSI) to develop commercial prototypes of a technology that helps autonomous vehicles navigate by using subsurface geology. Engineers at MIT Lincoln Laboratory, who developed <a href="http://news.mit.edu/2016/pinpointing-vehicles-with-high-precision-under-adverse-weather-conditions-0623" target="_self">localizing ground-penetrating radar (LGPR)</a>, have demonstrated that features in soil layers, rocks, and road bedding can be used to localize vehicles to centimeter-level accuracy. The LGPR has been used for lane keeping even when snow, fog, or dust obscures aboveground features.</p> <p>GSSI will build and sell the prototype LGPR systems. While developers of self-driving cars are likely the initial customers, companies providing equipment and services for trucking, construction, mining, and agriculture may also find interest in LGPR capabilities.</p> <p>"This technology could significantly impact the self-driving vehicle industry," says Byron Stanley, the lead researcher on the LGPR program. "Most autonomous vehicles rely on optical systems that 'see' road surfaces and surrounding infrastructure to localize themselves. Optical systems work well in fair weather conditions, but it is challenging and risky for them to work when snow covers lane markings and road surfaces or fog obscures points of reference. Even in fair conditions, having an independent sensor to rely on when your optics aren't working could add several orders of magnitude to the reliability of current autonomous lane keeping systems. This technology can save lives."</p> <p>The LGPR sensor uses high-frequency radar reflections of underground features to generate a baseline map of a road's subsurface. The idea is that whenever an LGPR vehicle drives along a road, the data can be used as a reference map. An LGPR vehicle on subsequent passes compares its current map against the reference map. The reference map can be correlated with the current map to create an estimate of the vehicle's location. This localization has been <a href="http://news.mit.edu/2016/pinpointing-vehicles-with-high-precision-under-adverse-weather-conditions-0623" target="_self">demonstrated to be accurate</a> to within a few centimeters, in real-time and at highway speeds, even at night in snow storms.</p> <p>During the 2017 Automated Vehicles Symposium held July 11-13 in San Francisco, Stanley and David Cist, vice president of R&amp;D at GSSI, showcased the LGPR concept, long-term map stability, and capabilities in a poster session on July 11 and in a "deep dive" discussion session the next afternoon. The annual Automated Vehicles Symposium is the world's largest meeting dedicated to issues in vehicle autonomy. Leading researchers and developers of vehicle automation from industry, government, and academia address the technology innovations, public policy, and human factors affecting progress toward safe vehicle automation.</p> <p>Stanley and his team are working with GSSI to study the long-term stability of the subterranean maps. Evidence so far shows that the deep subsurface features mapped by LGPR should be relatively immune to aboveground changes that can compromise optical sensors. Assessments of LGPR's accuracy over six- and 12-month periods show that the maps of primary roads remain valid; less stable are maps of some minor roads whose subsurfaces may be degraded by poor drainage. These results suggest that the underground mapping can be done once, with updates required only for the maps of some less-traveled roads or after road construction. Cist confirms these results from several decades of GPR testing: "For many years, our final validation of all antennas has been to run the same test path over the same road outside our facilities. Although our data show seasonal variability, the results clearly remain stable over decades."</p> <p>There are several ways that LGPR complements most sensors guiding self-driving vehicles:</p> <ul> <li>It is robust under conditions that pose difficulties for GPS, lidar, or camera sensors (e.g., in tunnels, canyons, snow, ice, fog, dust, dirt, lighting changes, and dynamic environments);</li> <li>the independence of the LGPR to changes to and dynamics of the aboveground environment — where landmarks are torn down or obscured, road markings fade, and signs are moved — provides added assurance of localization; and</li> <li>adding stable subsurface mapping reduces the need for continual modifications to high-resolution road maps.</li>
</ul> <p>Because of its simple design, the LGPR could be mass-produced for $300 or less. As a low-cost addition to sensor suites, LGPR will make autonomous vehicles safer and more capable.</p>
<p><b><a href="http://news.mit.edu/2017/lincoln-laboratory-enters-licensing-agreement-to-produce-localizing-ground-penetrating-radar-0718" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2600</wp:post_id>
		<wp:post_date><![CDATA[2017-07-18 18:05:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-07-18 18:05:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[lincoln-laboratory-enters-licensing-agreement-to-produce-its-localizing-ground-penetrating-radar]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/lincoln-laboratory-enters-licensing-agreement-to-produce-localizing-ground-penetrating-radar-0718]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Miniaturizing the brain of a drone</title>
		<link>https://fifthlevel.ai/archives/2601</link>
		<pubDate>Wed, 12 Jul 2017 03:59:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/miniaturizing-brain-smart-drones-0712</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In recent years, engineers have worked to shrink drone technology, building flying prototypes that are the size of a bumblebee and loaded with even tinier sensors and cameras. Thus far, they have managed to miniaturize almost every part of a drone, except for the brains of the entire operation — the computer chip.</p> <p>Standard computer chips for quadcoptors and other similarly sized drones process an enormous amount of streaming data from cameras and sensors, and interpret that data on the fly to autonomously direct a drone’s pitch, speed, and trajectory. To do so, these computers use between 10 and 30 watts of power, supplied by batteries that would weigh down a much smaller, bee-sized drone.</p> <p>Now, engineers at MIT have taken a first step in designing a computer chip that uses a fraction of the power of larger drone computers and is tailored for a drone as small as a bottlecap. They will present a new methodology and design, which they call “Navion,” at the Robotics: Science and Systems conference, held this week at MIT.</p> <p>The team, led by Sertac Karaman, the Class of 1948 Career Development Associate Professor of Aeronautics and Astronautics at MIT, and Vivienne Sze, an associate professor in MIT's Department of Electrical Engineering and Computer Science, developed a low-power algorithm, in tandem with pared-down hardware, to create a specialized computer chip.</p> <p>The key contribution of their work is a new approach for designing the chip hardware and the algorithms that run on the chip. “Traditionally, an algorithm is designed, and you throw it over to a hardware person to figure out how to map the algorithm to hardware,” Sze says. “But we found by designing the hardware and algorithms together, we can achieve more substantial power savings.”</p> <p>“We are finding that this new approach to programming robots, which involves thinking about hardware and algorithms jointly, is key to scaling them down,” Karaman says.</p> <p>The new chip processes streaming images at 20 frames per second and automatically carries out commands to adjust a drone’s orientation in space. The streamlined chip performs all these computations while using just below 2 watts of power — making it an order of magnitude more efficient than current drone-embedded chips.</p> <p>Karaman, says the team’s design is the first step toward engineering “the smallest intelligent drone that can fly on its own.” He ultimately envisions disaster-response and search-and-rescue missions in which insect-sized drones flit in and out of tight spaces to examine a collapsed structure or look for trapped individuals. Karaman also foresees novel uses in consumer electronics.</p> <p>“Imagine buying a bottlecap-sized drone that can integrate with your phone, and you can take it out and fit it in your palm,” he says. “If you lift your hand up a little, it would sense that, and start to fly around and film you. Then you open your hand again and it would land on your palm, and you could upload that video to your phone and share it with others.”</p> <p>Karaman and Sze’s co-authors are graduate students Zhengdong Zhang and Amr Suleiman, and research scientist Luca Carlone.</p> <p><strong>From the ground up</strong></p> <p>Current minidrone prototypes are small enough to fit on a person’s fingertip and are extremely light, requiring only 1 watt of power to lift off from the ground. Their accompanying cameras and sensors use up an additional half a watt to operate.</p> <p>“The missing piece is the computers — we can’t fit them in terms of size and power,” Karaman says. “We need to miniaturize the computers and make them low power.”</p> <p>The group quickly realized that conventional chip design techniques would likely not produce a chip that was small enough and provided the required processing power to intelligently fly a small autonomous drone.</p> <p>“As transistors have gotten smaller, there have been improvements in efficiency and speed, but that’s slowing down, and now we have to come up with specialized hardware to get improvements in efficiency,” Sze says.</p> <p>The researchers decided to build a specialized chip from the ground up, developing algorithms to process data, and hardware to carry out that data-processing, in tandem.</p> <p><strong>Tweaking a formula</strong></p> <p>Specifically, the researchers made slight changes to an existing algorithm commonly used to determine a drone’s “ego-motion,” or awareness of its position in space. They then implemented various versions of the algorithm on a field-programmable gate array (FPGA), a very simple programmable chip. To formalize this process, they developed a method called iterative splitting co-design that could strike the right balance of achieving accuracy while reducing the power consumption and the number of gates.</p> <p>A typical FPGA consists of hundreds of thousands of disconnected gates, which researchers can connect in desired patterns to create specialized computing elements. Reducing the number gates with co-design allowed the team to chose an FPGA chip with fewer gates, leading to substantial power savings.</p> <p>“If we don’t need a certain logic or memory process, we don’t use them, and that saves a lot of power,” Karaman explains.</p> <p>Each time the researchers tweaked the ego-motion algorithm, they mapped the version onto the FPGA’s gates and connected the chip to a circuit board. They then fed the chip data from a standard drone dataset — an accumulation of streaming images and accelerometer measurements from previous drone-flying experiments that had been carried out by others and made available to the robotics community.</p> <p>“These experiments are also done in a motion-capture room, so you know exactly where the drone is, and we use all this information after the fact,” Karaman says.</p> <p><strong>Memory savings</strong></p> <p>For each version of the algorithm that was implemented on the FPGA chip, the researchers observed the amount of power that the chip consumed as it processed the incoming data and estimated its resulting position in space.</p> <p>The team’s most efficient design processed images at 20 frames per second and accurately estimated the drone’s orientation in space, while consuming less than 2 watts of power.</p> <p>The power savings came partly from modifications to the amount of memory stored in the chip. Sze and her colleagues found that they were able to shrink the amount of data that the algorithm needed to process, while still achieving the same outcome. As a result, the chip itself was able to store less data and consume less power.</p> <p>“Memory is really expensive in terms of power,” Sze says. “Since we do on-the-fly computing, as soon as we receive any data on the chip, we try to do as much processing as possible so we can throw it out right away, which enables us to keep a very small amount of memory on the chip without accessing off-chip memory, which is much more expensive.”</p> <p>In this way, the team was able to reduce the chip’s memory storage to 2 megabytes without using off-chip memory, compared to a typical embedded computer chip for drones, which uses off-chip memory on the order of a few gigabytes.</p> <p>“Any which way you can reduce the power so you can reduce battery size or extend battery life, the better,” Sze says.</p> <p>This summer, the team will mount the FPGA chip onto a drone to test its performance in flight. Ultimately, the team plans to implement the optimized algorithm on an application-specific integrated circuit, or ASIC, a more specialized hardware platform that allows engineers to design specific types of gates, directly onto the chip.</p> <p>“We think we can get this down to just a few hundred milliwatts,” Karaman says. “With this platform, we can do all kinds of optimizations, which allows tremendous power savings.”</p> <p>This research was supported, in part, by Air Force Office of Scientific Research and the National Science Foundation.</p>
<p><b><a href="http://news.mit.edu/2017/miniaturizing-brain-smart-drones-0712" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2601</wp:post_id>
		<wp:post_date><![CDATA[2017-07-12 03:59:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-07-12 03:59:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[miniaturizing-the-brain-of-a-drone]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/miniaturizing-brain-smart-drones-0712]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Drones that drive</title>
		<link>https://fifthlevel.ai/archives/2602</link>
		<pubDate>Mon, 26 Jun 2017 13:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/drones-drive-flying-cars-0626</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Being able to both walk and take flight is typical in nature — many birds, insects, and other animals can do both. If we could program robots with similar versatility, it would open up many possibilities: Imagine machines that could fly into construction areas or disaster zones that aren’t near roads and then squeeze through tight spaces on the ground to transport objects or rescue people.</p> <p>The problem is that robots that are good at one mode of transportation are usually bad at another. Airborne drones are fast and agile, but generally have too limited of a battery life to travel for long distances. Ground vehicles, on the other hand, are more energy efficient, but slower and less mobile.</p> <p>Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) are aiming to develop robots that can both maneuver around on land and take to the skies. In a new paper, the team presented a system of eight quadcopter drones that can fly and drive through a city-like setting with parking spots, no-fly zones, and landing pads.</p> <div class="cms-placeholder-content-video"></div> <p>“The ability to both fly and drive is useful in environments with a lot of barriers, since you can fly over ground obstacles and drive under overhead obstacles,” says PhD student Brandon Araki, lead author on the paper. “Normal drones can't maneuver on the ground at all. A drone with wheels is much more mobile while having only a slight reduction in flying time.”</p> <p>Araki and CSAIL Director Daniela Rus developed the system, along with MIT undergraduate students John Strang, Sarah Pohorecky, and Celine Qiu, and Tobias Naegeli of ETH Zurich’s Advanced Interactive Technologies Lab. The team presented their system at IEEE’s International Conference on Robotics and Automation (ICRA) in Singapore earlier this month.</p> <p><strong>How it works</strong></p> <p>The project builds on Araki’s previous work developing a <a href="http://www.csail.mit.edu/node/2747">“flying monkey” robot</a> that crawls, grasps, and flies. While the monkey robot could hop over obstacles and crawl about, there was still no way for it to travel autonomously.</p> <p>To address this, the team developed various “path-planning” algorithms aimed at ensuring that the drones don’t collide. To make them capable of driving, the team put two small motors with wheels on the bottom of each drone. In simulations, the robots could fly for 90 meters or drive for 252 meters, before their batteries ran out.</p> <p>Adding the driving component to the drone slightly reduced its battery life, meaning that the maximum distance it could fly decreased 14 percent to about 300 feet. But since driving is still much more efficient than flying, the gain in efficiency from driving more than offsets the relatively small loss in efficiency in flying due to the extra weight.</p> <p>“This work provides an algorithmic solution for large-scale, mixed-mode transportation and shows its applicability to real-world problems,” says Jingjin Yu, a computer science professor at Rutgers University who was not involved in the research.</p> <p>The team also tested the system using everyday materials such as pieces of fabric for roads and cardboard boxes for buildings. They tested eight robots navigating from a starting point to an ending point on a collision-free path, and all were successful.</p> <p>Rus says that systems like theirs suggest that another approach to creating safe and effective flying cars is not to simply “put wings on cars,” but to build on years of research in adding driving capabilities to drones.<br />
&nbsp;<br />
“As we begin to develop planning and control algorithms for flying cars, we are encouraged by the possibility of creating robots with these capabilities at small scale,” Rus says. “While there are obviously still big challenges to scaling up to vehicles that could actually transport humans, we are inspired by the potential of a future in which flying cars could offer us fast, traffic-free transportation.”</p> <p><b><a href="http://news.mit.edu/2017/drones-drive-flying-cars-0626" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2602</wp:post_id>
		<wp:post_date><![CDATA[2017-06-26 13:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-06-26 13:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[drones-that-drive]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/drones-drive-flying-cars-0626]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Batteries that “drink” seawater could power long-range underwater vehicles</title>
		<link>https://fifthlevel.ai/archives/2646</link>
		<pubDate>Thu, 15 Jun 2017 15:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/batteries-drink-seawater-long-range-autonomous-underwater-vehicles-0615</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The long range of airborne drones helps them perform critical tasks in the skies. Now MIT spinout Open Water Power (OWP) aims to greatly improve the range of unpiloted underwater vehicles (UUVs), helping them better perform in a range of applications under the sea.</p> <p>Recently acquired by major tech firm L3 Technologies, OWP has developed a novel aluminum-water power system that’s safer and more durable, and that gives UUVs a tenfold increase in range over traditional lithium-ion batteries used for the same applications.</p> <p>The power systems could find a wide range of uses, including helping UUVs dive deeper, for longer periods of time, into the ocean’s abyss to explore ship wreckages, map the ocean floor, and conduct research. They could also be used for long-range oil prospecting out at sea and various military applications.</p> <p>With the acquisition, OWP now aims to ramp up development of its power systems, not just for UUVs, but also for various ocean-floor monitoring systems, sonar buoy systems, and other marine-research devices.</p> <p>OWP is currently working with the U.S. Navy to replace batteries in acoustic sensors designed to detect enemy submarines. This summer, the startup will launch a pilot with Riptide Autonomous Solutions, which will use the UUVs for underwater surveys. Currently, Riptide’s UUVs travel roughly 100 nautical miles in one go, but the company hopes OWP can increase that distance to 1,000 nautical miles.</p> <p>“Everything people want to do underwater should get a lot easier,” says co-inventor <a href="http://news.mit.edu/2011/student-profile-mckay">Ian Salmon McKay</a> ’12, SM ’13, who co-founded OWP with fellow mechanical engineering graduate Thomas Milnes PhD ’13 and <a href="http://news.mit.edu/2015/student-profile-ruaridh-macdonald-0902">Ruaridh Macdonald</a> '12, SM '14, who will earn his PhD in nuclear engineering this year. “We’re off to conquer the oceans.”</p> <p><strong>“Drinking” sea water for power</strong></p> <p>Most UUVs use lithium-based batteries, which have several issues. They’re known to catch fire, for one thing, so UUV-sized batteries are generally not shippable by air. Also, their energy density is limited, meaning expensive service ships chaperone UUVs to sea, recharging the batteries as necessary. And the batteries need to be encased in expensive metal pressure vessels. In short, they’re rather short-lived and unsafe.</p> <p>In contrast, OWP’s power system is safer, cheaper, and longer-lasting. It consists of a alloyed aluminum, a cathode alloyed with a combination of elements (primarily nickel), and an alkaline electrolyte that’s positioned between the electrodes.</p> <p>When a UUV equipped with the power system is placed in the ocean, sea water is pulled into the battery, and is split at the cathode into hydroxide anions and hydrogen gas. The hydroxide anions interact with the aluminum anode, creating aluminum hydroxide and releasing electrons. Those electrons travel back toward the cathode, donating energy to a circuit along the way to begin the cycle anew. Both the aluminum hydroxide and hydrogen gas are jettisoned as harmless waste.</p> <p>Components are only activated when flooded with water. Once the aluminum anode corrodes, it can be replaced at low cost.</p> <p>Think of the power system as type of underwater engine, where water is the oxidizer feeding the chemical reactions, instead of the air used by car engines, McKay says. “Our power system can drink sea water and discard waste products,” he says. “But that exhaust is not harmful, compared to exhaust of terrestrial engines.”</p> <p>With the aluminum-based power system, UUVs can launch from shore and don’t need service ships, opening up new opportunities and dropping costs. With oil prospecting, for example, UUVs currently used to explore the Gulf of Mexico need to hug the shores, covering only a few pipeline assets. OWP-powered UUVs could cover hundreds of miles and return before needing a new power system, covering all available pipeline assets.</p> <p>Consider also the Malaysian Airlines crash in 2014, where UUVs were recruited to search areas that were infeasible for equipment on the other vessels, McKay says. “In looking for the debris, a sizeable amount of the power budget for missions like that is used descending to depth and ascending back to the surface, so their working time on the sea floor is very limited,” he says. “Our power system will improve on that.”</p> <p><strong>Nailing the design</strong></p> <p>The OWP technology started as the co-founders’ side project, which was modified throughout two MIT classes and a lab. In 2011, McKay joined 2.013/2.014 (Engineering System Design/Development) taught by MIT professor of mechanical engineering Douglas Hart, a seasoned hardware entrepreneur who co-founded <a href="http://news.mit.edu/2013/brontes-technologies-0821">Brontes Technologies</a> and Lantos Technologies. Milnes, who was previously a systems engineer at Brontes and co-founded <a href="http://news.mit.edu/2014/3-d-scanning-with-your-smartphone-0131">Viztu Technologies</a>, was Hart’s teaching assistant.</p> <p>The class was charged with developing an alternate power source for UUVs. McKay gambled on an energy-dense but challenging element: aluminum. One major challenge with aluminum batteries is that certain chemical issues make it difficult to donate electrons to a circuit. Additionally, the product of the reactions, the aluminum hydroxide, sticks to the electrode’s surface, inhibiting further reaction. Continuing the work in 10.625 (Electrochemical Energy Conversion and Storage), taught by materials science Professor Yang Shao-Horn, the W. M. Keck Professor of Energy, McKay was able to overcome the first challenge by making a gallium-rich alloyed aluminum anode that successfully donated electrons, but it corroded very quickly.</p> <p>Seeing potential in the battery, Milnes joined McKay in further developing the battery as a side project. The two briefly moved operations to the lab of Evelyn Wang, the Gail E. Kendall Professor of Mechanical Engineering. There, they began developing electrolytes and alloys that inhibit parasitic corrosion processes and prevent that aluminum hydroxide layer from forming on the anode.</p> <p>Setting up shop at Greentown Labs in Somerville, Massachusetts, in 2013 — where the company still operates with about 10 employees — OWP further refined the power system’s design. Today, that power system uses a pump to circulate the electrolyte, scooping up unwanted aluminum hydroxide on the anode and dumping it onto a custom precipitation trap. When saturated, the traps with the waste are ejected and replaced automatically. The electrolyte prevents marine organisms from growing inside the power system.</p> <p>Now OWP’s chief science officer, McKay says the startup owes much of its success to MIT’s atmosphere of innovation, where many of his professors readily offered technical and entrepreneurial advice and allowed him to work on extracurricular projects.</p> <p>“It takes a village,” McKay says. “Those classes and that lab are where the idea took shape. People at MIT were doing strong science for science’s sake, but everyone was keenly aware of the possibility of bringing technologies to market. People were always having those great ‘What if?’ conversations — I probably had three to four different startup ideas in various stages of gestation at any given time, and so did all my friends. It was an environment that encouraged the playful exchange of ideas, and encouraged people to take on side projects with real prizes in mind.”</p>
<p><b><a href="http://news.mit.edu/2017/batteries-drink-seawater-long-range-autonomous-underwater-vehicles-0615" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2646</wp:post_id>
		<wp:post_date><![CDATA[2017-06-15 15:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-06-15 15:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[batteries-that-drink-seawater-could-power-long-range-underwater-vehicles]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/batteries-drink-seawater-long-range-autonomous-underwater-vehicles-0615]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>How Acquiring a Team of LiDAR Experts Strengthens our Self-Driving Future</title>
		<link>https://fifthlevel.ai/archives/439</link>
		<pubDate>Fri, 27 Oct 2017 15:01:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/8ef340603563</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Bryan Salesky, CEO, Argo AI</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PChni-ZY955Pc_gsenaFrA.jpeg" /></figure><p>We can’t talk about a future of self-driving cars without mentioning LiDAR technology — and we won’t be able to build that future without it. These sensors are crucial to creating a three-dimensional view of the world that helps autonomous vehicles find where they are on the road and detect other vehicles, pedestrians and cyclists.</p><p>To accelerate our mission to realize the self-driving future, Argo has acquired Princeton Lightwave, a company with extensive experience in the development and commercialization of LiDAR sensors. The technology that underpins their lineup of LiDAR sensors — which already serve the commercial mapping and defense industries — will help us extend the range and resolution needed to achieve self-driving capability in challenging urban environments. Princeton Lightwave’s technology also complements and expands the capability of LiDAR sensors already available to the automotive industry today.</p><p>With the addition of the Princeton Lightwave team, Argo is uniquely positioned to innovate in both sensor hardware and the interface between sensor and software — enabling us to achieve performance improvements that would not otherwise be possible. Princeton Lightwave’s technology will help us unlock new capabilities that will aid our virtual driver system in handling object detection in challenging scenarios, such as poor weather conditions, and safely operating at high speeds in dynamic environments.</p><p>As <a href="https://medium.com/self-driven/a-decade-after-darpa-our-view-on-the-state-of-the-art-in-self-driving-cars-3e8698e6afe8">we’ve talked about</a> before, even considering how far we’ve come in bringing about a world of self-driving cars, there’s still much we need to accomplish. We are constantly exploring how to increase the range, resolution and field of view of LiDAR, but we’re also looking to lower costs and manufacture these sensors at scale. We have to continue making improvements through dedicated research and development that’s aligned with our overall hardware and software strategy.</p><p>Our expanded team remains focused on accelerating the development of a virtual driver system that’s mandated for SAE levels four and five autonomous driving — meaning there’s no driver behind the wheel. By collaborating with our in-house hardware and software developers, as well as our supply base, we will work to create LiDAR sensors that not only meet the demanding performance required for high volume production, but also are affordable.</p><p>Argo’s expansion serves to expedite our mission to commercialize and deploy self-driving cars at scale. To our talented new team members from Princeton Lightwave: Welcome aboard! We’re thrilled to embrace your expertise as we all work together to build an exciting future of autonomous vehicles.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8ef340603563" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/how-acquiring-a-team-of-lidar-experts-strengthens-our-self-driving-future-8ef340603563">How Acquiring a Team of LiDAR Experts Strengthens our Self-Driving Future</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/how-acquiring-a-team-of-lidar-experts-strengthens-our-self-driving-future-8ef340603563?source=rss-ba51b2aa6fa1------2" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>439</wp:post_id>
		<wp:post_date><![CDATA[2017-10-27 15:01:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-10-27 15:01:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[how-acquiring-a-team-of-lidar-experts-strengthens-our-self-driving-future]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@ArgoAI]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/how-acquiring-a-team-of-lidar-experts-strengthens-our-self-driving-future-8ef340603563?source=rss-ba51b2aa6fa1------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[807]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>A Decade after DARPA: Our View on the State of the Art in Self-Driving Cars</title>
		<link>https://fifthlevel.ai/archives/440</link>
		<pubDate>Mon, 16 Oct 2017 14:01:03 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/3e8698e6afe8</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Bryan Salesky, CEO, Argo AI</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wZVqEYx0TFlZepm2Ix2NDg.jpeg" /></figure><p>A decade ago in the California high desert, 11 finalists competed in an unprecedented 60-mile race. Robot cars needed to safely and swiftly complete the mission without any human intervention — while also interacting with human-driven vehicles — in under six hours.</p><p>It was the 2007 DARPA Urban Challenge, an autonomous vehicle competition that unofficially kicked off today’s self-driving technology initiatives. The vehicles were considered incredible at the time, and looking back, this marked the beginning of a long journey.</p><p><a href="https://medium.com/@ArgoAI/why-we-created-argo-ai-aa3f43ebefb6">Why We Created Argo AI</a></p><p>DARPA ensured a certain level of success by carefully managing scope: Participants agreed to a set of rigorously defined traffic rules, and DARPA eliminated pedestrian and cyclist traffic from the challenge. Despite these simplifications, what the teams accomplished was impressive — with most putting their systems together largely from scratch in just 18 months.</p><p>The DARPA challenge highlighted the need for more advanced computational power and algorithm development. At the time, we relied heavily on rules-based programming techniques, which meant robotic systems of a decade ago tended to operate only in very constrained environments, around well-behaved road users that would not deviate much from an established set of rules.</p><p>Many of us at Argo have been in the field of robotics and self-driving cars for well over a decade, and as we now work to bring this technology to the masses, we are leveraging our extensive expertise including our learnings from the DARPA Urban Challenge. Just a few months shy of Argo AI’s first birthday, we’ve managed to assemble an experienced team of almost 200 employees, and we now have test vehicles on the road in Pittsburgh and Southeast Michigan.</p><p>We know firsthand the challenges that come with commercializing the software and hardware that fuel highly automated and intelligent systems. Working in outdoor conditions among vehicle traffic, pedestrians and cyclists operating without strict adherence to a set of rules can be tricky. The effects of real-world conditions like night and day, changing weather, different road geometries and materials can compound things. The dynamics of the environment bring inconsistencies and variability to what robotic system builders have traditionally needed to simplify into a basic set of assumptions.</p><p>In the past few years, the game has changed due in part to the computational power now available, but with this has come a new set of complexities we are still learning to manage. Many advancements in processing power, storage and artificial intelligence are coming together so that these computers can reason through problems without requiring a script. They will be able to learn from massive amounts of data, to recognize patterns with astonishing accuracy and to filter out anomalous inputs from sensors to focus on what matters the most.</p><p>As we embrace these advancements, we do so knowing that no single tool, technique or algorithm alone will categorically solve all of the self-driving challenges. Here is our take on some considerations to thoughtfully build a self-driving car.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Nu0-Dx1fKT7mwxJXZgl7GQ.jpeg" /></figure><h3><strong>Sensing the world</strong></h3><p>Sensors still have a long way to go. We use LiDAR sensors, which work well in poor lighting conditions, to grab the three-dimensional geometry of the world around the car, but LiDAR doesn’t provide color or texture, so we use cameras for that. Yet cameras are challenged in poor lighting, and tend to struggle to provide enough focus and resolution at all desired ranges of operation. In contrast, radar, while relatively low resolution, is able to directly detect the velocity of road users even at long distances.</p><p>That’s why we still have so many sensors mounted on the car — the strengths of one complement the weaknesses of another. Individual sensors don’t fully reproduce what they capture, so the computer has to combine the inputs from multiple sensors, then sort out the errors and inconsistencies. Combining all of this into one comprehensive and robust picture of the world for the computer to process is incredibly difficult.</p><p>Developing a system that can be manufactured and deployed at scale with cost-effective, maintainable hardware is even more challenging. We are innovating across the sensing hardware and software stack to lower costs, reduce sensor count, and improve range and resolution. There remains significant work to be done to accomplish these conflicting objectives and get the technology to reliably scale.</p><h3><strong>Understanding the world</strong></h3><p>Once an autonomous vehicle has the tools to “see” relevant objects around it, it’s up to the car itself to take the next step — identifying the type of object, whether it’s a pedestrian, cyclist, another vehicle or debris on the road, and how fast that object is moving. The car then must make a determination about that object’s likely behavior.</p><p>Advancements in artificial intelligence and machine learning, powered by ever-increasing computing and storage options in the cloud, have fueled new algorithms and driven new twists on old algorithms. These new tools are incredibly powerful at building algorithms that can robustly sift through millions of pixels of information that flow from our sensors every second, then making determinations about the location, size and speed of road users relevant to the car.</p><p>One part of the process in building these algorithms is to collect millions of miles of real-world data from our sensors, then use that information to teach an algorithm to detect the relevant road users despite the challenges presented by noisy or erroneous sensor data. Significant tool chains and operations teams manage this data flow and development process.</p><p>Our early results are indeed impressive, but we know full well that the devil is always in the details.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nVAFk2-bu2VH_rolVsbCYQ.jpeg" /></figure><h3><strong>Prediction</strong></h3><p>When we drive a car today, we’re subconsciously estimating the next few seconds of behavior from other road users — anticipating when a pedestrian might jaywalk or when another car may be about to cut us off. Attentive drivers are incredibly good at reacting in these situations — managing their speed and planning out contingencies to adapt to anomalous behavior from others. These same actions, which good drivers perform quickly while avoiding a drastic response, are also required for a self-driving car to navigate busy city streets.</p><p>We must build algorithms that enable our autonomous vehicle to respond to a deeper understanding of the likely behavior of other road users. We need to instill “thoughtfulness” into the technology to ensure that the car can operate safely, reliably and predictably.</p><p>For example, the car needs to know when it will have to move over slightly for a large truck to give it more room, or adjust its speed to stay out of another driver’s blind spot. At the same time, we have to build algorithms that enable it to know when it’s being overly conservative, when the car will need to “nudge” in dense traffic, or commit to an action consistently so that other road users can respond correctly. Throughout, as the computer absorbs all of the information, it’s key that it never gets distracted or learns the wrong model, as it will act strangely in anticipation of an action that never comes to fruition.</p><p>This is the balance we must deliver in building these predictive models, and it’s only from all of these examples and real-world driving that we can learn to predict the micro-maneuvers that turn out to be the leading indicators of the likely actions of other road users.</p><h3><strong>System integration and testing</strong></h3><p>Generally, the software that powers a self-driving car is what’s called a stochastic system. What this means is that the results are determined through a series of detected patterns and models applied to inherently random sensor inputs, rather than through a mathematical equation with a consistent set of inputs that translates to a consistent set of outputs.</p><p>Imagine driving down the same road twice and nothing changes between the first and the second trip. You’re highly unlikely to drive the same path at exactly the same speed the second time around. Self-driving vehicles are no different, though in general they will be more consistent than human drivers.</p><p>Testing stochastic systems requires a significant number of repetitions generated by real-world data for it to be representative. That means we must gather millions of miles of road experience to teach the software to drive with confidence. (Imagine needing to drive millions of miles to get your driver’s license!) But not all miles are created equal, so “accumulated miles” is not an expressive enough metric to track progress. Think of it this way: The skills you acquired learning to drive in a quiet Midwestern town will not translate should you find yourself driving in the heart of Manhattan.</p><p>The algorithms we build move millions of pixels per second through complex math and logic to calculate important outcomes about the state of the world of an autonomous vehicle. Given the high dimensionality of these inputs, it’s impossible to test across the space of every possible input combination — there would be many trillions of combinations to test, which is simply unmanageable.</p><p>So we need to be clever about how we use the recorded miles of driving experience from our test vehicles. We’re building tools that can extract the right set of miles that sufficiently covers the realistic and relevant scenarios that the vehicle is likely to see, and then test for the right response. This balance requires extensive driving experience and data collection in the target deployment area that covers as many diverse and challenging scenarios as possible. We must also collect sufficient variations around environmental changes in each scenario that might degrade a sensor’s output, such as weather and lighting conditions.</p><p>We have built a dedicated team of logistics and test operators to safely execute these test miles, plus a team of analytics professionals and software engineers who are creating the tools to manage the data flow that gives us confidence in our completeness in scenario coverage.</p><p>We’re still very much in the early days of making self-driving cars a reality. Those who think fully self-driving vehicles will be ubiquitous on city streets months from now or even in a few years are not well connected to the state of the art or committed to the safe deployment of the technology. For those of us who have been working on the technology for a long time, we’re going to tell you the issue is still really hard, as the systems are as complex as ever.</p><p>Everyone knows focused teams that innovate and work hard can solve amazingly difficult problems. At Argo, we see these challenges as an inspiration. They drive us to leverage the advancements of the last decade to propel us into a new era where the commercial success of self-driving cars will be a reality.</p><p>We’re taking a pragmatic approach to bringing about fully self-driving cars — incorporating the state of the art while acknowledging there’s no silver bullet. We’re playing the long game and avoiding the hype in our commitment to bring this important technology to maturity in the form of a great product that earns the trust of millions of people around the world.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3e8698e6afe8" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/a-decade-after-darpa-our-view-on-the-state-of-the-art-in-self-driving-cars-3e8698e6afe8">A Decade after DARPA: Our View on the State of the Art in Self-Driving Cars</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/a-decade-after-darpa-our-view-on-the-state-of-the-art-in-self-driving-cars-3e8698e6afe8?source=rss-ba51b2aa6fa1------2" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>440</wp:post_id>
		<wp:post_date><![CDATA[2017-10-16 14:01:03]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-10-16 14:01:03]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[a-decade-after-darpa-our-view-on-the-state-of-the-art-in-self-driving-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@ArgoAI]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/a-decade-after-darpa-our-view-on-the-state-of-the-art-in-self-driving-cars-3e8698e6afe8?source=rss-ba51b2aa6fa1------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[804]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>First U.S. Public Autonomous Shuttle Launches In Vegas, Crashes Shortly Thereafter</title>
		<link>https://fifthlevel.ai/archives/643</link>
		<pubDate>Fri, 10 Nov 2017 10:30:02 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/186235/first-us-public-autonomous-shuttle/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[Probably not the best kick-off news cycle for an autonomous debut. <p><b><a href="https://www.motor1.com/news/186235/first-us-public-autonomous-shuttle/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>643</wp:post_id>
		<wp:post_date><![CDATA[2017-11-10 10:30:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-11-10 10:30:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[first-u-s-public-autonomous-shuttle-launches-in-vegas-crashes-shortly-thereafter]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/videos/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/186235/first-us-public-autonomous-shuttle/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Intel Taps Lebron James To Promote Trust In Autonomous Cars</title>
		<link>https://fifthlevel.ai/archives/644</link>
		<pubDate>Wed, 11 Oct 2017 06:35:54 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/183021/intel-autonomous-cars-lebron-james-ambassador/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[You should feel less afraid of self-driving vehicles now. <p><b><a href="https://www.motor1.com/news/183021/intel-autonomous-cars-lebron-james-ambassador/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>644</wp:post_id>
		<wp:post_date><![CDATA[2017-10-11 06:35:54]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-10-11 06:35:54]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[intel-taps-lebron-james-to-promote-trust-in-autonomous-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/videos/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/183021/intel-autonomous-cars-lebron-james-ambassador/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Autonomous Continental Bee Concept Removes Sting From City Driving</title>
		<link>https://fifthlevel.ai/archives/645</link>
		<pubDate>Thu, 05 Oct 2017 16:01:30 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/182560/continental-bee-concept-autonomous-vehicle/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[Continental wants to have thousands of them networked together in a city. <p><b><a href="https://www.motor1.com/news/182560/continental-bee-concept-autonomous-vehicle/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>645</wp:post_id>
		<wp:post_date><![CDATA[2017-10-05 16:01:30]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-10-05 16:01:30]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[autonomous-continental-bee-concept-removes-sting-from-city-driving]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/videos/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/182560/continental-bee-concept-autonomous-vehicle/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Toyota Builds Mule With 2 Steering Wheels To Test Autonomous Tech</title>
		<link>https://fifthlevel.ai/archives/646</link>
		<pubDate>Thu, 28 Sep 2017 21:30:02 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/181935/toyota-research-institute-test-mule/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[The company believes that drivers and autonomous vehicles should work together. <p><b><a href="https://www.motor1.com/news/181935/toyota-research-institute-test-mule/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>646</wp:post_id>
		<wp:post_date><![CDATA[2017-09-28 21:30:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-09-28 21:30:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[toyota-builds-mule-with-2-steering-wheels-to-test-autonomous-tech]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/videos/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/181935/toyota-research-institute-test-mule/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Audi Aicon Concept Live Look</title>
		<link>https://fifthlevel.ai/archives/647</link>
		<pubDate>Wed, 13 Sep 2017 15:43:45 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/180280/audi-aicon-live-look-frankfurt/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[Our colleagues at Motor1-UK are on the scene in Frankfurt, putting together videos on some of the hottest cars at the show. <p><b><a href="https://www.motor1.com/news/180280/audi-aicon-live-look-frankfurt/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>647</wp:post_id>
		<wp:post_date><![CDATA[2017-09-13 15:43:45]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-09-13 15:43:45]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[audi-aicon-concept-live-look]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/videos/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/180280/audi-aicon-live-look-frankfurt/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Q&amp;A Session with our CEO Karlheinz Wurm</title>
		<link>https://fifthlevel.ai/archives/750</link>
		<pubDate>Mon, 06 Nov 2017 09:45:55 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://web159.s124.goserver.host/?p=2865</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Der Beitrag <a rel="nofollow" href="http://aid-driving.eu/first-qa-session-with-our-ceo-karlheinz-wurm/">Q&#038;A Session with our CEO Karlheinz Wurm</a> erschien zuerst auf <a rel="nofollow" href="http://aid-driving.eu">AID</a>.</p> <p><b><a href="http://aid-driving.eu/first-qa-session-with-our-ceo-karlheinz-wurm/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>750</wp:post_id>
		<wp:post_date><![CDATA[2017-11-06 09:45:55]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-11-06 09:45:55]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[qa-session-with-our-ceo-karlheinz-wurm]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/first-qa-session-with-our-ceo-karlheinz-wurm/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Echoing Apple&#039;s $1B investment in Didi Chuxing, Alphabet sinks $1B into Lyft</title>
		<link>https://fifthlevel.ai/archives/755</link>
		<pubDate>Thu, 19 Oct 2017 16:23:38 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/17/10/19/echoing-apples-1b-investment-in-didi-chuxing-alphabet-sinks-1b-into-lyft</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/23320-29300-Lyft_Pink_Mustache-l.jpg" alt="Article Image" border="0" /> <br><br> In a deal that values Lyft at $11 billion, Google's parent company Alphabet has invested $1 billion into the ride sharing service, more than a year after Apple placed its own $1 billion bet on Chinese taxi provider Didi Chuxing. <p><b><a href="https://appleinsider.com/articles/17/10/19/echoing-apples-1b-investment-in-didi-chuxing-alphabet-sinks-1b-into-lyft" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>755</wp:post_id>
		<wp:post_date><![CDATA[2017-10-19 16:23:38]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-10-19 16:23:38]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[echoing-apples-1b-investment-in-didi-chuxing-alphabet-sinks-1b-into-lyft]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/17/10/19/echoing-apples-1b-investment-in-didi-chuxing-alphabet-sinks-1b-into-lyft]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[806]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>New video rumored to show close-up of Apple car&#039;s updated &#039;Project Titan&#039; testbed</title>
		<link>https://fifthlevel.ai/archives/756</link>
		<pubDate>Wed, 18 Oct 2017 16:20:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/17/10/18/new-video-purports-to-show-close-up-of-apples-updated-project-titan-testbed</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/23305-29261-applecar-testbed2-l.jpg" alt="Article Image" border="0" /> <br><br> A short video clip is said to show updated test hardware for Apple's self-driving car platform, suggesting that the company is making some progress. <p><b><a href="https://appleinsider.com/articles/17/10/18/new-video-purports-to-show-close-up-of-apples-updated-project-titan-testbed" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>756</wp:post_id>
		<wp:post_date><![CDATA[2017-10-18 16:20:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-10-18 16:20:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[new-video-rumored-to-show-close-up-of-apple-cars-updated-project-titan-testbed]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/17/10/18/new-video-purports-to-show-close-up-of-apples-updated-project-titan-testbed]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[805]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Renault Symbioz Can Morph Into A Modular Room For Your House</title>
		<link>https://fifthlevel.ai/archives/1156</link>
		<pubDate>Tue, 12 Sep 2017 09:26:41 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/179965/renault-symbioz-concept-revealed-frankfurt/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[We're all living in 2017, but Renault is already thinking about the year 2030. <p><b><a href="https://www.motor1.com/news/179965/renault-symbioz-concept-revealed-frankfurt/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1156</wp:post_id>
		<wp:post_date><![CDATA[2017-09-12 09:26:41]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-09-12 09:26:41]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[renault-symbioz-can-morph-into-a-modular-room-for-your-house]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/videos/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/179965/renault-symbioz-concept-revealed-frankfurt/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Presenting Drive.ai’s First Open House!</title>
		<link>https://fifthlevel.ai/archives/2141</link>
		<pubDate>Wed, 15 Nov 2017 20:16:21 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/2a53461b3ba1</guid>
		<description></description>
		<content:encoded><![CDATA[<p>On September 14, 2017, Drive.ai hosted an Engineering Open House. This was our first time hosting an event of this kind, and it was a smashing success! We’re thrilled to share a recap and can’t wait for our next Engineering Open House.</p><p>We had initially gotten the idea to do an open house upon realizing that we’re part of an industry that tends to be very secretive. One of our company values is transparency — we strongly believe in being as honest and transparent as possible at all levels of our organization, and wanted to share this value externally by offering people a sneak peek into what goes on behind the scenes at a self driving company.</p><p>We had stations set up throughout the evening, ranging from hardware and platform to perception and planning, staffed by several members of our engineering team (kudos to you all!). Attendees rotated between each station to learn specifics from the project teams we have in place.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*8fSphx3TnyR_Zp-I." /></figure><p><em>Brody Huval, one of our co-founders, showcasing one of our internal visualization products to a captivated audience.</em></p><p>The evening wasn’t just about the technical lectures — we wanted an ambience where people could relax after the long workday, enjoy themselves, and maybe even get a little silly. One of the most popular stations was the Drive.ai photobooth, which we set up in the parking lot with one of our demo vehicles. As you can see, people really enjoyed themselves (and the taco hats!).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*q9Q6vzsRY6Anucut." /></figure><p><em>Photobooth was a blast!</em></p><p>No event is complete without good food, so we also enjoyed empanadas, chicken skewers, and baklava — not to mention a beer and wine bar.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*q2dFykGKDXvCdJEZ." /></figure><p><em>Attendees enjoying appetizers throughout the evening</em></p><p>While we initially anticipated around 100 attendees,, there was an overwhelming turnout the night of, and we reached capacity very quickly. Apologies to anyone that we had to turn away at the door — your interest is greatly appreciated, and we will do our best to get you in next time! And thanks to our operations team for reorganizing the space to accommodate everyone.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xiD76B3wA6bFbsP4." /></figure><p><em>Great furniture re-arrangement in our All Hands area to accommodate 100+ people over the course of the evening.</em></p><p>As an additional perk for our open house guests, we announced that anyone who applied for one of our open roles would automatically be entered into a contest to win a free autonomous demo ride.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2tEiC3HlJbF7YTHP." /></figure><p><em>Stay tuned to hear who wins a ride in one of our autonomous vehicles!</em></p><p>Beyond our guests, we had many of our own employees at the open house. Our Drive.ai team put in time to volunteer at stations, talk to guests, and take care of things behind the scenes to ensure the event went smoothly. As per usual, the team went above and beyond to support their colleagues ( and a big thanks to our office team for providing everyone with brand new Drive.ai gear the day before the event!).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*jV7x8t2keoU_010X." /></figure><p><em>Team was excited to show off their brand new drive.ai swag!</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Y6HdRyXoP2Lz52zF." /></figure><p><em>Welcoming attendees to drive.ai!</em></p><p>As our first event, the focus of the evening was providing a high level overview of our product. Future events will dive deeper into the different components of our technology, so stay tuned!</p><p>Until next time, thanks from the entire Drive.ai team!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qLIZ6TKEshUUuCkv." /></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2a53461b3ba1" width="1" height="1"> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2141</wp:post_id>
		<wp:post_date><![CDATA[2017-11-15 20:16:21]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-11-15 20:16:21]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[presenting-drive-ais-first-open-house]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@drive.ai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@drive.ai/presenting-drive-ais-first-open-house-2a53461b3ba1?source=rss-37851f177ff8------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>AppleInsider podcast discusses iPhone 8 production, FM radios, and Apple acquiring a health company</title>
		<link>https://fifthlevel.ai/archives/2422</link>
		<pubDate>Fri, 20 Oct 2017 12:30:50 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/17/10/20/appleinsider-podcast-discusses-iphone-8-production-fm-radios-and-apple-acquiring-a-health-company</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/23160-28919-DSC08132-l.jpg" alt="Article Image" border="0" /> <br><br> This week on the AppleInsider podcast, Neil and Victor talk about iPhone 8 production numbers being cut, anticipation for iPhone X, the FCC's useless call for FM radios on iPhones, and Apple buying a healthcare company, and much more. <p><b><a href="https://appleinsider.com/articles/17/10/20/appleinsider-podcast-discusses-iphone-8-production-fm-radios-and-apple-acquiring-a-health-company" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2422</wp:post_id>
		<wp:post_date><![CDATA[2017-10-20 12:30:50]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-10-20 12:30:50]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[appleinsider-podcast-discusses-iphone-8-production-fm-radios-and-apple-acquiring-a-health-company]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/17/10/20/appleinsider-podcast-discusses-iphone-8-production-fm-radios-and-apple-acquiring-a-health-company]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Self-driving vehicles: The “platform” business model</title>
		<link>https://fifthlevel.ai/archives/2479</link>
		<pubDate>Sat, 23 Sep 2017 20:32:30 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.driverless-future.com/?p=1091</guid>
		<description></description>
		<content:encoded><![CDATA[<p>How will autonomous car technology generate profits? Among the many different business models &#8211; from self-driving mobility services to models centered on data, advertising or entertainment &#8211; platform-oriented business models are currently receiving much attention, not the least because Waymo seems to be leaning towards them.</p>
<p>The term &#8220;platform&#8221; can be understood in different ways: In the automotive context it is usually understood as a car platform where many different models share the same technology under the hood which reduces development costs and allows economies of scale. In a more general, wider interpretation platform business models aim to build a unique competitive position through a complex technology or service which is combined with an ecosystem of users and partners. Ideally the platform exhibits network effects: the larger the ecosystem, the more attractive it becomes to its users and partners and the harder it becomes for competitors to challenge the position.</p>
<h1>Waymo&#8217;s integrated hard- and software platform</h1>
<p>When Waymo&#8217;s CEO John Krafcik talks about Waymo&#8217;s strategy he emphasizes the integrated hard- and software platform which Waymo is building. Currently this platform is embodied in the ugly white box  on top of Waymo&#8217;s self-driving Chrysler Pacificas which are occasionally driving around Phoenix. Most of the self-driving hard- and software in the box has been engineered by Waymo/Google: Not just the software, also a novel 360 degree spinning Lidar (with better performance than the Velodyne Lidar, costs reduced by almost an order of magnitude); radar sensors (with better short range detection of stationary objects); the computing platform (developed from scratch in collaboration with Intel); cameras, microphones. Ideally, this box, Waymo&#8217;s &#8220;better driver&#8221;, could be integrated easily into other car models. However, this will always require more work than just adding the box because some sensors will still need to be mounted on the car; more importantly, the car must be ready for self-driving (e.g. redundant safety components) and must be able to communicate with the box by reporting its physical conditions to the box and accepting driving instructions from it.</p>
<p>Can there be much doubt that such a universal driving module would be a highly profitable product? There are many application scenarios (vehicles for commercial use: taxis, buses, trucks, logistics) where self-driving modules would be economically viable for the customer even if priced at very high margins. Startups and established companies should see much opportunity for quickly bringing self-driving vehicles of many kinds onto the market. The technology provider could realize economies of scale while still keeping the total cost for the customer significantly below the alternatives (i.e. where self-driving technology is self-developed or sourced from a variety of vendors).</p>
<h1>Platform economics in the consumer car space</h1>
<p>Unfortunately, this calculation does not apply to the consumer car space: Consumers are not willing to pay a significant premium for self-driving car technology because they value their own time differently than commercial users of self-driving car technology. In addition, the equation changes for auto makers selling large volumes of vehicles: with a century of experience in managing and cutting costs auto makers will look for every way they can find to slash the price of the self-driving car technology and bring margins down. The larger the sales volume, the higher is the incentive to find other, more cost-effective solutions. Even if they initially agree to source the universal self-driving hard- and software modules, they will work hard to reduce their dependency on it. And they will find many ways to scale back the size of the external self-driving car module: they will want sensors to be integrated into the car &#8211; rather than to come with the self-driving platform &#8211; and they will want to source them independently. They will clamor to structure and compartmentalize the interface between the self-driving module and their vehicles and they will fight to standardize and take over some of those functions, so that they get control over them. There will be fights over access to the data, over controlling the interface with the user. And it will be hard for the universal self-driving module provider to beat all of those demands back because the OEMs have experience and market knowledge and their car models have special use cases in various segments that the self-driving module provider is not familiar with, does not own and therefore can not easily implement independently. If the provider of the SDC technology platform can not impose lasting, full control over the whole extent of the self-driving platform (prohibiting partial sourcing of components, keeping all modifications to the platform under their own control (even those developed in the context of a particular customer relationship) etc., avoiding any replacement of functionality by the OEM) his power position and margins are likely to deteriorate significantly over time. In the other extreme, the OEM risks losing their established central position in the market to a newcomer who now controls the &#8216;heart&#8217; of the vehicles. The middle ground is a slippery slope characterized by an uneasy, highly unstable and competitive relationship between both partners where each continually tries to boost their power position to the detriment of the other.</p>
<p>Thus Waymo&#8217;s apparent lack of success at finding partners in the auto industry does not come as a big surprise. Why should companies that are used to investing billions for  designing a new car model  succumb to a company that has invested not much more than a billion dollars (<a href="https://spectrum.ieee.org/cars-that-think/transportation/self-driving/google-has-spent-over-11-billion-on-selfdriving-tech">approximately 1.1 bio $ between 2009 and 2015</a>) into self-driving car technology? Shouldn&#8217;t they just follow the same path, jump-start their own efforts and ensure that they reduce the gap?</p>
<h1>Self-driving software can&#8217;t establish a lasting competitive advantage</h1>
<p>For anyone who examines the technology and its potential there can be little doubt that many actors will eventually master self-driving car technology. There are many commercial players who have every incentive and sufficient resources to solve the problem. This includes General Motors which has spent <a href="https://www.thestreet.com/story/13648023/1/general-motors-discloses-why-it-paid-581m-for-cruise-automation.html">581 million dollars to acquire Cruise Automation</a> and is making a concerted effort to reach<a href="https://medium.com/kylevogt/how-we-built-the-first-real-self-driving-car-really-bd17b0dbda55"> manufacturing readiness on the first self-driving car model</a>. There are big European OEMs which are determined to solve the self-driving equation but there are also countries which regard the technology as vital to their economic and military interests. There are investors who understand the economic potential of the technology. Furthermore, although the self-driving car problem is exceptionally hard, it has a ceiling; it will not keep increasing and becoming more and more difficult. Over time, algorithms, simulation environments, tools test data collection and test case generation, hard- and software will become more refined and more easily available. Thus it is very unlikely that a provider of self-driving car technology will be able to establish a lasting advantage over the competition just on the basis of the technology. On the contrary: the time will come where the technology will be mastered by many and be commoditized. The time will come where self-driving car technology will be seen as a natural part of every vehicle, where cars will no longer be differentiated on the basis of their self-driving car technology and where customers will no longer care very much what kind of self-driving car technology is inside. Because safety requirements will be very stringent, vendors of self-driving car technology will have a hard time making the case that their technology is significantly better than the competing products.</p>
<h1>Platform models with network effects?</h1>
<p>But couldn&#8217;t there be a way for the first market entrant to establish a platform position in the wider sense where the technical self-driving car solution forms the base for a self-sustaining ecosystem of customers and partners which exerts a pull on the market and erects a powerful barrier against entry for competitors?</p>
<p>There are several strategies which could be applied toward this end: those who enter the market first and expand quickly can realize economies of scale, which keeps costs down and can discourage competitors by keeping prices low. But keeping prices down means foregoing much of the rents associated with significant productivity increases due to reduced costs of mobility. It is more than questionable whether this would discourage competitors or whether it would be interpreted as a play towards dominance in a lucrative market &#8211; an economic signal that might actually entice competitors to redouble their efforts.</p>
<p>Another approach would be to use current dominance in the technology to establish a hard-to-assail business position, a self-growing platform, around the technology. Self-driving car technology requires much more than the car&#8217;s hard- and software. There are many legal aspects which require substantial effort. Various service infrastructures need to be established &#8211; some to fulfill legal requirements, others out of practical necessity &#8211; and might become key parts of the platform ecosystem: California self-driving car regulations already mandate that operators of self-driving cars ensure that high-definition maps are kept up to date and are regularly distributed to the cars. The same regulations describe a remote operations service which assists fully self-driving cars in challenging situations (i.e. a 24/7 remote operations center). Infrastructures are needed for cleaning and maintenance, accident handling, secure over-the-air updates of self-driving car software. The scope of platform services could be extended further to include services for managing fleets of self-driving taxis, trucks and buses as well as associated customer facing services (reservation, payment processing etc.).</p>
<p>Companies which provide the full breadth of such services (or manage access to it) certainly have a favorable competitive position, but it is questionable to what degree this can protect the platform and establish a barrier against entry of competitors. Precursors to most of the platform services described above already exist today and companies exist already that would be willing to extend their services to the self-driving car market. Today many OEMs already operate remote assistance centers (GM OnStar, LexusLink, BWM Assist etc.)  which could easily be extended to provide assistance to fully-self driving cars. Several companies are focused on building and maintaining high definition maps (among others  <a href="https://here.com/en">Here</a> which was purchased by the German OEMs). Rental car and mobility services companies already have experience with some of the additional services needed and would certainly aim extend their business models to the self-driving car space. Thus it is unlikely that such a Waymo self-driving platform could not be replicated with a determined effort by some of the OEMs or other players.</p>
<h1>SDC platforms not similar to operating system or marketplace platforms</h1>
<p>The market for self-driving car technology is not similar to other markets where we have seen platform models succeed. This is not like some of the operating system (Windows, Android) which have grown into a platform, where this platform is the base for millions of different applications and uses, where the platform grows because with more users the breadth of applications and uses increase. In contrast, self-driving mobility is a much more specific &#8211; and for safety and security reasons &#8211; limited application domain where scale effects matter but the diversity and number of applications will be comparatively low. A software platform for self-driving cars can never be as open as Windows or Android. A self-driving software platform will most likely evolve in a way that the platform has a very limited external application programming interface which partners may latch onto. But this also means that competitors which provide their own universal self-driving car modules or platforms should find ways to expose similar interfaces to their partners and these partners could more easily support multiple self-driving car platforms with their services and applications. Thus a self-driving hard- and software is not likely to achieve an operating-system like lock-in effect for its partners and customers.</p>
<p>The market also does not resemble an Airbnb, Ebay or Uber, domain-specific optimized marketplaces which link a large number of product or service providers to a large number of customers and which increase in value and attractiveness with an increasing number of participants, thus quickly erecting barriers to competition. Yes, self-driving car technology can be the basis for establishing mobility services which will tend to rapidly establish a dominant, hard-to-assail position in a region. This mobility-as-a-service business model does have a lock-in effect but this is a very different type of business model than the self-driving hard- and software platform model which we are currently examining.</p>
<p>Thus, the pioneers of self-driving hard- and software can base their business models on viable platform strategies centered around a universal self-driving hard- and software model complemented with associated services and business relationships. Given the economic value that can be realized in many markets and business scenarios with self-driving vehicle technology the business model will initially be very profitable. As in many other markets the pioneers have the potential of establishing a leading and hopefully lasting market position. But their competitive advantage will fall over time as the market becomes commoditized and it will be hard to keep competitors out &#8211; unlike the platform models in other markets which enjoy considerable network effects.</p>
<h1>The problems with Waymo&#8217;s focus on a platform business model</h1>
<p>Thus Waymo&#8217;s apparent focus on a universal self-driving platform-based business model seems to be questionable. When Waymo decided to shelve the activities related to their self-driving firefly electric two-seaters, they seem to have made a decision against squarely focusing on the mobility services model, the one business model in the self-driving car space that exhibits strong network effects and which would provide a permanent advantage for the first mover.</p>
<p>A side problem of Wamo&#8217;s universal self-driving platform is that it does not seem to be well executed. To make their platform truly universal, they would need to expose themselves to many different use cases and ensure that the platform works for cars, trucks, buses, even self-driving machines of different types. Many startups are currently working on products and services in the self-driving space and would be keen to cooperate with a provider of a self-driving car modules but there is no evidence, that Waymo is branching out to them. Companies such as EasyMile, Navya, LocalMotors, truck manufacturers, and many others would be more than willing to jump on the bandwagon and thus ensure that the platform really becomes universal. Waymo would profit from learning about differing requirements in different application scenarios which would necessarily lead to a more customizable structure of the self-driving &#8220;box&#8221; which Waymo envisions placing on top of a vehicle. That the top box may not be the best idea can easily be seen when we consider the context of trucks where a top box is much less compelling because it would not achieve full 360 degree unobstructed sensor vision. Another worry about Waymo&#8217;s approach to a universal driving platform is the reliance on their own sensors. With the current innovation in the automotive sensor market it is not very likely that their sensor suite can remain ahead of the competition for long. A universal self-driving car platform needs the ability to rapidly incorporate new sensors and even new sensor types. Impressive as Waymo&#8217;s self-developed sensors may be, there is also the risk of paying less attention to external innovations.</p>
<h1>Conclusion</h1>
<p>For the market as a whole, Waymo&#8217;s detour focusing on a business model based on some incarnation of a universal self-driving hard- and software platform (&#8220;the better driver&#8221;) may be a positive development. It reduces the risk that one player will dominate the field, has given auto makers time to understand the nature of the challenges better and increase their determination to close the gap. Most auto makers have now understood the dimension of the challenge (although some have difficulties balancing their priorities between autonomous driving and electric vehicles). General Motors is an excellent example of an auto-maker getting up to speed: their acquisition of Cruise Automation is a win-win for both companies and both companies together are not plagued by the competitive stalemate that a collaboration between a universal self-driving module provider and established auto makers would engender. Being the most advanced player, Waymo is likely to profit greatly from its self-driving car technology but a problematic platform-focused commercialization strategy may be giving its competitors some welcome breathing space for catching up.</p> <p><b><a href="http://www.driverless-future.com/?p=1091" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2479</wp:post_id>
		<wp:post_date><![CDATA[2017-09-23 20:32:30]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-09-23 20:32:30]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[self-driving-vehicles-the-platform-business-model]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?feed=rss2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?p=1091]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>3 Questions: Lisa Parks on drones, warfare, and the media</title>
		<link>https://fifthlevel.ai/archives/2540</link>
		<pubDate>Fri, 17 Nov 2017 04:59:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/3-questions-lisa-parks-drones-warfare-and-media-1117</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>Drones have become a common part of warfare — but their use remains a subject of public contention. <a href="https://cmsw.mit.edu/profile/lisa-parks/">Lisa Parks</a>, a professor in MIT’s program in Comparative Media Studies/Writing and director of its Global Media Technologies and Cultures Lab, has spent extensive time analyzing this public debate. Now, she has co-edited a new volume examining the subject, while contributing a piece to it herself. The book, “<a href="https://www.dukeupress.edu/life-in-the-age-of-drone-warfare" style="">Life in the Age of Drone Warfare</a>,” has just been published by Duke University Press. </em>MIT News<em> talked with Parks this week about the impact and public perception of drones. </em></p> <p><strong>Q:</strong> We know much more about drones now, in 2017, than we did several years ago. How has the public understanding of drones evolved?</p> <p><strong>A:</strong> When the story first broke in 2009 about a secret CIA drone war on the Afghan/Pakistan border, there were many follow-up investigative reports. You also saw a lot of discussion in the news media suggesting &nbsp;that drones represented a “humane” kind of warfare, that they allowed the U.S. to minimize casualties through precision targeting in the war theater. … Drones, the logic went, made war more clean and antiseptic. Then people began to drill down more deeply and listen to public sentiment from around the world, as there were mass demonstrations, peaking in 2014, against drone wars, not only in Pakistan but also in London and Washington, D.C. Drones were being used in the war in Iraq, and Syria, and Libya as well … and [people were] trying to document in photographs and testimonials what these scenes looked like after a U.S. strike. So the picture became a little more complicated at that point, and there was more vociferous opposition and antiwar activity in relation to military drone technology.</p> <p><strong>Q:</strong> You have an essay in this book about the “vertical mediation” effect drones have, that is, that they are changing conditions on the ground just by being in the air. Could you explain this idea a little more?</p> <p><strong>A:</strong> I came at this topic as a media scholar who had studied war, media, and satellites in the past … and I was not satisfied with the existing media theories that were available. When you think about drones in the military context, you can’t limit the analysis to [drones] being like a video game, because the effects of the drone’s movements can change the material conditions in the air and on the ground. So we need a theory of what I call “vertical mediation” to grapple with the transformations that are happening between orbit and the ground. Drones, just by moving over an area, are [altering] life on Earth, reorganizing where and how people move, what their disposition is or what their feeling is about the sky above them — and who they want to be around and who they try to avoid. When drones are operating in an area over time, above a certain region, they change the status of sites and motions on the ground.</p> <p><strong>Q:</strong> But how different are drones from previous military tools? We had video game-like imagery from Gulf War in 1991, and there have been aerial bombing campaigns throughout the 20th century, which also altered how people act on the ground.</p> <p><strong>A:</strong> In the book we try to make clear that current forms of drone warfare are part of a longer history of aerial militarization, that this is not unique to the era of the drone. One chapter of the book, for instance, focuses on drone experiments during World War II. Today’s military drones arguably make warfare more flexible and mobile, but they still require extensive ground infrastructure, not to mention agreements with host countries. One of the most distinguishing aspects of drone use is that they have been mobilized to fight wars without public deliberation. Drones are being used right now, not just for warfare but for patrolling, [which] is a sign of how available they are to be mobilized. As a humanist and a social scientist, I’m very interested in the effects of drone use, on societies, economies, and cultures in different parts of the world.</p> <p>Some critics align the drone with a kind of totalitarian tendency. There is a [video] parody called “The Ethical Governor,” which is about a world in which the drones run everything for us, and we delegate the power of public administration and governance to these automated entities. And that’s one vision, where the human becomes subservient to this new level of automation.</p> <p>On the other hand, there’s a lot of optimism and DIY-hobbist, civilian-based impetus in drone technology too. We see a lot of that here at MIT. I never think a technology is just one thing. A lot of people are trying to design drones to do good things. Those efforts are sincere and important. At the same time, we [can] be surprised even by technologies designed to do good things: What happens if civilian drones get weaponized? We really need to be thinking about the complex potentials of a technology like the drone, and the different kinds of agencies or capabilities it enables. That’s what our book tries to investigate while bringing some new voices into the conversation who have the social good at heart.</p>
<p><b><a href="http://news.mit.edu/2017/3-questions-lisa-parks-drones-warfare-and-media-1117" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2540</wp:post_id>
		<wp:post_date><![CDATA[2017-11-17 04:59:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-11-17 04:59:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[3-questions-lisa-parks-on-drones-warfare-and-the-media]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/3-questions-lisa-parks-drones-warfare-and-media-1117]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Summit discusses impact of machines on jobs, productivity, and the global economy</title>
		<link>https://fifthlevel.ai/archives/2593</link>
		<pubDate>Tue, 07 Nov 2017 22:15:03 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/artificial-intelligence-summit-discusses-impact-of-machines-on-jobs-productivity-global-economy-1107</guid>
		<description></description>
		<content:encoded><![CDATA[<p>This week MIT hosted a summit on “AI and the Future of Work”, focused on helping industry, government, and the workforce navigate the opportunities and challenges of artificial intelligence and automation.</p> <p>Hosted by MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Initiative on the Digital Economy (IDE), the event featured speakers such as Alphabet chairman Eric Schmidt and machine-learning pioneer Yann LeCun.</p> <p>Taking place in MIT’s Kresge Auditorium, the summit featured remarks from CSAIL Director Daniela Rus and IDE Director Erik Brynjolfsson, as well as a keynote address by Schmidt. Computer scientists, economists, and industry experts spoke about a range of topics that included legal policy, technical challenges, and assistive technologies.</p> <p>During his opening remarks, MIT President L. Rafael Reif noted the important societal gains that could stem from AI, such as economic growth, higher living standards, longer life spans and less disease. However, he also described the fear that many Americans face related to automation and job loss.</p> <p>“Seventy-two&nbsp;percent of Americans say they feel worried about a future [in which] robots and computers can do many human jobs,” said Reif. “It's clear to almost everyone that deep change is happening. For most people it's not clear how to respond.”</p> <p>Still, a common theme throughout the event was the potential physical and mental benefits of automating monotonous work. Schmidt expressed his belief that AI will not only make workers more productive, but help spur a future of ubiquitous personal assistants, saving time and helping people lead happier lives.</p> <p>“The presumption is that existing jobs are lost, and you can’t imagine the jobs that are created,” he said. “[But] AI is replacing tasks.&nbsp;There is a surplus of jobs and not enough people to fill them.”</p> <p>Of course, many technical issues still remain. CSAIL researcher and autonomous-driving expert John Leonard discussed some of the challenges underlying what he calls the “great space race of the 21st&nbsp;century.” He spoke about the concept of “parallel autonomy,” where humans are are still in control of the car, but the vehicle onboard computer will step in if the driver is about to make a dangerous move. He is among several MIT researchers who are working with Toyota to use this approach with self-driving cars.</p> <p>Cybersecurity expert <a href="https://internetpolicy.mit.edu/team/daniel-weitzner/" target="_blank">Daniel Weitzner</a> spoke on a panel that examined a delicate practical question about AI: who’s at fault when it makes a mistake? Weitzner spoke about the limits of deep learning systems that are high-performing, but whose decisions still aren’t fully understood by the humans that created them.</p> <p>“People are learning ways to trick these systems, making them downright harmful to individuals and more difficult to figure out who is at fault,” said Weitzner. “I imagine we could create a static mechanism that conforms to a set of rules and regulations that we can lay out, and build to those rules.”</p> <p>Other sessions included a discussion about job training with Rusty Justice, whose company Bit Source “turns coal miners into data miners.” Kai-Fu Lee, an influential former Google China executive who is now CEO of Sinovation Ventures, spoke about the state of AI in China, and predicted that AI would be able to do half all job tasks in China in the next 10 to 15 years.</p> <p>The summit builds off other major MIT AI initiatives, including the <a href="http://news.mit.edu/2017/ibm-mit-joint-research-watson-artificial-intelligence-lab-0907" target="_self">new lab with IBM</a> and “SystemsThatLearn@CSAIL,”&nbsp;which focuses on democratizing AI systems.</p> <p>Even with all of the AI advances that have been made, most speakers at the summit agreed that there’s still ample room for inventing better and smarter systems.</p> <p>“We’ve always said that human-level intelligence is 20 years off,” said MIT AI Professor Patrick Winston. “Eventually we’ll be right."&nbsp;</p>
<p><b><a href="http://news.mit.edu/2017/artificial-intelligence-summit-discusses-impact-of-machines-on-jobs-productivity-global-economy-1107" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2593</wp:post_id>
		<wp:post_date><![CDATA[2017-11-07 22:15:03]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-11-07 22:15:03]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[summit-discusses-impact-of-machines-on-jobs-productivity-and-the-global-economy]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/artificial-intelligence-summit-discusses-impact-of-machines-on-jobs-productivity-global-economy-1107]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Small European nation becomes a “living lab” for urban innovation researchers</title>
		<link>https://fifthlevel.ai/archives/2594</link>
		<pubDate>Fri, 13 Oct 2017 04:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/european-nation-andorra-living-lab-media-lab-urban-innovation-1013</guid>
		<description></description>
		<content:encoded><![CDATA[<p>When you think of innovation hubs around the world, Andorra, a tiny country tucked between Spain and France, may not come to mind.</p> <p>But, for several years, the 180-square-mile nation of 77,000 people has served as a “living lab” for researchers from MIT Media Lab’s City Science Initiative to prototype, deploy, and test urban innovation.</p> <p>The researchers have, for example, developed <a href="https://www.media.mit.edu/groups/city-science/overview/">CityScope Andorra</a>, a 3-D augmented-reality platform that visualizes complex urban data on a small-scale model of the country in real-time. The platform simulates the impact of multiple urban interventions —&nbsp;from urban planning proposals to shared autonomous vehicles — and facilitates civic engagement and decision making.</p> <p>The researchers have also tested the <a href="https://www.media.mit.edu/projects/mod/overview/">Persuasive Electric Vehicle (PEV)</a>, an ultra-lightweight, three-wheel, shared-use autonomous vehicle designed to operate in bicycle lanes. If deployed at scale, the PEV could help address the severe traffic and parking problems in Andorra, a car-dependent country without an airport or train service.&nbsp;</p> <p>“Urban innovation that is focused on human interaction must be tested in a real-world context,” says Kent Larson, director of the City Science Initiative, who spearheads the projects. “Working in Andorra is a fantastic opportunity. In such a tiny country, we can work directly with the ministers, policy decisions can be made rapidly and efficiently, and we have access to mobile and energy data for the entire country to help us understand the complex behaviors of residents and tourists.”</p> <p><strong>Urban modeling and simulation</strong></p> <p>In 2014, Andorran government officials met with Larson’s City Science Initiative to discuss collaboration opportunities. Andorra had aspirations — and still does — of becoming the world’s first “smart country” by making use of big data to help develop and deploy innovation.</p> <p>This sparked a partnership in which Andorra would fund the City Science Initiative to develop projects related to data collection and analysis, urban mobility, urban planning for a new innovation district, and sensors deployed in schools for learning.&nbsp;</p> <p>“In order to solve the great societal challenges of our era — from climate change to jobs to health care to food and water — we have to make cities more responsive to changing economic conditions, human needs, and opportunities related to rapidly evolving technology” Larson says.</p> <p>A number of notable projects have stemmed from the partnership. One is CityScope Andorra, currently being used in the redevelopment of a district in Andorra la Vella. Urban planners and the community can interactively test scenarios on the model, such as how many parks to include, whether to make buildings taller, the optimal mix of residential and commercial real estate, and whether to allow private vehicles or only car-share services. By weighing the density, proximity, and diversity of the district, Larson adds, Andorra could also determine the city’s “innovation potential,” which is “the probability of creative collisions that can be estimated from the density and mix of people with diverse experiences who have opportunities to exchange ideas.”</p> <p>Users can add and move modules representing various types of housing or office space, tune the heights of each element, and toggle alternative mobility systems. The impacts of these changes on traffic, walkable access to amenities, and other metrics are then updated in real time.</p> <p>For Andorra, whose geographical size is only about twice that of Boston, researchers modeled the entire country to analyze, among other things, different behaviors of tourists, an economic driver for the country.</p> <p>By visualizing how thousands of tourists come and go to major events throughout the year, for instance, CityScope helped the Andorran government improve the experience and analyze the impact of the events, says Josep Maria Missé, Andorra’s secretary of state for economic diversification and innovation.</p> <p>“CityScope has helped us see how many days people come to, how many people came to see the events, which parts of the country had the most people, and how they move around,” he says. “This will help the planning of future events.”</p> <p>The researchers also developed and deployed an app to enable children, teachers, government officials, and the public in Andorra’s major cities to capture photos of different areas and document places they like or dislike. “In that way, we can generate these unusual data sets about human perception of an environment and visualize that on CityScope. We are interested in how children and adults may perceive the city differently,” Larson says.</p> <p>Another project, called TerMITes (which stands for the third-generation of MIT Environmental Sensors), deploys small sensors to schools in Andorra’s capital so students can learn how to measure energy consumption in their buildings and invent strategies to reduce it, while collecting building-efficiency data for CityScope.</p> <p>“It’s a way to instill innovation in students … and help students learn about energy efficiency,” Missé says, “[while] using it as a tool for planning the future of Andorra.”</p> <p><strong>Rethinking mobility</strong></p> <p>For innovation at the street level, the researchers developed the PEV, an autonomous, on-demand vehicle that looks somewhat similar to the cargo bikes found in Amsterdam. In operation, the PEV would “combine the features of a bike-sharing system, such as Boston’s Hubway with the on-demand pick-up and drop-off of an Uber or Lyft service,” Larson says.</p> <p>The vehicle was designed as an alternative to popular driverless cars and taxis. Driving in Boston, Larson says he averages about 12 miles per hour, which is slower than average bike speeds. “It makes no sense to put one person in a 4,000-pound car that goes 120 miles per hour, to travel a short distance and low speeds,” he says.</p> <p>Users are meant to call the PEV via an app. The vehicle then navigates a city’s bike lanes to meet the passenger. Computer vision keeps the vehicle within the painted lines and a low-cost Lidar system scans for objects, such as people and other vehicles. Users can pedal themselves or ride in a fully automated mode.</p> <p>The PEV uses parts that are less expensive than automated cars — meaning the price range would be comparable to that of a high-end bike — and has fewer hurdles to becoming street-ready in a place such as Andorra, which has no restrictions against testing autonomous bike-lane vehicles. By replacing conventional automobiles in cities, such as Andorra’s capital, the PEV has potential to cut emissions by more than 60 percent, Larson says.</p> <p>So far, Larson’s group has tested the vehicle on the streets of Cambridge and at the Taiwan Air Force Base in Taipei. In Andorra, the group aims to deploy a small fleet of PEVs to study how they impact cities, with an eye toward commercialization. “Next steps are increasingly ambitious testing in the wild,” Larson says. “If successful, we would consider spinning it off as a startup or partnering with one of our corporate sponsors.”</p> <p><strong>A life of its own</strong></p> <p>Following MIT’s lead, others are now turning to Andorra as a living lab, including company sponsors of the Media Lab such as Denso, the Japanese automotive supplier.&nbsp; &nbsp;</p> <p>This is good for Andorra, Missé says. Down the road, the PEV could serve as a platform for further development of autonomous vehicles for various tech companies. Andorra is also crunching retail data with CityScope to help companies better reach and serve their customers. “That’s an important way to include the private sector in these projects,” Missé says.</p> <p>In September, Larson, MIT researchers, and Andorran government officials hosted an inaugural meeting of the City Science Summit 2017. Presenting their work were researchers representing the international network of City Science collaborators: Tongji University (Shanghai), National Taipei University of Technology (Taipei), Aalto University (Helsinki), and HafenCity University (Hamburg).</p> <p>The event also saw the opening of an innovation facility in the capital. Located on the ground floor of a public building, the facility contains co-working spaces used for meetings, technological collaborations, and workshops and lectures. An aim is to bring together government officials, urban planners, architects, and the public to work together and use MIT-built technologies to improve the country.</p> <p>“This is a highly visible community-engagement space, which will be a platform where anything that is related to innovation and the future of Andorra can be presented and discussed,” Larson says.</p> <p>In a couple of years, Larson says, his group will conclude its work in Andorra. But he hopes that Andorran researchers and companies will carry the torch: “I will consider our project a success if, at the end of our involvement, all these projects we have seeded have a life of their own.”</p>
<p><b><a href="http://news.mit.edu/2017/european-nation-andorra-living-lab-media-lab-urban-innovation-1013" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2594</wp:post_id>
		<wp:post_date><![CDATA[2017-10-13 04:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-10-13 04:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[small-european-nation-becomes-a-living-lab-for-urban-innovation-researchers]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/european-nation-andorra-living-lab-media-lab-urban-innovation-1013]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>3 Questions: Iyad Rahwan on the “psychological roadblocks” facing self-driving cars</title>
		<link>https://fifthlevel.ai/archives/2595</link>
		<pubDate>Mon, 11 Sep 2017 14:59:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/3-questions-iyad-rahwan-psychological-roadblocks-facing-self-driving-cars-0911</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>This summer, a survey released by the American Automobile Association showed that 78 percent of Americans feared riding in a self-driving car, with just 19 percent trusting the technology. What might it take to alter public opinion on the issue? Iyad Rahwan, the AT&amp;T Career Development Professor in the MIT Media Lab, has studied the issue at length, and, along with Jean-Francois Bonnefon of the Toulouse School of Economics and Azim Shariff of the University of California at Irvine, has authored a new commentary on the subject, titled, “Psychological roadblocks to the adoption of self-driving vehicles,” published today in </em>Nature Human Behavior. <em>Rahwan spoke to </em>MIT News<em> about the hurdles automakers face if they want greater public buy-in for autonomous vehicles. &nbsp;</em></p> <p><strong>Q:</strong> Your new paper states that when it comes to autonomous vehicles, trust “will determine how widely they are adopted by consumers, and how tolerated they are by everyone else.” Why is this?</p> <p><strong>A:</strong> It’s a new kind of agent in the world. We’ve always built tools and had to trust that technology will function in the way it was intended. We’ve had to trust that the materials are reliable and don’t have health hazards, and that there are consumer protection entities that promote the interests of consumers. But these are passive products that we choose to use. For the first time in history we are building objects that are proactive and have autonomy and are even adaptive. They are learning behaviors that may be different from the ones they were originally programmed for. We don’t really know how to get people to trust such entities, because humans don’t have mental models of what these entities are, what they’re capable of, how they learn.</p> <p>Before we can trust machines like autonomous vehicles, we have a number of challenges. The first is technical: the challenge of building an AI [artificial intelligence] system that can drive a car. The second is legal and regulatory: Who is liable for different kinds of faults? A third class of challenges is psychological. Unless people are comfortable putting their lives in the hands of AI, then none of this will matter. People won’t buy the product, the economics won’t work, and that’s the end of the story. What we’re trying to highlight in this paper is that these psychological challenges have to be taken seriously, even if [people] are irrational in the way they assess risk, even if the technology is safe and the legal framework is reliable.</p> <p><strong>Q:</strong> What are the specific psychological issues people have with autonomous vehicles?</p> <p><strong>A:</strong> We classify three psychological challenges that we think are fairly big. One of them is dilemmas: A lot of people are concerned about how autonomous vehicles will resolve ethical dilemmas. How will they decide, for example, whether to prioritize safety for the passenger or safety for pedestrians? Should this influence the way in which the car makes a decision about relative risk? And what we’re finding is that people have an idea about how to solve this dilemma: The car should just minimize harm. But the problem is that people are not willing to buy such cars, because they want to buy cars that will always prioritize themselves.</p> <p>A second one is that people don’t always reason about risk in an unbiased way. People may overplay the risk of dying in a car crash caused by an autonomous vehicle even if autonomous vehicles are, on the average, safer. We’ve seen this kind of overreaction in other fields. Many people are afraid of flying even though you’re incredibly less likely to die from a plane crash than a car crash. So people don’t always reason about risk.</p> <p>The third class of psychological challenges is this idea that we don’t always have transparency about what the car is thinking and why it’s doing what it’s doing. The carmaker has better knowledge of what the car thinks and how it behaves … which makes it more difficult for people to predict the behavior of autonomous vehicles, which can also dimish trust. One of the preconditions of trust is predictability: If I can trust that you will behave in a particular way, I can behave according to that expectation.</p> <p><strong>Q:</strong> In the paper you state that autonomous vehicles are better depicted “as being perfected, not as perfect.” In essence, is that your advice to the auto industry?</p> <p><strong>A:</strong> Yes, I think setting up very high expectations can be a recipe for disaster, because if you overpromise and underdeliver, you get in trouble. That is not to say that we should underpromise. We should just be a bit realistic about what we promise. If the promise is an improvement on the current status quo, that is, a reduction in risk to everyone, both pedestrians as well as passengers in cars, that’s an admirable goal. Even if we achieve it in a small way, that’s already progress that we should take seriously. I think being transparent about that, and being transparent about the progress being made toward that goal, is crucial.</p>
<p><b><a href="http://news.mit.edu/2017/3-questions-iyad-rahwan-psychological-roadblocks-facing-self-driving-cars-0911" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2595</wp:post_id>
		<wp:post_date><![CDATA[2017-09-11 14:59:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-09-11 14:59:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[3-questions-iyad-rahwan-on-the-psychological-roadblocks-facing-self-driving-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/3-questions-iyad-rahwan-psychological-roadblocks-facing-self-driving-cars-0911]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Cognata CEO Danny Atsmon to speak at EY Journey 2017 conference about his vision of autonomous cars</title>
		<link>https://fifthlevel.ai/archives/3291</link>
		<pubDate>Mon, 23 Oct 2017 20:50:03 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.cognata.com/?p=411</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Cognata CEO, Danny Atsmon, to speak on stage at EY Journey 2017.</p>
<p>Danny will share his vision on the journey to an autonomous car along with a panel of experts including: Yahal Zilka, Rani Wellingstein, Yoav Leitersdorf, Michael Granoff and Joseph Heyer.</p>
<p>&#8220;Everything must move! The automotive industry has been experiencing significant disruptions arising from technological and social trends such as electric, autonomous, smart mobility and shared economy. These trends are undermining the foundations of the business models employed by established players. The recent digitization of mobility has required new competences which have already been developed in Israel in other fields including those targeting military defense applications or Big Data applications for intelligence services. The Israeli automotive and smart mobility industry is becoming an innovation lab for the movements connected with electric, autonomous and smart mobility.&#8221;</p>
<p>Come and Hear Danny and the rest of the panel on: Wednesday, October 25 15:00 &#8211; 16:00, Hall D, Hilton Tel Aviv</p>
<p>&nbsp;</p>
<p><img data-attachment-id="396" data-permalink="http://www.cognata.com/cognata-present-nvidia-gtc-israel-conference/gtc/" data-orig-file="https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/GTC.png?fit=1079%2C419" data-orig-size="1079,419" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="GTC" data-image-description="" data-medium-file="https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/GTC.png?fit=300%2C116" data-large-file="https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/GTC.png?fit=1024%2C398" class="aligncenter wp-image-396 size-medium" src="https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/GTC.png?resize=300%2C116" alt="" width="300" height="116" srcset="https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/GTC.png?resize=300%2C116 300w, https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/GTC.png?resize=768%2C298 768w, https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/GTC.png?resize=1024%2C398 1024w, https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/GTC.png?w=1079 1079w" sizes="(max-width: 300px) 100vw, 300px" data-recalc-dims="1" /></p>
<p>&nbsp;</p>
<p><a href="http://www.journey-israel.com/agenda/session/202113" target="_blank" rel="noopener">http://www.journey-israel.com/agenda/session/202113</a></p>
<p>The post <a rel="nofollow" href="http://www.cognata.com/cognata-ceo-danny-atsmon-speak-ey-journey-2017-conference-vision-autonomous-cars/">Cognata CEO Danny Atsmon to speak at EY Journey 2017 conference about his vision of autonomous cars</a> appeared first on <a rel="nofollow" href="http://www.cognata.com">Cognata Autonomous Simulation</a>.</p> <p><b><a href="http://www.cognata.com/cognata-ceo-danny-atsmon-speak-ey-journey-2017-conference-vision-autonomous-cars/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3291</wp:post_id>
		<wp:post_date><![CDATA[2017-10-23 20:50:03]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-10-23 20:50:03]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[cognata-ceo-danny-atsmon-to-speak-at-ey-journey-2017-conference-about-his-vision-of-autonomous-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.cognata.com/blog/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.cognata.com/cognata-ceo-danny-atsmon-speak-ey-journey-2017-conference-vision-autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Cognata wins the NVIDIA’s Inception Award competition for promising AI startups</title>
		<link>https://fifthlevel.ai/archives/3413</link>
		<pubDate>Thu, 19 Oct 2017 01:52:56 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.cognata.com/?p=402</guid>
		<description></description>
		<content:encoded><![CDATA[<p>&nbsp;</p>
<p>NVIDIA’s Inception Award competition for promising AI startups found its perfect home today in a packed auditorium in Tel Aviv, the epicenter of Israel’s startup culture.</p>
<p>in a nation with the highest number of startups per capita — some 5,000 fledgling companies in a country of 8.5 million people — six of the best competed head-on at <a href="https://www.nvidia.com/en-il/gtc/">GTC Israel</a>, our inaugural local edition of the GPU Technology Conference, which is on a global swing through six cities this fall.</p>
<p>In quarter-hour blocks, their CEOs described their business strategy and fielded questions from five judges with deep startup experience, including execs from three Israeli VC firms specializing in AI. The format, longer than the typical several-minute martial-arts like square-offs that place in other startup competitions.</p>
<p>As more than 150 GTC attendees looked on, the judges named as winner <a href="http://www.cognata.com/">Cognata</a>, which uses simulations to train autonomous vehicles across wide-ranging, realistic environments without ever pulling out of the driveway. The prize: an <a href="https://www.nvidia.com/en-us/data-center/dgx-station/">NVIDIA DGX Station</a> personal AI supercomputer, with four of the latest NVIDIA V100 GPU accelerators providing the processing capability of 400 CPUs, worth $69,000.</p>
<p><a href="https://blogs.nvidia.com/blog/2017/10/18/inception-awards-gtc-israel/">https://blogs.nvidia.com/blog/2017/10/18/inception-awards-gtc-israel/</a></p>
<p>&nbsp;</p>
<p><img data-attachment-id="403" data-permalink="http://www.cognata.com/cognata-wins-nvidias-inception-award-competition-promising-ai-startups/award-image/" data-orig-file="https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/award-image.jpg?fit=1280%2C960" data-orig-size="1280,960" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="award image" data-image-description="" data-medium-file="https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/award-image.jpg?fit=300%2C225" data-large-file="https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/award-image.jpg?fit=1024%2C768" class="wp-image-403 aligncenter" src="https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/award-image.jpg?resize=428%2C321" alt="" width="428" height="321" srcset="https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/award-image.jpg?resize=300%2C225 300w, https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/award-image.jpg?resize=768%2C576 768w, https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/award-image.jpg?resize=1024%2C768 1024w, https://i2.wp.com/www.cognata.com/wp-content/uploads/2017/10/award-image.jpg?w=1280 1280w" sizes="(max-width: 428px) 100vw, 428px" data-recalc-dims="1" /></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://www.cognata.com/cognata-wins-nvidias-inception-award-competition-promising-ai-startups/">Cognata wins the NVIDIA’s Inception Award competition for promising AI startups</a> appeared first on <a rel="nofollow" href="http://www.cognata.com">Cognata Autonomous Simulation</a>.</p> <p><b><a href="http://www.cognata.com/cognata-wins-nvidias-inception-award-competition-promising-ai-startups/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3413</wp:post_id>
		<wp:post_date><![CDATA[2017-10-19 01:52:56]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-10-19 01:52:56]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[cognata-wins-the-nvidias-inception-award-competition-for-promising-ai-startups]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.cognata.com/blog/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.cognata.com/cognata-wins-nvidias-inception-award-competition-promising-ai-startups/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Why Teaming with Postmates Will Help Ford Expand On-Demand Delivery to Everyone</title>
		<link>https://fifthlevel.ai/archives/389</link>
		<pubDate>Tue, 09 Jan 2018 17:01:02 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/c11739e76ba4</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Sherif Marakby, Ford Vice President, Autonomous Vehicles and Electrification</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*W1w6SPdxdZo2c1O8Z-dgdg.jpeg" /></figure><p>Location, location, location! For decades, businesses have operated on the mantra that to succeed, they must find the perfect spot, but in a rapidly changing mobility landscape, is that still true? What if businesses — small, medium and large — could use self-driving vehicles to loosen the grip of geography, changing the way they operate and the way our communities operate in the process?</p><p>At Ford, we believe that’s the kind of potential autonomous technology holds, and why we expect our partnership with <a href="https://postmates.com">Postmates</a>, a company that’s already making big improvements to the lives of city dwellers, can make an even bigger impact on local economies. Like us, <a href="https://medium.com/u/5f6a915e7ca4">Postmates</a> is on a mission to help people unlock the best of their cities, with a focus on creating a reliable on-demand delivery network — one that gives consumers better access to the goods they need.</p><p><a href="https://medium.com/@Postmates/4269c47868a4">Postmates</a> offers on-demand delivery of anything from anywhere right to your doorstep — from restaurant carryout and coffee shop orders to hardware store items and grocery goods. As the first on-demand partner for our self-driving vehicle program, we’re setting out together to explore new ways in which this technology can serve people living and working in our communities by offering more efficient deliveries, and by connecting more consumers to smaller and local businesses.</p><p>Small businesses play a crucial role in supporting all our cities, but in many ways it’s difficult for them to compete with large businesses, which oftentimes limits them to operating in a specific part of town. With self-driving vehicles, we can ensure that geography alone does not equal destiny.</p><p>Throughout the year, Ford and Postmates will conduct pilot programs to explore how self-driving technology could change the delivery experience for consumers, enable brick-and-mortar retailers to reach new customer bases, and transform the way commerce moves in the communities in which we operate.</p><p>In the future, when a consumer uses Postmates to place a purchase — whether for groceries, takeout or other goods — a self-driving vehicle could be what delivers her order. As part of our testing trials, we’ll study both what the merchant experience needs to be at the point of delivery and what the customer experience needs to be at that same point.</p><p>Already, what I’ve learned about Postmates and the way it’s <a href="https://blog.postmates.com/how-on-demand-is-supporting-not-hurting-local-businesses-3d5df5193a1c">supporting local businesses</a> is impressive, including the fact that retailers who have taken advantage of the platform have seen sales quadruple. Retailers in low-income communities have even used the service to expand their reach into other neighborhoods. We’re here to help the company take it to an all new level.</p><p>The way commerce is moving around in cities is dramatically changing, and emerging technology will undoubtedly have an impact on the future of on-demand delivery. With the knowledge we’ll gain from our partnership with Postmates, we anticipate we’ll be able to better deploy self-driving technology in a way that can help people get what they need faster, while also supporting local businesses that are a big part of communities around the world.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c11739e76ba4" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/why-teaming-with-postmates-will-help-ford-expand-on-demand-delivery-to-everyone-c11739e76ba4">Why Teaming with Postmates Will Help Ford Expand On-Demand Delivery to Everyone</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/why-teaming-with-postmates-will-help-ford-expand-on-demand-delivery-to-everyone-c11739e76ba4?source=rss----7192a82ca09a---4" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>389</wp:post_id>
		<wp:post_date><![CDATA[2018-01-09 17:01:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-09 17:01:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[why-teaming-with-postmates-will-help-ford-expand-on-demand-delivery-to-everyone]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driven/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/why-teaming-with-postmates-will-help-ford-expand-on-demand-delivery-to-everyone-c11739e76ba4?source=rss----7192a82ca09a---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[813]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Coming to a City Near You: Test-Driving Our Autonomous Vehicle Business</title>
		<link>https://fifthlevel.ai/archives/390</link>
		<pubDate>Tue, 09 Jan 2018 17:01:02 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/27a05a2b082e</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Jim Farley, Ford Executive Vice President and President, Global Markets</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JdSVh0qw2AR_AYkWXpnJbA.jpeg" /></figure><p>With the transformative potential of self-driving technology on the horizon, there are many ways we can change how our cities work for the better. To that end, we are working to verify that the technology underpinning future self-driving cars operates safely and reliably, to ensure that our self-driving vehicles are designed to deliver trusted experiences and to prove out that the business model under which they are deployed is viable.</p><p>For us, these three paths will converge this year as we begin testing self-driving vehicles in a new city — the first in which we plan to validate our autonomous vehicle business model. In partnership with <a href="https://medium.com/u/ba51b2aa6fa1">Argo AI</a>, we will expand development and testing of the technology that enables a vehicle to drive itself. And, we will conduct more research into the customer experience to help design an all-new, purpose-built self-driving vehicle.</p><p>But, it’s the new partnership platform we have just launched that excites me most because it will help us validate our self-driving business model by working with partners — like Domino’s Pizza, Lyft, and now <a href="https://medium.com/self-driven/why-teaming-with-postmates-will-help-ford-expand-on-demand-delivery-to-everyone-c11739e76ba4">Postmates</a> — and reaching out to different communities and stakeholders to understand their needs, as well as how they can benefit from autonomous vehicles.</p><p><a href="https://medium.com/self-driven/how-ford-and-lyft-are-teaming-up-to-take-self-driving-cars-mainstream-9bf2974a912a">How Ford and Lyft Are Teaming Up to Take Self-Driving Cars Mainstream</a></p><p>We’ll talk more about our city collaboration later this year. In the meantime, our new platform will make it easy to connect to and work with our partners, who can benefit by accessing our fleet of self-driving vehicles to serve their customers. Lyft, for example, is already testing the platform, which includes specific communications protocols that will be used to request and dispatch autonomous vehicles from our fleet for times and locations with surging customer demand, or to areas that are often underserved.</p><p>It’s through this platform that we’ll be able to deploy our fleet of autonomous vehicles. And that’s when the possibilities open up.</p><p>We want to complement human-driven ride-hailing services with our self-driving vehicles — deploying them based on need — and help our partners expand into areas that aren’t regularly served while keeping prices affordable. We want all types of businesses and organizations, including non-profit service providers and even individuals, to have equal opportunity to expand and enhance their role in the community.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TcgWpt-XpHTHJglJjKFv8Q.jpeg" /></figure><p>With the help of our partner cities, we can also identify public transportation dead zones where people may not have a way to get to work. <a href="https://www.brookings.edu/research/missed-opportunity-transit-and-jobs-in-metropolitan-america/">According</a> to the Brookings Institution, 70 percent of jobs are not reachable by public transportation for a typical metropolitan resident in the United States. For the 30 percent of jobs that are reachable, the commute still takes an average of 90 minutes. Working with a network of service partners on our platform, we are committed to prioritizing the needs of places and residents like these by providing accessibility options with our autonomous vehicles.</p><p>In the realm of goods delivery, we want to support small and medium-size businesses who may want to take advantage of delivery services but don’t have the ability to tap into complex, expensive logistics systems. We can help small businesses that may not be able to afford a dedicated delivery vehicle, or medium-size businesses that may have a vehicle that is underutilized and therefore a burden on their operations. These types of businesses can choose to use our self-driving vehicles instead.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*C3dEOtL8eFOiZdFE8cA9Jg.jpeg" /></figure><p>What this means is our fleet will be able to provide extra support to restaurants who want to deliver more during peak dinner times, as well as to a local florist who could fill additional orders with an available vehicle. Even shops that don’t offer delivery but are interested in the service as a growth opportunity can tap into the system we’re building. With our platform, we can enable local businesses to scale in ways that wouldn’t have been possible before.</p><p>This is why we’re pleased to be teaming up with Postmates and conducting pilot programs with the service this year. Like us, Postmates is on a mission to help people unlock the best of their cities by creating a reliable on-demand “anything” network — one that gives people better access to the goods and services they need in their daily lives, while also helping local businesses increase sales by reaching new customers all over town.</p><p>Additionally, we plan to expand the work we’ve started with Lyft and Domino’s, both of which are helping us design for the best possible customer experience. In our first phase of research with the team at Domino’s, for example, we found customers enjoyed the voice instructions that played over speakers mounted on the self-driving vehicle explaining how to get their pizza from the car upon arrival at their house. Many even wanted to talk back to the car, saying, “Bye,” upon its departure. That insight is helping us to design experiential elements not just for delivery modes, but for all self-driving vehicle interactions.</p><p><a href="https://medium.com/self-driven/how-pizza-is-helping-us-design-our-self-driving-future-a78720818e99">How Pizza Is Helping Us Design Our Self-Driving Future</a></p><p>The truth is that autonomous vehicles have the potential to change the way cities operate, and they will affect far more people than just those looking to hail a ride. When you combine the power and reach of self-driving vehicles with a human touch, the result is tighter communities within our biggest cities, and stronger relationships between businesses and customers. By working together, we can create a future in which the full range of transportation needs within all of our communities can be met.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=27a05a2b082e" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/coming-to-a-city-near-you-test-driving-our-autonomous-vehicle-business-27a05a2b082e">Coming to a City Near You: Test-Driving Our Autonomous Vehicle Business</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/coming-to-a-city-near-you-test-driving-our-autonomous-vehicle-business-27a05a2b082e?source=rss----7192a82ca09a---4" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>390</wp:post_id>
		<wp:post_date><![CDATA[2018-01-09 17:01:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-09 17:01:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[coming-to-a-city-near-you-test-driving-our-autonomous-vehicle-business]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driven/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/coming-to-a-city-near-you-test-driving-our-autonomous-vehicle-business-27a05a2b082e?source=rss----7192a82ca09a---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[814]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Motor City Homecoming: Why Moving Back to Detroit Strengthens Self-Driving Development</title>
		<link>https://fifthlevel.ai/archives/391</link>
		<pubDate>Thu, 14 Dec 2017 21:37:44 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/a79ce66e0729</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Sherif Marakby, Ford Vice President, Autonomous Vehicles and Electrification</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MrJMoZozABqfqdLFO7DFtg.jpeg" /></figure><p>It’s easy to get sentimental about homecomings. At Ford, this homecoming is a long time in the making.</p><p>Beginning in 2018, we’ll be reestablishing our presence in Detroit by moving our autonomous and electric vehicle business development teams to the city’s Corktown neighborhood. This is a special move for several reasons, especially since Detroit is where our visionary founder got his start, learning the lessons he needed to make his dreams of empowering people a reality.</p><p>Similarly, relocating our employees is just the beginning of our next chapter in Detroit. Shifting our teams into “The Factory” — a 45,000-square-foot building with a history that spans more than a century — has implications that go far beyond optics: This is a decision that will help strengthen the development of our self-driving vehicles and services. Employees will work hand in hand with our product development, purchasing and marketing, sales and service teams back in Dearborn — where we are undergoing an exciting campus transformation — and will help us move toward our winning aspiration of being the world’s most trusted mobility company, designing smart vehicles for a smart world.</p><p>As a company, we are committed to doing our part to improve the entire system of transportation — whether it is moving people from one destination to another or delivering goods. Since we want to deploy self-driving vehicles in a way that enhances people’s lives and improves urban living, it is especially important to understand exactly how these vehicles will need to operate in big cities that are already tackling challenges such as congestion, lack of access, pollution and unpredictable accidents.</p><p>While we’re already conducting extensive research into how autonomous vehicles would interact with people on a daily basis, moving more than 220 of our employees into Corktown early next year will only boost our efforts. As a byproduct of living and working in a bustling city neighborhood, employees will develop intimate knowledge of the opportunities and challenges that come with getting around in an urban environment. Being able to identify problems and generate solutions will be a process that will inform our development as a result.</p><p>After all, unlocking the true potential of self-driving technology requires asking questions that go well beyond vehicle mechanics. Moving into a thriving Detroit neighborhood like Corktown allows us to do just that, giving our teams a chance to explore infrastructure needs more deeply, as well as less-discussed areas such as curb management, all of which will need to be optimized to support the effective use of self-driving vehicles in the future.</p><p>Additionally, with both our autonomous and electric teams working side by side, we can build on our plan to develop our first self-driving car as a hybrid electric vehicle. Employees will get an up-close and personal view of how urban businesses and residents go about their days, offering us the opportunity to develop practical solutions that ensure our self-driving cars can meet their needs when the technology goes live on day one.</p><p>Detroit is a place we’ve always called “home,” and returning to the city is about a lot more than reestablishing a connection with our past — it’s about focusing on what’s next. This is where we’ll build our self-driving car business and define our electric vehicle strategy, reclaiming a historic space to help build a more sustainable future. Just like Henry Ford did in Detroit more than 100 years ago, we’re looking ahead to create solutions that will help us all move forward more easily.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a79ce66e0729" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/motor-city-homecoming-why-moving-back-to-detroit-strengthens-self-driving-development-a79ce66e0729">Motor City Homecoming: Why Moving Back to Detroit Strengthens Self-Driving Development</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/motor-city-homecoming-why-moving-back-to-detroit-strengthens-self-driving-development-a79ce66e0729?source=rss----7192a82ca09a---4" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>391</wp:post_id>
		<wp:post_date><![CDATA[2017-12-14 21:37:44]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-12-14 21:37:44]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[motor-city-homecoming-why-moving-back-to-detroit-strengthens-self-driving-development]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driven/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/motor-city-homecoming-why-moving-back-to-detroit-strengthens-self-driving-development-a79ce66e0729?source=rss----7192a82ca09a---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[812]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>How We’re Working with Universities to Stay on the Cutting Edge of Research</title>
		<link>https://fifthlevel.ai/archives/438</link>
		<pubDate>Tue, 21 Nov 2017 13:34:19 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/5be60d99e265</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Bryan Salesky, CEO, Argo AI</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2L005H3aZlgw0XXp5YcSug.jpeg" /></figure><p>Developing reliable self-driving cars requires pushing the envelope in computer science and artificial intelligence. While we’re applying the latest techniques in computer vision and machine learning to the work we’re doing every day, some of the most advanced research is being done at the university level. That’s why it’s critical to stay connected to the academic community, so we can cultivate the young minds that will help us bring cutting-edge work out of the lab and into the real world.</p><p>To do this, we’ve formed unique affiliations with <a href="https://www.cmu.edu/">Carnegie Mellon University</a> and <a href="http://www.gatech.edu/">Georgia Institute of Technology</a> to work with three world-class faculty members. Simon Lucey, Deva Ramanan and James Hays are collaborating with Argo AI to push the limits in computer vision and machine learning. These research scientists are playing an instrumental role in developing the core technologies that will allow self-driving cars both to see and understand the world around them, and to predict road user behavior.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hE_XrU6U5_6LCpU5sWWYtg.jpeg" /><figcaption>Professors Deva Ramanan, Ph. D. (left) and Simon Lucey, Ph. D. (right) of Carnegie Mellon University</figcaption></figure><p>While Lucey, Ramanan and Hays are spending time at Argo AI , they are also benefiting from Argo funding that supports student research covering a wide range of issues pertaining to self-driving cars. We’re backing their campus work and supporting the time spent with their university colleagues to ensure we are investing in the future of robotics.</p><p>Last year, fewer than 60,000 students graduated in the field of computer and information science, <a href="https://nces.ed.gov/programs/digest/d16/tables/dt16_322.10.asp">according</a> to the National Center for Education Statistics. Yet there are close to 500,000 computing jobs <a href="https://code.org/promote">available</a> right now across the United States. So it’s important we support the faculty’s continued presence and engagement on campus, as this talent shortage can only be addressed if industry and academia work together to support research and invest wisely in educational programs that get students more involved in science and technology.</p><p>The following collaborations reflect one way Argo is investing in that future and looking to apply lessons from academia to make self-driving cars a reality.</p><p><strong>Deva Ramanan </strong>is<strong> </strong>an associate professor at the Carnegie Mellon University Robotics Institute, where his research interests span computer vision and machine learning. With a focus on visual recognition, Ramanan’s work involves training computer programs to identify people by distinguishing different body parts and comparing them against a trove of human and nonhuman models.</p><p>“Making sure self-driving cars can accurately identify people in all of their different shapes, sizes and positions is an essential step to establishing their safety and reliability,” said Ramanan. “I look forward to developing solutions for this problem with Argo, while continuing to stay connected with my students at Carnegie Mellon and uncovering new areas of research for them to explore.”</p><p>Prior to this, Ramanan was an associate professor at the University of California, Irvine. He was awarded the David Marr Prize in 2009 and the PASCAL VOC Lifetime Achievement Prize in 2010. He earned an NSF Career Award in 2010, the UCI Chancellor’s Award for Excellence in Undergraduate Research in 2011 and the PAMI Young Researcher Award in 2012. Popular Science named Ramanan one of its “Brilliant 10” researchers in 2012.</p><p>Also from Carnegie Mellon is <strong>Simon Lucey,</strong> an associate research professor at the Robotics Institute. Lucey leads the organization’s <a href="http://www.cs.cmu.edu/~CI2CV">CI2CV</a> Computer Vision Lab, which is engaged in cutting-edge research developing technology in computer vision and machine learning. His work includes training computers to extract geometric information from images and videos, novel approaches for applying vision and learning to embedded devices, and developing ways for computer systems to effectively read facial actions and body behavior.</p><p>“Working with Argo is a great opportunity to develop systems that can more readily interpret and anticipate the 3D world,” said Lucey. “Not only will students at Carnegie Mellon benefit from this collaboration by spotting fertile ground for further research, but we’ll be able to make great strides in preparing autonomous vehicles for the real world.”</p><p>Prior to his current position, Lucey spent five years as a principal research scientist at Australia’s leading federal research agency, the Commonwealth Scientific and Industrial Research Organization. Lucey was awarded the prestigious Australian Research Council’s Future Fellowship in 2009, and holds numerous NSF awards. He also has research gifts from Apple, Adobe, Samsung and Bosch.</p><p><strong>James Hays</strong> is an associate professor at Georgia Tech’s School of Interactive Computing, where he focuses on using internet-scale data and crowd-sourcing to improve scene understanding and allow smarter image synthesis and manipulation. His research interests span computer vision, graphics, robotics and machine learning.</p><p>“The opportunities and challenges of creating perception systems for autonomous driving are significant,” said Hays. “At Argo AI, we have the opportunity to train algorithms at a scale beyond what is possible in academia, but we also have the challenge of making a system that works reliably in a real world that’s not as well-behaved as carefully curated academic data sets. These challenges will help identify interesting longer-term research areas for my lab at Georgia Tech.”</p><p>Previously, Hays was the Manning assistant professor of computer science at Brown University. He did postdoctoral work at Massachusetts Institute of Technology, earning his Ph.D. from Carnegie Mellon University in 2009. Hays is the recipient of the Sloan fellowship and NSF CAREER award.</p><p>Along with our academic partners, Argo is dedicated to making transportation safer, and more accessible, affordable and convenient for all. Our unique collaboration with Carnegie Mellon and Georgia Tech is enabling us to push through our vision to deploy self-driving cars for the future, while ensuring the academic community has the tools to develop the next generation of computer scientists.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5be60d99e265" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/how-were-working-with-universities-to-stay-on-the-cutting-edge-of-research-5be60d99e265">How We’re Working with Universities to Stay on the Cutting Edge of Research</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/how-were-working-with-universities-to-stay-on-the-cutting-edge-of-research-5be60d99e265?source=rss-ba51b2aa6fa1------2" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>438</wp:post_id>
		<wp:post_date><![CDATA[2017-11-21 13:34:19]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-11-21 13:34:19]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[how-were-working-with-universities-to-stay-on-the-cutting-edge-of-research]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@ArgoAI]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/how-were-working-with-universities-to-stay-on-the-cutting-edge-of-research-5be60d99e265?source=rss-ba51b2aa6fa1------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[808]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Nissan&#039;s Autonomous Tech Is Also For . . . Self-Parking Slippers?</title>
		<link>https://fifthlevel.ai/archives/641</link>
		<pubDate>Fri, 26 Jan 2018 17:36:23 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/228624/nissan-autonomous-tech-parking-slippers/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[Does anyone else want to see a robot battle between these slippers and a Roomba? <p><b><a href="https://www.motor1.com/news/228624/nissan-autonomous-tech-parking-slippers/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>641</wp:post_id>
		<wp:post_date><![CDATA[2018-01-26 17:36:23]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-26 17:36:23]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[nissans-autonomous-tech-is-also-for-self-parking-slippers]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/videos/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/228624/nissan-autonomous-tech-parking-slippers/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Q Inspiration Concept Reveals The Face Of Infinitis To Come</title>
		<link>https://fifthlevel.ai/archives/642</link>
		<pubDate>Mon, 15 Jan 2018 22:37:45 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/227132/infiniti-q-inspiration-concept-debut/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[This striking appearance is Infiniti's future design language. <p><b><a href="https://www.motor1.com/news/227132/infiniti-q-inspiration-concept-debut/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>642</wp:post_id>
		<wp:post_date><![CDATA[2018-01-15 22:37:45]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-15 22:37:45]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[q-inspiration-concept-reveals-the-face-of-infinitis-to-come]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/videos/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/227132/infiniti-q-inspiration-concept-debut/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple now has 27 self-driving Lexus cars on the road in California</title>
		<link>https://fifthlevel.ai/archives/732</link>
		<pubDate>Thu, 25 Jan 2018 16:57:29 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/01/25/apple-now-has-27-self-driving-lexus-cars-on-the-road-in-california</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/24537-32205-21115-23739-170427-Lexus-2-l-l.jpg" alt="Article Image" border="0" /> <br><br> The state of California has now permitted Apple to have 27 self-driving cars on its public roads, as the company expands its homegrown automated driving technology, internally known as &quot;Project Titan.&quot; <p><b><a href="https://appleinsider.com/articles/18/01/25/apple-now-has-27-self-driving-lexus-cars-on-the-road-in-california" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>732</wp:post_id>
		<wp:post_date><![CDATA[2018-01-25 16:57:29]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-25 16:57:29]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-now-has-27-self-driving-lexus-cars-on-the-road-in-california]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/01/25/apple-now-has-27-self-driving-lexus-cars-on-the-road-in-california]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[815]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Autonomous Intelligent Driving has partnered with Mapillary</title>
		<link>https://fifthlevel.ai/archives/749</link>
		<pubDate>Tue, 16 Jan 2018 14:05:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://web159.s124.goserver.host/?p=2856</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Der Beitrag <a rel="nofollow" href="http://aid-driving.eu/autonomous-intelligent-driving-has-licensed-the-mapillary-vistas-dataset/">Autonomous Intelligent Driving has partnered with Mapillary</a> erschien zuerst auf <a rel="nofollow" href="http://aid-driving.eu">AID</a>.</p> <p><b><a href="http://aid-driving.eu/autonomous-intelligent-driving-has-licensed-the-mapillary-vistas-dataset/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>749</wp:post_id>
		<wp:post_date><![CDATA[2018-01-16 14:05:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-16 14:05:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[autonomous-intelligent-driving-has-partnered-with-mapillary]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/autonomous-intelligent-driving-has-licensed-the-mapillary-vistas-dataset/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple AI director talks advances in machine learning &amp; mapping for self-driving car platform</title>
		<link>https://fifthlevel.ai/archives/751</link>
		<pubDate>Sun, 10 Dec 2017 03:53:23 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/17/12/09/apple-ai-director-talks-advances-in-machine-learning-mapping-for-self-driving-car-platform</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/23965-30909-23305-29261-applecar-testbed2-l-l.jpg" alt="Article Image" border="0" /> <br><br> Apple's director of AI research, Ruslan Salakhutdinov, gave peers a small glimpse into the company's self-driving platform this week, discussing some internal projects at the NIPS machine learning conference. <p><b><a href="https://appleinsider.com/articles/17/12/09/apple-ai-director-talks-advances-in-machine-learning-mapping-for-self-driving-car-platform" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>751</wp:post_id>
		<wp:post_date><![CDATA[2017-12-10 03:53:23]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-12-10 03:53:23]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-ai-director-talks-advances-in-machine-learning-mapping-for-self-driving-car-platform]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/17/12/09/apple-ai-director-talks-advances-in-machine-learning-mapping-for-self-driving-car-platform]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[811]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple rumored to be using Arizona proving grounds to test self-driving cars</title>
		<link>https://fifthlevel.ai/archives/752</link>
		<pubDate>Tue, 28 Nov 2017 21:08:28 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/17/11/28/apple-rumored-to-be-using-arizona-proving-grounds-to-test-self-driving-cars</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/23814-30504-damfrxtwktpcogkbaogx-l.jpg" alt="Article Image" border="0" /> <br><br> Apple may be leasing former Fiat Chrysler proving grounds in Surprise, Ariz. to test its self-driving car platform in a range of different conditions, according to a report. <p><b><a href="https://appleinsider.com/articles/17/11/28/apple-rumored-to-be-using-arizona-proving-grounds-to-test-self-driving-cars" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>752</wp:post_id>
		<wp:post_date><![CDATA[2017-11-28 21:08:28]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-11-28 21:08:28]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-rumored-to-be-using-arizona-proving-grounds-to-test-self-driving-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/17/11/28/apple-rumored-to-be-using-arizona-proving-grounds-to-test-self-driving-cars]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[810]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple research paper details LiDAR-based 3D object recognition for autonomous vehicle navigation</title>
		<link>https://fifthlevel.ai/archives/753</link>
		<pubDate>Wed, 22 Nov 2017 05:06:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/17/11/22/apple-research-paper-details-lidar-based-3d-object-recognition-for-autonomous-vehicle-navigation</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/23760-30373-171121-LiDAR-l.jpg" alt="Article Image" border="0" /> <br><br> Apple researchers are pushing forward with efforts to bring autonomous vehicle systems to public roads, and last week published an academic paper outlining a method of detecting objects in 3D point clouds using trainable neural networks. While still in its early stages, the technology could mature to improve accuracy in LiDAR navigation solutions. <p><b><a href="https://appleinsider.com/articles/17/11/22/apple-research-paper-details-lidar-based-3d-object-recognition-for-autonomous-vehicle-navigation" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>753</wp:post_id>
		<wp:post_date><![CDATA[2017-11-22 05:06:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-11-22 05:06:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-research-paper-details-lidar-based-3d-object-recognition-for-autonomous-vehicle-navigation]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/17/11/22/apple-research-paper-details-lidar-based-3d-object-recognition-for-autonomous-vehicle-navigation]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[809]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Two from AeroAstro named to Aviation Week&#039;s “20 Twenties” for 2018</title>
		<link>https://fifthlevel.ai/archives/1138</link>
		<pubDate>Wed, 17 Jan 2018 19:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/mit-aeroastro-aguilar-brown-named-aviation-week-20-twenties-0117</guid>
		<description></description>
		<content:encoded><![CDATA[<p>MIT aeronautics and astronautics graduate students Alexa Aguilar and Arthur J. Brown have been selected as recipients of <a href="http://aviationweek.com/future-aerospace/aviation-week-network-announces-20-twenties-winners-2018">Aviation Week Network’s</a> 2018 “Tomorrow’s Technology Leaders: The 20 Twenties” awards.</p> <p>The awards recognize 20 students nominated by universities for academic performance, civic contributions, research, or design projects. The program is part of an effort to create and awareness by technology hiring managers, students, and faculty of elements that contribute to business and academic success.</p> <p>Aguilar works in AeroAstro’s <a href="http://starlab.mit.edu/">Space Telecommunications, Astronomy, and Radiation Laboratory</a> (STAR Lab) on Cubesat Lasercommunication Infrared CrosslinK, a cubesat mission collaboration including NASA's Ames Research Center, the University of Florida, and MIT. A&nbsp;mission objective is to demonstrate a laser crosslink between two spacecraft at 20 megabits per second.</p> <p>“I’m responsible for managing the optical link budgets, performing a trade study on receivers using a time-to-digital converter versus a traditional analog-to-digital converter, and potentially designing a novel optical receiver to replace commercial off-the-shelf components,” Aguilar says. “For this mission, I helped with the engineering model assembly that identified problem areas in the initial design, which were later fixed for the flight mode.”</p> <p>Professor and STAR Lab Director Kerri Cahoy says&nbsp;Aguilar&nbsp;“has been supporting our nanosatellite laser communications project sponsored by NASA, and she continues to support other MIT projects that she worked on in the past.”</p> <p>Cahoy&nbsp;praises&nbsp;Aguilar for her “sharp intellect, high productivity, cheerful energy, and outreach and advocacy for space exploration and innovation, which have made an impact on our group and the department.”</p> <p>Brown’s research focuses on on-demand aviation —&nbsp;specifically, an air taxi service using small, autonomous, vertical-takeoff-and-landing, battery-powered electric aircraft.</p> <p>“The proposed service offers numerous advantages over existing transport solutions, including greatly reduced commute times, by avoiding gridlock; lower energy costs, due to the use of electricity instead of gasoline; reduced environmental impact in terms of noise, greenhouse gases, lead, and other emissions, and due to the use of electric propulsion; and lower or no pilot operating costs, due to autonomy,”&nbsp;Brown says.</p> <p>Brown is an officer of MIT’s <a href="https://www.media.mit.edu/publications/the-academy-of-courageous-minority-engineers-a-model-for-supporting-minority-graduate-students-in-the-completion-of-science-and-engineering-degrees/">Academy of Courageous Minority Engineers</a> and a member of the <a href="http://gsc.mit.edu/">Graduate Student Council’s</a> Diversity and Inclusion subcommittee.</p> <p>“In my opinion, Arthur’s work exceeds all published investigation and produces tools to make industry-grade decisions for on-demand aviation,” says AeroAstro Professor Wesley Harris, Brown’s thesis advisor.</p> <p>Harris also praises&nbsp;Brown’s involvement with organizations focusing on underrepresented students. “He’s influenced the MIT administration to structure programs and activities that enable advancement of underrepresented students, and done so with a positive, firm approach,” he says.</p> <p>Aviation Week Network president Greg Hamilton says 20 Twenties&nbsp;recognition is built on “three pillars of what the aerospace industry values most: learning, civic service, and high-value research. This year’s winners reflect these pillars, while bringing to the fore the innovation and creativity that are hallmarks for this generation.”</p> <p>Aguilar and Brown will be honored at Aviation Week’s Annual Laureates Awards on&nbsp;March 1 in Washington.</p>
<p><b><a href="http://news.mit.edu/2018/mit-aeroastro-aguilar-brown-named-aviation-week-20-twenties-0117" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1138</wp:post_id>
		<wp:post_date><![CDATA[2018-01-17 19:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-17 19:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[two-from-aeroastro-named-to-aviation-weeks-20-twenties-for-2018]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/mit-aeroastro-aguilar-brown-named-aviation-week-20-twenties-0117]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>What We Learned Driving an Autonomous Vehicle for 24 Hours Straight</title>
		<link>https://fifthlevel.ai/archives/2069</link>
		<pubDate>Tue, 09 Jan 2018 00:25:22 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/587defe151bd</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>Drive.ai’s First Drive-a-thon</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fFe6nGuMP9uv-lx-." /></figure><p>At Drive.ai, we’re a team of over 100 people working to build the <a href="https://medium.com/@drive.ai/introducing-drive-ai-and-a-new-vision-for-self-driving-4334ed967c62">world’s best autonomous driving platform</a> — to power a transformation in the way people, packages, and pizza get from one place to another.</p><p>Indeed, the world of mobility is ready for a change. In major metropolitan areas, a third or more of cars on the road on any given block are simply looking for a parking space [1]. In the world of tomorrow, however, individual vehicle ownership will plummet. Fleets of intelligent, autonomous vehicles will roam the roads, dynamically adjusting to the ebb and flow of the commute day. While our world is becoming more automated at a startling pace, the role of humans in this new transportation ecosystem will not disappear, certainly not overnight. So if we do want to build a fleet the can be on the road 24/7, what does that take?</p><p>Last month, we ran a 24-hour endurance event with the goal to see what it takes to achieve maximum autonomous vehicle uptime on a small scale. We had already driven at all times of day and in all kinds of conditions, but we wanted to further explore what it takes to run an autonomous <em>fleet</em> at maximum capacity. We called it the Drive-a-thon, and here’s how it went and what we learned.</p><p><strong>The Drive-a-thon!</strong></p><p>To start, we set ambitious goals: 400 autonomous miles, 90% autonomous uptime in a variety of different operating conditions, and most importantly, <em>zero</em> safety incidents. (We defined uptime as safety driver aboard, autonomous system engaged, doors closed, and vehicle on the road.) Our geofence of operation on urban and suburban streets had speed limits ranging from 25 to 40 mph, so hitting 400 miles would not be as simple as hopping on Highway 101-S and setting the cruise control. The whole event would take place on surface streets in Mountain View, CA: from 9am on November 16th until 9am on November 17th, 2017. We made it a company bonding activity; many employees spent the night at the office to cheer on the supporting cast.</p><p>We had one “primary” autonomous vehicle running continuously for 24 hours, and others coming on- and off-line as well throughout the event. Autonomous ride-hailing is a crucial component of our business model and a key piece of our technology stack, so successfully handling the pick-up and drop-off of some Drive.ai employees, friends, and family was another goal of the Drive-a-thon. We recently launched an upgraded version of our ride hailing app, and the Drive-a-thon provided a great opportunity for internally testing new use cases.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/688/0*wai1_UlT8HMFgEaU." /></figure><p><em>Sneak peak of Drive.ai’s ride hailing app</em></p><p>Today, it takes a lot of careful planning to keep even a single autonomous vehicle functioning for 24 hours straight. For starters, we are required by law to have a trained safety driver behind the wheel whenever we are on public roads in California. Our safety drivers demonstrate an even higher level of attentiveness and control than the average person on the road that is <em>actually </em>driving. However, one cannot expect a driver, supervising or otherwise, to be able to stay alert forever. For us, this meant shifts; we split the 24 hour period among five of our trained autonomous vehicle operators.</p><p>In addition to drivers, members of our mechanical team were on call back at Drive.ai headquarters, ready at a moment’s notice to attend to anything from a flat tire to a faulty sensor. Our infrastructure team was also on standby, having just installed an upgrade to our data logging system. One underappreciated aspect of autonomous vehicle operation is the data pipeline; our vehicles log multiple gigabytes per minute of sensor and telemetry data, for which we have ample onboard storage capacity for typical testing. For an endurance event, however, this meant “hot swapping,” or changing out logging drives without powering down the system.</p><p>Finally, we wanted this event to be fun. We invited the whole company, plus many friends and family, to come by: to keep the operations team company, to track our progress, and of course, to go on rides in our autonomous vehicles!</p><p>On the morning of our event, Murphy’s law of autonomous vehicle testing held true, and we awoke to a steady downpour of rain, which was forecast to persist throughout the day. Although we had plenty of <a href="https://www.youtube.com/watch?v=GMvgtPN2IBU">experience testing in rain in the prodigious wetness from the start of the year</a>, this would be our most thorough soaking to date. Nevertheless, we got right to it. At 9am, we were up and running.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*17UIfAe96YVe_7fM." /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/622/0*isDeNhS4O-bOa123." /></figure><p><em>Cruising along for a rainy commute. New safety driver, same rain</em></p><p>Interspersed throughout the day, we ran our own internal ridesharing app and completed dozens of rides, all while keeping our sights on the 90% autonomous uptime target.</p><p>As the evening rolled in, the downpour had lessened, and our driving world became a little drier and a lot darker. We passed the uptime halfway mark at 9pm, and were on decent pace to meet our goals, but knew that it was going to be a challenge to keep up the momentum for another twelve hours. Driving at night is more challenging from a perception standpoint. However, driving later into the evening meant that we encountered fewer cars on the roads — giving us a modest uptick in the distance we were able to cover per hour.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/673/0*0qrB9y_6WdDIuOJU." /></figure><p><em>Tents around our office. Autonomous driving was even more</em><strong><em> “</em></strong><em>in tents” than usual</em></p><p>When midnight rolled around, we decided to have some fun back at the office. We set up an indoor campsite for the all of our dedicated personnel that were staying overnight, ordered pizza, played games, and cheered on the operations team as they worked late into the night.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/712/0*id14PcfmTsDjOzzZ." /></figure><p><em>Snack break!</em></p><p>The wee hours of the morning were comparatively quiet on the roads (given the rainy maelstrom of the previous day), but held their own challenge. Keeping safety drivers vigilant and alert 20 hours in is crucial, so for each shift, we had at least one guest in the car, taking pictures, playing music, and enjoying our new autonomous in car UI and UX (blog post on that to come!).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/956/0*JwHo8W9a38o6kAd0." /></figure><p><em>Cruising late at night</em></p><p>As the sun rose, traffic picked up again, and we embarked on the third rush hour of our Drive-a-thon. During the final shift, from 7am until the checkered flag waved at 9am, it was amazing to see an autonomous vehicle run as smoothly in the twenty-fourth hour of operation as the first. For any vehicle, operating for 24 hours — with only one 5-minute stop for gas — is a lot. Despite the challenge, we were able to do it: keeping the sensors, computers, displays, and auxiliary systems alive and well for the entirety of the 24 hour event. A few minutes after 9am, we pulled back into the garage at HQ. The Drive-a-thon was a success!</p><p><strong>The Results: By the Numbers</strong></p><p>We had set aggressive goals for the event: 90% autonomous uptime, 400 autonomous miles, and zero safety incidents.</p><p>For the 24 hour period, our primary vehicle was in autonomous mode for 22 hours and 40 minutes, effectively 94% uptime.<strong> </strong>The main contributors to our downtime were changing out safety drivers (~35 minutes), swapping out data drives and related software maintenance (~25 minutes), addressing a software bug (~15 minutes), and one stop for gas (~5 minutes). When we passed the finish line at 9am on November 17th, we had driven 410 miles, meaning our average speed, including stops, was 17 mph for the event. Finally, and most importantly, we concluded our event safely.</p><p><strong>Insights: What does it take to run an autonomous vehicle fleet?</strong></p><p>The introduction of autonomous vehicles is transformative not only in how we move people and goods, but also for our world as a whole: land use, infrastructure, the insurance industry, wireless communications, and more will change radically over the coming decades. It’s not just about building autonomous vehicles, but also about developing the ecosystem in which they will operate. There are numerous considerations in running an efficient autonomous fleet:</p><ul><li><strong>Parking lots turn into Maintenance Centers: </strong>The average vehicle is parked for 22+ hours a day [2], while autonomous fleets will target 80%+ utilization. While this means a lower percentage of vehicles parked at any given time, the parking needs of an AV are different. Autonomous fleets will need<strong> </strong>specialized maintenance centers to serve as charging or fueling stops, data depots, sensor calibration sites, cleaning facilities, and more. We are in store for a dramatic repurposing of today’s paved paradise.</li><li><strong>Pick-up and Drop-off:</strong> Autonomous mobility is hailed as a panacea for today’s “last mile problem,” but there are many aspects of a contemporary taxi experience that we take for granted. How do you make sure that the correct person has gotten into the vehicle? What if the person or people entering need help with luggage? What if an individual has a disability? This “last meter problem” is a critical consideration for fleets operating without humans at the helm. Negotiating a safe place to pull over for pick up or drop off is an often overlooked problem for an autonomous vehicle. Is it acceptable to block a driveway or a bike lane for a few minutes? If there is space, we need to detect the edge of the curb correctly, and ideally not use one that is painted red. If there is no legal space to stop, what do we do? Negotiating a contingency plan with a human driver is a common practice, but robots are still very new to this.</li><li><strong>AV + EV</strong>: Electrification will no doubt play a huge role in the future of autonomous mobility. Energy cost for the hybrid platform we used in our drive-a-thon was about $0.09/mile, while a contemporary electric vehicle like the Tesla Model S is closer to $0.04/mile [3]. However, until charging and battery swapping technology matures further, hybrid vehicles will be at the sweet spot for maximizing uptime. Charging overnight is not an option for a vehicle at 90% utilization. Electric vehicle adoption has not yet fully taken off, but we believe that the introduction of AVs will be a major force in improving the infrastructure needed to support next generation energy storage technology. Businesses will be the ones who will really want this benefit, which is another reason Drive.ai has positioned to sell to business fleets.</li><li><strong>Personnel: </strong>Today, many jurisdictions require a trained safety driver, either behind the wheel or supervising the behavior of the autonomous vehicle remotely. New jobs will be created for other tasks as well, such as: vehicle cleaning, general hardware maintenance, replacement and re-calibration of sensors from wear and tear, customer support, and more. Even in a safer, more automated world, we must still expect the unexpected, and many customers will prefer a human to assist in when issues arise. Will robots be able to empathize and react to unexpected situations as well as humans? This is an important consideration for us as we develop a user experience that builds trust and understanding with our customers.</li><li><strong>Data: </strong>Autonomous vehicles generate gigabytes of data every minute, all of which must be accessible later for us to measure ride quality, track any critical incidents, and improve the technology over time. Some data can be streamed off-board via cellular, but most of it must be stored on-board, which means hard drives must be swapped regularly. The story hardly ends once the data is off the vehicle; it goes straight into our multi-stage data pipelines, which process data for future replays, debugging, visualization, annotation, and other analysis.</li><li><strong>Human Robot Interaction:</strong> A passenger in an autonomous vehicle has opted into an experience where a robot is handling all aspects of the driving task. Inside the car, our riders can visualize information about what the car sees and how it is making decisions. Outside of the vehicle, however, interaction between our vehicle and its environment is a different story. Once we have removed the human driver from the picture, we no longer have the ability to make eye contact and emote with the outside world, at least not in the same way. We know that the transition from driven to driverless will not happen overnight. We have designed our systems to build trust in the future of autonomous mobility, not only with our customers, but in the heterogeneous world into which our technology must embark. Human-robot-interaction is yet another crucial consideration.</li></ul><p>Looking back, our Drive-a-thon was a fantastic learning experience. Deploying an endurance event on a single autonomous vehicle gave us an excellent microcosm for understanding what it will take to keep a <em>fleet </em>of autonomous vehicles running at maximum capacity. And in 2018, we have our sights set on doing just that!</p><p>— — — —</p><ol><li>Shoup, D., and Campbell, “H. Gone parkin’.” The New York Times. &lt;http://www.nytimes.com/2007/03/29/opinion/29shoup.html&gt; 2007.</li><li>Morris, D. “Today’s Cars are Parked 95% of the Time.” Fortune. &lt;http://www.reinventingparking.org/2013/02/cars-are-parked-95-of-time-lets-check.html&gt; 2016.</li><li>Noland, David. “Life with Tesla Model S: One Year and 15,000 Miles Later.” Green Car Reports. &lt;https://www.greencarreports.com/news/1090685_life-with-tesla-model-s-one-year-and-15000-miles-later&gt; 2014.</li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=587defe151bd" width="1" height="1"> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2069</wp:post_id>
		<wp:post_date><![CDATA[2018-01-09 00:25:22]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-09 00:25:22]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[what-we-learned-driving-an-autonomous-vehicle-for-24-hours-straight]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@drive.ai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@drive.ai/what-we-learned-driving-an-autonomous-vehicle-for-24-hours-straight-587defe151bd?source=rss-37851f177ff8------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Self-driving vehicles: outlook for 2018</title>
		<link>https://fifthlevel.ai/archives/2477</link>
		<pubDate>Sat, 30 Dec 2017 16:21:42 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.driverless-future.com/?p=1110</guid>
		<description></description>
		<content:encoded><![CDATA[<p>After the race for fully self-driving cars heated up in 2016, 2017 became a year with exciting developments &#8211; many billions of dollars changed hands for self-driving car related acquisitions<sup>(1)</sup> and many collaborations were started<sup>(2)</sup>. But besides progress, 2017 also showed some limits: Tesla was plagued by defections from their SDC team and had to cancel their fully autonomous coast to coast test drive planned for the end of 2017 and shift the target date for their fully self-driving capability back by 2 years. Volvo effectively cancelled their planned Gothenburg self-driving car trials (by changing the scope to a test of driver assistance technologies).</p>
<p>Nevertheless an enormously important milestone for the adoption of self-driving cars has been reached in 2017: Waymo is now operating self-driving cars without test driver on public roads in Phoenix, Arizona. Five years ago we had expected this milestone to be reached around 2018. This unequivocally demonstrates to the world that self-driving cars are viable and that they can no longer be considered a technology that is half a decade or more away.</p>
<p>This milestone (and the multitude of achievements of the many actors involved up to the end of 2017) also change the dynamics of the global distributed innovation process around autonomous vehicles. It is beginning to shift from the typical chaotic process involving many different actors with little formal organization trying out different paths and approaches to a more mature process. The acquisitions we have seen in 2016 and 2017 are an indicator that the global innovation process is consolidating and getting closer to move from the early stage of an innovation process (called &#8216;fluid phase&#8217; in innovation theory) to the &#8216;transitional phase&#8217;. This is a major step typically associated with deep structural changes in the innovation process. We may reach a peak in the number of companies competing to develop self-driving car technology in 2018 or 2019 before seeing a market shakeout thereafter.</p>
<p>For the auto-industry, 2018 will be a crucial year because the time is running out for most OEMs to ensure that they can weather the changes caused by self-driving cars and &#8211; maybe even more importantly &#8211; that they can identify, understand and profit from new opportunities. There can be no doubt that car sales will come under pressure in the early 2020ies as autonomous mobility services (both for local and long-distance travel) grab a significant share of the mobility market, consumers fundamentally change their car-buying behavior and some emerging markets adjust their traffic infrastructure policies to take advantage of self-driving car technology.</p>
<p>OEMS that have not yet committed to a serious self-driving car strategy risk their medium-term competitive position. With every year that passes, it will become more difficult to adjust to the changes coming to the auto industry. It is unlikely that OEMs will be able to offset losses in demand for privately owned cars by building self-driving cars and selling or leasing them to mobility service providers (or operating them themselves). When the industry gradually comes to accept the reality of shrinking demand for automobiles, it will become more and more difficult to adjust because profitability will fall rapidly and with it the ability to change. Several automakers are likely to fall into the Kodak trap: Kodak was the first company to develop a digital camera. It always understood digital cameras but it failed to reinvent its business model in time and then was unable to turn around the already sinking ship which was bleeding from all sides. The European, Korean and Japanese auto makers need to strongly accelerate their self-driving car activities if they want to survive the coming turmoils of the next decade. General Motors seems to be the only OEM which currently is well positioned in this space. It is pity that Daimler, one of the earliest pioneers of self-driving cars, appears to be content to mostly watch from the sidelines.</p>
<p>In 2018, we can expect another change in the maturing innovation process: The focus will start to move away from the core technical issues towards the implications for the automobile as a whole (its interior, exterior and structural design, its supporting and sales infrastructure etc.) and towards the business models associated with self-driving cars. There are many more use cases for self-driving technology than just ferrying people around; many of these use cases have strong services components which OEMs (or their challengers) need to embrace. 2018 may also be the year where players beyond the auto industry start to seriously consider the implications, opportunities and risks. Retail will be deeply affected by dramatically falling local distribution costs. In the next decade nany supermarkets will have to close their doors as products can be delivered conveniently (and with very customer-flexible timing) to the doorstep. Hospitals, care and emergency services will need to adjust to fewer traffic related injuries. Most industries will need to consider the implications and opportunities associated with significantly lower transportation costs (affecting both inbound and outbound logistics and possibly providing new product or service opportunities). Cities, countries, architects, construction firms need to start planning for a future where mobility is provisioned differently and where space and capacity requirements for transportation are changing. Railways and transportation companies need to consider the challenges which will be raised by autonomous mobility services providers. Self-driving cars and machines will also have major impact on construction and agriculture industries and provide new opportunities there.</p>
<p>2018 may also be the year where the opposition to self-driving cars finds their voice. While self-driving cars have enormous benefits they will eliminate many jobs (not just professional drivers but also in the auto industry and many other industries). Society needs to find ways to cope with the fundamental changes that result from software-based devices with capabilities which some call &#8216;artificial&#8217; intelligence and we all need to consider in depth how the fabric of society will be impacted and what changes on the different subsystems of society will be necessary. This process should not be underestimated and requires a major, multi-disciplinary effort.</p>
<p>In 2018, every business, organization, political actor, and any forward-thinking individual should take the time to look beyond the technicalities of self-driving cars and carefully consider their implications, opportunities and risks!</p>
<p>Update: 2018-01-16: Removed a sentence stating that BMW seemed to have reduced the extents of its targets for autonomy in 2021.</p>
<p>(1) Acquisitions: Intel/MobilEye, Delphi/Nutonomy, Cruise Automation/Strobe, Ford/ArgoAI (Ford majority stakeholder), ArgoAI/Princeton Lightwave</p>
<p>2) Cooperations: Waymo with Lyft, Avis and others, Daimler/Bosch, Baidu/Apollo platform, Intel Alliance, Uber/Daimler</p> <p><b><a href="http://www.driverless-future.com/?p=1110" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2477</wp:post_id>
		<wp:post_date><![CDATA[2017-12-30 16:21:42]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-12-30 16:21:42]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[self-driving-vehicles-outlook-for-2018]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?feed=rss2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?p=1110]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Fleets of self-driving cars will not be limited to high-density urban areas</title>
		<link>https://fifthlevel.ai/archives/2478</link>
		<pubDate>Thu, 30 Nov 2017 19:46:12 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.driverless-future.com/?p=1101</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Self-driving mobility services are likely to be adopted quickly in high density urban areas. In these regions, car ownership is likely to fall significantly. Several studies have shown that one autonomous taxi might provide sufficient transport capacity to service the mobility needs which are currently fulfilled with 6 to 10 privately owned vehicles. These studies have considered local motorized mobility in large cities such as Ann Arbor, Lisbon, Austin and others.</p>
<p>But how will autonomous fleets impact mobility and car ownership in less densely populated areas? About <a href="https://factfinder.census.gov/bkmk/table/1.0/en/PEP/2016/GCTPEPANNR.US24PR">86%</a> of the US population live in metropolitan statistical areas (i.e. areas that have a relatively high population density at its core). These are not limited to the great cities and agglomerations on the west and east coast but include much smaller areas such as the Grand Forks metropolitan area which comprises 2 adjacent counties in North Dakota and Minnesota with about 100,000 inhabitants (in 2014) and a population density of 11 people per km square. Of course, self-driving mobility services will be very viable in the urban core of this metro area where about 60,000 people live. The remaining 40,000 people living in rural parts of this area have significant, predictable mobility demands for trips towards and back from the urban core. Thus there is a potential for self-driving mobility services even in the outer, less densely populated parts of metropolitan statistical areas. A further <a href="https://factfinder.census.gov/bkmk/table/1.0/en/PEP/2016/GCTPEPANNR.US25PR">8.6%</a> of the US population live in in micropolitan statistical areas (i.e. areas which are centered around an urban cluster with at least 10,000 but less than 50,000 people). The remaining 6% of the US population live neither in a metropolitan nor a micropolitan statistical area (see the white area in <a href="https://www2.census.gov/geo/maps/metroarea/us_wall/Feb2013/cbsa_us_0213_large.gif" target="_blank">the map of metropolitan and micropolitan areas in US</a>). It is instructive to consider their situation.</p>
<p>Let&#8217;s take <a href="https://en.wikipedia.org/wiki/Sidney,_Montana">Sidney,</a> Montana as an example (<a href="https://www.google.com/maps/place/Sidney,+MT">Google maps</a>): This is a small town with just about 5,000 inhabitants in eastern Montana. It is far away from more populated centers. The nearest larger city is Williston, ND with about 20,000 inhabitants at a distance of 70km. The next city with more than 100,000 inhabitants is Billings, MT at a distance of about 430km. There seems to be a significant mobility demand for trips to Billings: more than four flights leave for Billings every day (airfare about 40 USD). Uber is already active in this town and popular destinations/pick up spots include the airport, high school, health center and Holiday Inn Express.</p>
<p>The US currently has a stock of about<a href="https://www.rita.dot.gov/bts/sites/rita.dot.gov.bts/files/publications/national_transportation_statistics/html/table_01_11.html" target="_blank"> 240 million light duty cars</a>, which translates to about 750 cars for a thousand people. Because this ratio is higher in areas with lower population density, there should be significantly more than 5*750=3750 light duty cars in Sidney. Because a large share of the daily trips are local, and because their average speed is high compared to the speed in congested cities, autonomous fleets should be able to provide high-yield mobility services with a relatively small fleet. With a replacement rate of 1 to 7, about 535 self-driving vehicles could theoretically replace the town&#8217;s entire vehicle stock. The local mobility demands of 5000 people are also large enough that a mobility services provider can start with the smallest economically viable fleet size of probably somewhere between 10 and 20 cars and then grow the fleet as demand picks up. The low regional population density has an interesting consequence for non-local trips: The number of typical destinations is small; the number of routes people can travel from/to Sidney is quite limited. Therefore the potential for on-demand shuttles is high; Williston, with it&#8217;s Walmart (about a 1 hour drive) is an obvious target. Such shuttles have another side effect: they can provide the same mobility service to all locations which they pass on their route. Such shuttles therefore effectively will bring access to self-driving mobility services to some very rural dwellings.</p>
<p>Today, households in low density areas of the US have much higher car ownership rates than the rest of the population: there simply are  no viable alternatives. Self-driving cars fundamentally change this situation. Wherever there is a minimum of demand for personal mobility, self-driving mobility services become economically viable. The number of persons needed to sustain a self-driving taxi resource is rather small; towns with just a few thousand of inhabitants should always provide enough demand to allow a small fleet of self-driving taxis to operate. Initially it may only be the seniors who use these services but then households will start to think about the number of cars they really need and gradually demand for these services (and with it, supply) will increase.</p>
<p>In many lower density areas of the United States, car ownership is a prerequisite for finding work and &#8211; as a consequence &#8211; people without cars suffer and economic opportunities are lost. For seniors access to medical services and just getting around can be extremely difficult. The young face similar problems. These examples show that we can expect sufficient demand for self-driving mobility services in most parts of the United States &#8211; including many small towns and even in many areas that have low population densities. The impact of fleets of self-driving cars will not at all be limited to big cities!</p> <p><b><a href="http://www.driverless-future.com/?p=1101" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2478</wp:post_id>
		<wp:post_date><![CDATA[2017-11-30 19:46:12]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-11-30 19:46:12]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[fleets-of-self-driving-cars-will-not-be-limited-to-high-density-urban-areas]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?feed=rss2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?p=1101]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>MIT AgeLab&#039;s Bryan Reimer wins Autos2050 award</title>
		<link>https://fifthlevel.ai/archives/2537</link>
		<pubDate>Wed, 17 Jan 2018 17:45:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/mit-agelab-bryan-reimer-honored-autos2050-award-0117</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Bryan Reimer, an&nbsp;AgeLab&nbsp;research scientist and the associate director of the&nbsp;New England University Transportation Center,&nbsp;has been selected to&nbsp;receive an inaugural <a href="https://autos2050.com/">Autos2050 Driving Innovation Award</a>&nbsp;for his&nbsp;work with vehicle automation.&nbsp;</p> <p>The award is given jointly by the Auto Alliance and the Alliance for Transportation Innovation&nbsp;to leaders shaping the future of auto-mobility.</p> <p>Reimer's work with the Advanced Vehicle Technology Consortium has focused on providing a data-driven understanding of how people engage with vehicle automation, using advanced computer-vision software and big-data analytics. The consortium has already gathered data from&nbsp;78 drivers, 7,146 participant days, and 275,589 miles of travel.</p> <p>This year's winners include CEOs, governors, and members of Congress.&nbsp;</p> <p>"These winners were chosen for making game-changing contributions to the automotive industry over the past year, and they will be honored at the first Autos2050SM&nbsp;conference and Driving Innovation Awards Dinner," says&nbsp;Paul Brubaker, the president and CEO of the&nbsp;Alliance for Transportation Innovation.</p> <p>The awards will be conferred at a reception in Washington on Jan.&nbsp;24.&nbsp;&nbsp;</p>
<p><b><a href="http://news.mit.edu/2018/mit-agelab-bryan-reimer-honored-autos2050-award-0117" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2537</wp:post_id>
		<wp:post_date><![CDATA[2018-01-17 17:45:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-17 17:45:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[mit-agelabs-bryan-reimer-wins-autos2050-award]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/mit-agelab-bryan-reimer-honored-autos2050-award-0117]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>New depth sensors could be sensitive enough for self-driving cars</title>
		<link>https://fifthlevel.ai/archives/2538</link>
		<pubDate>Fri, 22 Dec 2017 04:59:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/new-depth-sensors-could-be-sensitive-enough-self-driving-cars-1222</guid>
		<description></description>
		<content:encoded><![CDATA[<p>For the past 10 years, the Camera Culture group at MIT’s Media Lab has been developing innovative imaging systems — from a camera that can <a href="http://news.mit.edu/2012/camera-sees-around-corners-0321">see around corners</a> to one that can read text in <a href="http://news.mit.edu/2016/computational-imaging-method-reads-closed-books-0909">closed books</a> — by using “time of flight,” an approach that gauges distance by measuring the time it takes light projected into a scene to bounce back to a sensor.</p> <p>In a new paper appearing in <em>IEEE Access</em>, members of the Camera Culture group present a new approach to time-of-flight imaging that increases its depth resolution 1,000-fold. That’s the type of resolution that could make self-driving cars practical.</p> <p>The new approach could also enable accurate distance measurements through fog, which has proven to be a major obstacle to the development of self-driving cars.</p> <p>At a range of 2 meters, existing time-of-flight systems have a depth resolution of about a centimeter. That’s good enough for the assisted-parking and collision-detection systems on today’s cars.</p> <p>But as Achuta Kadambi, a &nbsp;joint PhD student in electrical engineering and computer science and media arts and sciences and first author on the paper, explains, “As you increase the range, your resolution goes down exponentially. Let’s say you have a long-range scenario, and you want your car to detect an object further away so it can make a fast update decision. You may have started at 1 centimeter, but now you’re back down to [a resolution of] a foot or even 5 feet. And if you make a mistake, it could lead to loss of life.”</p> <p>At distances of 2 meters, the MIT researchers’ system, by contrast, has a depth resolution of 3 micrometers. Kadambi also conducted tests in which he sent a light signal through 500 meters of optical fiber with regularly spaced filters along its length, to simulate the power falloff incurred over longer distances, before feeding it to his system. Those tests suggest that at a range of 500 meters, the MIT system should still achieve a depth resolution of only a centimeter.</p> <p>Kadambi is joined on the paper by his thesis advisor, Ramesh Raskar, an associate professor of media arts and sciences and head of the Camera Culture group.</p> <p><strong>Slow uptake</strong></p> <p>With time-of-flight imaging, a short burst of light is fired into a scene, and a camera measures the time it takes to return, which indicates the distance of the object that reflected it. The longer the light burst, the more ambiguous the measurement of how far it’s traveled. So light-burst length is one of the factors that determines system resolution.</p> <p>The other factor, however, is detection rate. Modulators, which turn a light beam off and on, can switch a billion times a second, but today’s detectors can make only about 100 million measurements a second. Detection rate is what limits existing time-of-flight systems to centimeter-scale resolution.</p> <p>There is, however, another imaging technique that enables higher resolution, Kadambi says. That technique is interferometry, in which a light beam is split in two, and half of it is kept circulating locally while the other half — the “sample beam” — is fired into a visual scene. The reflected sample beam is recombined with the locally circulated light, and the difference in phase between the two beams — the relative alignment of the troughs and crests of their electromagnetic waves — yields a very precise measure of the distance the sample beam has traveled.</p> <p>But interferometry requires careful synchronization of the two light beams. “You could never put interferometry on a car because it’s so sensitive to vibrations,” Kadambi says. “We’re using some ideas from interferometry and some of the ideas from LIDAR, and we’re really combining the two here.”</p> <p><strong>On the beat</strong></p> <p>They’re also, he explains, using some ideas from acoustics. Anyone who’s performed in a musical ensemble is familiar with the phenomenon of “beating.” If two singers, say, are slightly out of tune — one producing a pitch at 440 hertz and the other at 437 hertz — the interplay of their voices will produce another tone, whose frequency is the difference between those of the notes they’re singing — in this case, 3 hertz.</p> <p>The same is true with light pulses. If a time-of-flight imaging system is firing light into a scene at the rate of a billion pulses a second, and the returning light is combined with light pulsing 999,999,999 times a second, the result will be a light signal pulsing once a second — a rate easily detectable with a commodity video camera. And that slow “beat” will contain all the phase information necessary to gauge distance.</p> <p>But rather than try to synchronize two high-frequency light signals — as interferometry systems must — Kadambi and Raskar simply modulate the returning signal, using the same technology that produced it in the first place. That is, they pulse the already pulsed light. The result is the same, but the approach is much more practical for automotive systems.</p> <p>“The fusion of the optical coherence and electronic coherence is very unique,” Raskar says. “We’re modulating the light at a few gigahertz, so it’s like turning a flashlight on and off millions of times per second. But we’re changing that electronically, not optically. The combination of the two is really where you get the power for this system.”</p> <p><strong>Through the fog</strong></p> <p>Gigahertz optical systems are naturally better at compensating for fog than lower-frequency systems. Fog is problematic for time-of-flight systems because it scatters light: It deflects the returning light signals so that they arrive late and at odd angles. Trying to isolate a true signal in all that noise is too computationally challenging to do on the fly.</p> <p>With low-frequency systems, scattering causes a slight shift in phase, one that simply muddies the signal that reaches the detector. But with high-frequency systems, the phase shift is much larger relative to the frequency of the signal. Scattered light signals arriving over different paths will actually cancel each other out: The troughs of one wave will align with the crests of another. Theoretical analyses performed at the University of Wisconsin and Columbia University suggest that this cancellation will be widespread enough to make identifying a true signal much easier.</p> <p>“I am excited about medical applications of this technique,” says Rajiv Gupta, director of the Advanced X-ray Imaging Sciences Center at Massachusetts General Hospital and an associate professor at Harvard Medical School. “I was so impressed by the potential of this work to transform medical imaging that we took the rare step of recruiting a graduate student directly to the faculty in our department to continue this work.”</p> <p>“I think it is a significant milestone in development of time-of-flight techniques because it removes the most stringent requirement in mass deployment of cameras and devices that use time-of-flight principles for light, namely, [the need for] a very fast camera,” he adds. “The beauty of Achuta and Ramesh’s work is that by creating beats between lights of two different frequencies, they are able to use ordinary cameras to record time of flight.”</p>
<p><b><a href="http://news.mit.edu/2017/new-depth-sensors-could-be-sensitive-enough-self-driving-cars-1222" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2538</wp:post_id>
		<wp:post_date><![CDATA[2017-12-22 04:59:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-12-22 04:59:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[new-depth-sensors-could-be-sensitive-enough-for-self-driving-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/new-depth-sensors-could-be-sensitive-enough-self-driving-cars-1222]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Unlocking marine mysteries with artificial intelligence</title>
		<link>https://fifthlevel.ai/archives/2539</link>
		<pubDate>Fri, 15 Dec 2017 04:59:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2017/unlocking-marine-mysteries-artificial-intelligence-1215</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Each year the melting of the Charles River serves as a harbinger for warmer weather. Shortly thereafter is the return of budding trees, longer days, and flip-flops. For students of class 2.680 (Unmanned Marine Vehicle Autonomy, Sensing and Communications), the newly thawed river means it’s time to put months of hard work into practice.</p> <p>Aquatic environments like the Charles present challenges for robots because of the severely limited communication capabilities. “In underwater marine robotics, there is a unique need for artificial intelligence — it’s crucial,” says MIT Professor Henrik Schmidt, the course’s co-instructor. “And that is what we focus on in this class.”</p> <p>The class, which is offered during spring semester, is structured around the presence of ice on the Charles. While the river is covered by a thick sheet of ice in February and into March, students are taught to code and program a remotely-piloted marine vehicle for a given mission. Students program with MOOS-IvP, an autonomy software used widely for industry and naval applications.</p> <p>“They’re not working with a toy,” says Schmidt’s co-instructor, Research Scientist Michael Benjamin. “We feel it’s important that they learn how to extend the software — write their own sensor processing models and AI behavior. And then we set them loose on the Charles.”</p> <div class="cms-placeholder-content-video"></div> <p>As the students learn basic programming and software skills, they also develop a deeper understanding of ocean engineering. “The way I look at it, we are trying to clone the oceanographer and put our understanding of how the ocean works into the robot,” Schmidt adds. This means students learn the specifics of ocean environments — studying topics like oceanography or underwater acoustics.&nbsp;</p> <p>Students develop code for several missions they will conduct on the Charles River by the end of the semester. These missions include finding hazardous objects in the water, receiving simulated temperature and acoustic data along the river, and communicating with other vehicles.</p> <p>“We learned a lot about the applications of these robots and some of the challenges that are faced in developing for ocean environments,” says Alicia Cabrera-Mino ’17, who took the course last spring.</p> <p>Augmenting robotic marine vehicles with artificial intelligence is useful in a number of fields. It can help researchers gather data on temperature changes in our ocean, inform strategies to reverse global warming, traverse the 95 percent of our oceans that has yet to be explored, map seabeds, and further our understanding of oceanography.</p> <p>According to graduate student Gregory Nannig, a former navigator in the U.S. Navy, adding AI capabilities to marine vehicles could also help avoid navigational accidents. “I think that it can really enable better decision making,” Nannig explains. “Just like the advent of radar or going from celestial navigation to GPS, we’ll now have artificial intelligence systems that can monitor things humans can’t.”</p> <p>Students in 2.680 use their newly acquired coding skills to build such systems. Come spring, armed with the software they’ve spent months working on and a better understanding of ocean environments, they enter the MIT Sailing Pavilion prepared to test their artificial intelligence coding skills on the recently melted Charles River.</p> <p>As marine vehicles glide along the Charles, executing missions based on the coding students have spent the better part of a semester perfecting, the mood is often one of exhilaration. “I’ve had students have big emotions when they see a bit of AI that they’ve created,” Benjamin recalls. “I’ve seen people call their parents from the dock.”</p> <p>For this artificial intelligence to be effective in the water, students need to combine software skills with ocean engineering expertise. Schmidt and Benjamin have structured 2.680 to ensure students have a working knowledge of these twin pillars of robotic marine vehicle autonomy.</p> <p>By combining these two research areas in their own research, Schmidt and Benjamin hope to create underwater robots that can go places humans simply cannot. “There are a lot of applications for better understanding and exploring our ocean if we can do it smartly with robots,” Benjamin adds.</p>
<p><b><a href="http://news.mit.edu/2017/unlocking-marine-mysteries-artificial-intelligence-1215" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2539</wp:post_id>
		<wp:post_date><![CDATA[2017-12-15 04:59:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-12-15 04:59:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[unlocking-marine-mysteries-with-artificial-intelligence]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2017/unlocking-marine-mysteries-artificial-intelligence-1215]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Cognata Builds Cloud-Based Autonomous Vehicle Simulation Platform with NVIDIA and Microsoft</title>
		<link>https://fifthlevel.ai/archives/3289</link>
		<pubDate>Tue, 09 Jan 2018 14:53:15 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.cognata.com/?p=454</guid>
		<description></description>
		<content:encoded><![CDATA[<p><strong><br />
</strong></p>
<p>(Las Vegas, Nevada – Jan. 9, 2018)<strong>   </strong>Cognata, Ltd., today announced that it is launching the first cloud-based simulation engine for autonomous vehicle validation powered with technologies from NVIDIA and Microsoft. The announcement was made at <a href="http://www.ces.tech/">CES 2018</a> in Las Vegas, where Cognata is exhibiting Jan. 9 – 12, 2018, at <a href="https://ces18.mapyourshow.com/7_0/exhibitor/exhibitor-details.cfm?ExhID=T0006976">Westgate 2301 &amp; 2303</a>, Tech East.</p>
<p>The <a href="http://www.cognata.com/">Cognata</a> platform leverages artificial intelligence, deep learning, and computer vision to provide the only solution capable of validating autonomous vehicles with unlimited scalability today. The <a href="http://www.nvidia.com/page/home.html">NVIDIA</a> DRIVE platform supports autonomous applications with software that helps developers and researchers optimize, validate, and deploy their work. Cognata will run the combined technologies on the <a href="https://azure.microsoft.com/en-us/">Microsoft Azure</a> cloud-based platform. The collaborative solution represents a major milestone for the autonomous vehicle industry, and promises to bring safer self-driving vehicles to market much faster than anticipated.</p>
<p>“As Cognata launches our cloud based simulation platform and removes the hardware barrier from the simulation world, it’s gratifying for us to work with major tech powers like NVIDIA and Microsoft,” Cognata CEO Danny Atsmon says. “There’s a natural synergy in our technologies that will help us continue to develop the most powerful platform available, and ensure that our customers can drive as many miles as they choose, with the cloud-enabled flexibility of adding additional computing resources on the fly.”</p>
<p>NVIDIA DRIVE is a cloud-to-car scalable AI platform that enables autonomous vehicles to understand their environment and drive safely. By training and testing deep neural networks in the datacenter, and then performing real-time, low-latency inferencing in the vehicle, it enables Level 2 through Level 5 driving systems.</p>
<p>“NVIDIA partners with the world’s leading automakers, including Audi, Mercedes-Benz, Volvo and Tesla, and it’s equally important to work with important, dynamic, startups like Cognata,” says Rishi Dhall, Senior Director, Business Development at NVIDIA. “Its GPU-based innovations are fueling new solutions for the transportation industry.”</p>
<p>The alliance with Microsoft’s Azure cloud platform helps Cognata bring the power of scaled cloud computing infrastructure to the automotive world, along with NVIDIA’s cloud-based GPU technology. Car makers are required to accumulate 10 billion miles worth of test drives, which can take years to complete. Cognata’s simulation platform can advance that process by enabling autonomous vehicle manufacturers to log highly realistic virtual test drives on the virtual roadways of its simulated environment, thereby trimming years off the road-testing process.</p>
<p>“Breakthrough cloud technologies like AI and machine learning are rapidly changing how we think about driving—or not driving,” says Doug Seven, Principal Group Product Manager, Microsoft. “To take advantage of the opportunities, car makers are leaning on and working with companies like Cognata who have expertise in advanced simulation technology and with Microsoft because we are one of a few tech platforms that truly have the global cloud infrastructure and have met regulatory commitments required to support connected vehicles.”</p>
<p>Further, the Cognata simulation engine cloud-based services allows auto makers to avoid the expense of substantial upgrades to their infrastructure that the validation process would otherwise require. “We are enabling vehicle OEMs, tier-one and tier-two companies in the autonomous market to drive millions of miles that would normally require multi-million-dollar initial investments in hardware,” Atsmon notes. The platform is not hosted on local servers, so users don’t need to add hardware and are not limited by local server capacity.</p>
<p><strong>About Cognata</strong></p>
<p>Cognata provides a fast lane to autonomous driving with its testing and evaluation solution for self-driving vehicles—a realistic automotive simulation platform where virtual cars travel virtual roads in virtual cities, all remarkably true to real-world conditions. Led by CEO Danny Atsmon, a widely respected expert in ADAS and deep learning, Cognata brings the disruptive potential of artificial intelligence, deep learning, and computer vision to the autonomous driving simulation world. Cognata’s simulated testing and evaluation environment shaves years off the validation time by generating fast, highly accurate results, and eliminates the safety concerns, high costs, and limited scalability of road-testing in the physical world. Cognata was founded in 2016 by a team of experts in deep learning, autonomous vehicles and computer vision. The company is headquartered in Rehovot, Israel, close to the Weizmann Institute of Science. For more information, visit <a href="http://www.cognata.com">http://www.cognata.com</a>.</p>
<p>The post <a rel="nofollow" href="http://www.cognata.com/cognata-builds-cloud-based-autonomous-vehicle-simulation-platform-nvidia-microsoft/">Cognata Builds Cloud-Based Autonomous Vehicle Simulation Platform with NVIDIA and Microsoft</a> appeared first on <a rel="nofollow" href="http://www.cognata.com">Cognata Autonomous Simulation</a>.</p> <p><b><a href="http://www.cognata.com/cognata-builds-cloud-based-autonomous-vehicle-simulation-platform-nvidia-microsoft/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3289</wp:post_id>
		<wp:post_date><![CDATA[2018-01-09 14:53:15]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-09 14:53:15]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[cognata-builds-cloud-based-autonomous-vehicle-simulation-platform-with-nvidia-and-microsoft]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.cognata.com/blog/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.cognata.com/cognata-builds-cloud-based-autonomous-vehicle-simulation-platform-nvidia-microsoft/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>See the Fast Lane to Autonomous Driving Simulation in Action! Visit Cognata at CES 2018</title>
		<link>https://fifthlevel.ai/archives/3290</link>
		<pubDate>Fri, 15 Dec 2017 14:55:40 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.cognata.com/?p=444</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Join <a href="http://www.cognata.com/">Cognata</a> at <strong>Westgate Booth 2301 &amp; 2303, Tech East,</strong> and see the fast lane to autonomous driving in action! <a href="http://www.ces.tech/">CES 2018</a> offers a great chance to experience our remarkably realistic simulated test-drive environment, where actual cities are reproduced with the mile-by-mile detail of an exhilarating video game.</p>
<p>Self-driving carmakers can shave years off the validation process with our solution—and dramatically accelerate time to market.</p>
<p>Road-testing in the physical world is frustratingly slow and expensive, hobbled by limited scalability and rife with safety concerns. Our automotive testing and evaluation platform eliminates those obstacles by simulating a huge number of virtual miles and a broad range of edge cases, then generating speedy, highly accurate results.</p>
<p>Come meet our team of experts in artificial intelligence, deep learning, and computer vision, and learn how Cognata’s solution helps solve one of the automotive industry&#8217;s biggest challenges today—validating autonomous vehicles to ensure those cars are truly prepared to drive safely in the real world.</p>
<p>To Schedule a meeting with our team at CES, please use this link: <a href="http://www.cognata.com/appointment-booking/">http://www.cognata.com/appointment-booking/</a></p>
<p>&nbsp;</p>
<p><img data-attachment-id="448" data-permalink="http://www.cognata.com/see-fast-lane-autonomous-driving-simulation-action-visit-cognata-ces-2018/attachment/000026/" data-orig-file="https://i0.wp.com/www.cognata.com/wp-content/uploads/2017/12/000026.png?fit=1280%2C720" data-orig-size="1280,720" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="000026" data-image-description="" data-medium-file="https://i0.wp.com/www.cognata.com/wp-content/uploads/2017/12/000026.png?fit=300%2C169" data-large-file="https://i0.wp.com/www.cognata.com/wp-content/uploads/2017/12/000026.png?fit=1024%2C576" class="aligncenter wp-image-448" src="https://i0.wp.com/www.cognata.com/wp-content/uploads/2017/12/000026.png?resize=359%2C202" alt="" width="359" height="202" srcset="https://i0.wp.com/www.cognata.com/wp-content/uploads/2017/12/000026.png?resize=300%2C169 300w, https://i0.wp.com/www.cognata.com/wp-content/uploads/2017/12/000026.png?resize=768%2C432 768w, https://i0.wp.com/www.cognata.com/wp-content/uploads/2017/12/000026.png?resize=1024%2C576 1024w, https://i0.wp.com/www.cognata.com/wp-content/uploads/2017/12/000026.png?w=1280 1280w" sizes="(max-width: 359px) 100vw, 359px" data-recalc-dims="1" /></p>
<p>The post <a rel="nofollow" href="http://www.cognata.com/see-fast-lane-autonomous-driving-simulation-action-visit-cognata-ces-2018/">See the Fast Lane to Autonomous Driving Simulation in Action! Visit Cognata at CES 2018</a> appeared first on <a rel="nofollow" href="http://www.cognata.com">Cognata Autonomous Simulation</a>.</p> <p><b><a href="http://www.cognata.com/see-fast-lane-autonomous-driving-simulation-action-visit-cognata-ces-2018/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3290</wp:post_id>
		<wp:post_date><![CDATA[2017-12-15 14:55:40]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-12-15 14:55:40]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[see-the-fast-lane-to-autonomous-driving-simulation-in-action-visit-cognata-at-ces-2018]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.cognata.com/blog/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.cognata.com/see-fast-lane-autonomous-driving-simulation-action-visit-cognata-ces-2018/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Same driver, different vehicle: Bringing Waymo self-driving technology to trucks</title>
		<link>https://fifthlevel.ai/archives/256</link>
		<pubDate>Fri, 09 Mar 2018 12:40:42 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/e55824b55b8f</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uln03d1H6X0Y8iDgSXrQ3Q.jpeg" /></figure><p>Waymo’s mission has always been to make it safe and easy for <em>people</em> and <em>things</em> to move around. So far, the focus has mostly been on <em>people</em>; last fall we put the world’s <a href="https://medium.com/waymo/with-waymo-in-the-drivers-seat-fully-self-driving-vehicles-can-transform-the-way-we-get-around-75e9622e829a">first fleet of fully self-driving cars</a> on public roads in the Phoenix area.</p><p>Now we’re turning our attention to <em>things</em> as well. Starting next week, Waymo will launch a pilot in Atlanta where our self-driving trucks will carry freight bound for Google’s data centers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9MdaEOY-fpxslE9USrP7PA.jpeg" /></figure><p>Over the past year, we’ve been conducting road tests of Waymo’s self-driving trucks in California and Arizona. Our software is learning to drive big rigs in much the same way a human driver would after years of driving passenger cars. The principles are the same, but things like braking, turning, and blind spots are different with a fully-loaded truck and trailer.</p><p>Now we’re headed to Georgia. Atlanta is one of the biggest logistics hubs in the country, making it a natural home for Google’s logistical operations and the perfect environment for our next phase of testing Waymo’s self-driving trucks.</p><p>This pilot, in partnership with Google’s logistics team, will let us further develop our technology and integrate it into the operations of shippers and carriers, with their network of factories, distribution centers, ports and terminals. As our self-driving trucks hit the highways in the region, we’ll have highly-trained drivers in the cabs to monitor systems and take control if needed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hwIKE8Mp-HTId1mJKUz2HQ.jpeg" /></figure><p>We’ve been able to make rapid progress because our driver — Waymo’s self-driving technology — is not only experienced, but adaptable. Our self-driving trucks use the same <a href="https://medium.com/waymo/introducing-waymos-suite-of-custom-built-self-driving-hardware-c47d1714563">suite of custom-built sensors</a> that power our self-driving minivan. They benefit from the same advanced self-driving software that has enabled our cars to go fully driverless in Arizona. And our engineers and AI experts are leveraging the same <a href="https://medium.com/waymo/waymo-reaches-5-million-self-driven-miles-61fba590fafe">five million miles</a> we’ve already self-driven on public roads, plus the <a href="https://medium.com/waymo/simulation-how-one-flashing-yellow-light-turns-into-thousands-of-hours-of-experience-a7a1cb475565">five <em>billion</em> miles</a> we’ve driven in simulation. In short, our near-decade of experience with passenger vehicles has given us a head start in trucking.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EZ5RXeFZVB9jiqIbctE2uw.jpeg" /></figure><p>Trucking is a vital part of the American economy, and we believe self-driving technology has the potential to make this sector safer and even stronger. With Waymo in the driver’s seat, we can reimagine many different types of transportation — from ride-hailing to logistics. So if you’re in the Atlanta area, look for a bright blue Waymo truck making a run!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e55824b55b8f" width="1" height="1"><hr><p><a href="https://medium.com/waymo/same-driver-different-vehicle-bringing-waymo-self-driving-technology-to-trucks-e55824b55b8f">Same driver, different vehicle: Bringing Waymo self-driving technology to trucks</a> was originally published in <a href="https://medium.com/waymo">Waymo</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>256</wp:post_id>
		<wp:post_date><![CDATA[2018-03-09 12:40:42]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-09 12:40:42]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[same-driver-different-vehicle-bringing-waymo-self-driving-technology-to-trucks]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/waymo]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/waymo/same-driver-different-vehicle-bringing-waymo-self-driving-technology-to-trucks-e55824b55b8f?source=rss----7075a35566d9---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[822]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Waymo reaches 5 million self-driven miles</title>
		<link>https://fifthlevel.ai/archives/257</link>
		<pubDate>Wed, 28 Feb 2018 07:59:47 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/61fba590fafe</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Fuld-H7bbGN5vxAd1BrtgQ.jpeg" /></figure><p>The Waymo odometer rolled over to 5,000,000 in February: our self-driving cars have now covered five million miles on public roads.</p><p>That’s a major milestone. Yet what’s really exciting for us is how we’re picking up speed. Our 1st million miles took six years to complete. Our 4th million miles took six months, and this 5th million took just under three months. Today we’re driving as many miles in one day as the average American adult drives in a whole year.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2Uw5zU-yNlvLzhxx." /></figure><p>We’re driving more and learning faster, and we’re doing it in increasingly diverse conditions. We’ve now test driven in 25 U.S. cities, gaining experience in different weather conditions and terrains: from the snowy streets of Michigan, to the steep hills of San Francisco, to the desert conditions of Greater Phoenix. And because the lessons we learn from one vehicle can be shared with the entire fleet, every new mile counts even more.</p><p>What’s more, that real-world experience is multiplied on our private test track and in simulation. Last year alone our software drove 2.7 billion miles in the virtual world, where we create and test entirely new driving scenarios for extra practice.</p><p><strong>Now, take a 360° ride with one of the world’s most experienced drivers</strong></p><p>All this experience is what has allowed us to refine our technology and put our fully self-driving cars on public roads. And to help everyone understand how this works, we’ve created an immersive 360° video that we’re launching today: the <a href="https://www.youtube.com/watch?v=B8R148hFxPw">Waymo 360° experience</a>.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FB8R148hFxPw%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DB8R148hFxPw&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FB8R148hFxPw%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/db291c6a0a910cfe9ef4154c38d18930/href">https://medium.com/media/db291c6a0a910cfe9ef4154c38d18930/href</a></iframe><p>This experience combines footage and real-time data from a trip around Metro Phoenix, Arizona in one of our Chrysler Pacifica hybrid minivans. It puts you at the very heart of the technology, letting you see through the “eyes” of our state-of-the-art sensors, including LiDAR, radar, and cameras. It brings to life how Waymo sees the world around it, recognizes objects like cars and pedestrians, predicts what those things will do, and then plans a safe path ahead.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*i6AToXUfI5-RflCKBVRrBA.gif" /><figcaption><em>Seeing what Waymo “sees” on real city streets</em></figcaption></figure><p>The 360-degree format also gives you the chance to take your first ride in a fully self-driving car — putting you right there in the passenger seat, so you can feel what it’s like to be driven by one of the most experienced drivers on the road.</p><p>You can view <a href="https://www.youtube.com/watch?v=B8R148hFxPw">Waymo 360° experience</a> today on desktop, mobile, or with a virtual reality headset including Cardboard and Daydream. To learn how we recreated the authentic experience of our fully self-driving car, learn more about the <a href="https://medium.com/waymo/recreating-the-self-driving-experience-the-making-of-the-waymo-360-37a80466af49">making-of the video</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=61fba590fafe" width="1" height="1"><hr><p><a href="https://medium.com/waymo/waymo-reaches-5-million-self-driven-miles-61fba590fafe">Waymo reaches 5 million self-driven miles</a> was originally published in <a href="https://medium.com/waymo">Waymo</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>257</wp:post_id>
		<wp:post_date><![CDATA[2018-02-28 07:59:47]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-28 07:59:47]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[waymo-reaches-5-million-self-driven-miles]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/waymo]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/waymo/waymo-reaches-5-million-self-driven-miles-61fba590fafe?source=rss----7075a35566d9---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[819]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Recreating the self-driving experience: the making of the Waymo 360° video</title>
		<link>https://fifthlevel.ai/archives/258</link>
		<pubDate>Wed, 28 Feb 2018 07:59:49 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/37a80466af49</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5i9FNXI7WIcElveIGrJ4Iw.jpeg" /></figure><blockquote>To bring Waymo’s technology to life, we leaned on the unique format of 360 video.</blockquote><p><em>By Meiling Tan, Head of Marketing, Waymo</em></p><p>Waymo has been working on self-driving technology since our early days at Google in 2009. Today, we have the world’s <a href="https://medium.com/waymo/with-waymo-in-the-drivers-seat-fully-self-driving-vehicles-can-transform-the-way-we-get-around-75e9622e829a">first and only fleet of fully self-driving cars</a> on public roads, and have self-driven <a href="https://medium.com/@waymo/waymo-reaches-5-million-self-driven-miles-61fba590fafe">over 5 million miles</a>.</p><p>Today we’re introducing a <a href="https://www.youtube.com/watch?v=B8R148hFxPw">Waymo 360° experience</a> — an immersive video that recreates in full detail a self-driving ride. Go ahead: take control of the camera to see through the “eyes” of our car and take a ride in a Waymo yourself. On mobile, just move your phone around to explore — or use it with a virtual reality headset like Cardboard for a more immersive experience. If you’re on desktop, just drag the video around your screen.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FB8R148hFxPw%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DB8R148hFxPw&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FB8R148hFxPw%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/db291c6a0a910cfe9ef4154c38d18930/href">https://medium.com/media/db291c6a0a910cfe9ef4154c38d18930/href</a></iframe><p>This is the latest step in our efforts to help people understand our technology and how it can make it safer and easier for everyone to get around. It follows on the launch of <a href="https://medium.com/waymo/lets-talk-self-driving-cars-72743d39cad8">Let’s Talk Self-Driving</a>, the world’s first self-driving public education campaign, and our first-ever <a href="https://medium.com/waymo/waymos-safety-report-how-we-re-building-a-safer-driver-ce5f1b0d4c25">Safety Report</a>.</p><p><strong>Seeing the world as Waymo does</strong></p><p>Over the years, we’ve seen that one of the best ways to explain how our technology works is to show people an engineering tool called <a href="https://www.youtube.com/watch?v=fbWeKhAPMig">x-view</a>, which displays what the self-driving car “sees” all around it. To bring this experience to life, we leaned on the unique format of 360 video to demonstrate Waymo’s constant 360-degree field of view. Viewers can see firsthand what it means to have millions of data points surrounding them in all directions at any given moment — and perhaps even compare that to how human drivers perceive the world.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*i6AToXUfI5-RflCKBVRrBA.gif" /><figcaption><em>Seeing what Waymo “sees” on real city streets</em></figcaption></figure><p>To create an authentic representation of Waymo’s technology, we started by driving on the streets of Metro Phoenix, Arizona with one of our self-driving Chrysler Pacifica hybrid minivans. Then we worked with our engineers to use the data from those trips as the basis of our 360° video.</p><p>The laser visualization in the video presents a simplified view of the LiDAR data collected by our sensors. The radar animation illustrates how these sensors bounce radar waves off surrounding objects to gauge their position and speed. The traffic lights in the camera scene are examples of visible objects that were detected by our high-resolution cameras. The predictions of what each pedestrian, cyclist or vehicle will do next in the video are based off real-time predictions made by our self-driving software.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RNw96Uw5omzG_9n5Vol8NQ.jpeg" /><figcaption><em>What Waymo sees in “x-view” is translated it into the Waymo 360</em>°<em> experience</em></figcaption></figure><p><strong>Taking a ride with Waymo</strong></p><p>We’ve also found that people who ride with Waymo tend to relax quickly as they follow the car’s live field of view in the passenger screen, and realize that the ride itself feels just like an ordinary ride from A to B. This 360° video allows you to get as close to this experience as possible. You’re watching the car self-drive on a real city street, as you ride in the back.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*E4SC_nMptFeQK7z9dnNx8A.gif" /><figcaption><em>Taking a ride in Waymo’s fully self-driving car</em></figcaption></figure><p>As our technical progress continues, we’ll continue with more public education and awareness initiatives as well. For now, enjoy the Waymo 360° experience!</p><p><em>The Waymo 360</em>°<em> experience was created in partnership with Google Creative Lab.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=37a80466af49" width="1" height="1"><hr><p><a href="https://medium.com/waymo/recreating-the-self-driving-experience-the-making-of-the-waymo-360-video-37a80466af49">Recreating the self-driving experience: the making of the Waymo 360° video</a> was originally published in <a href="https://medium.com/waymo">Waymo</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>258</wp:post_id>
		<wp:post_date><![CDATA[2018-02-28 07:59:49]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-28 07:59:49]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[recreating-the-self-driving-experience-the-making-of-the-waymo-360-video]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/waymo]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/waymo/recreating-the-self-driving-experience-the-making-of-the-waymo-360-video-37a80466af49?source=rss----7075a35566d9---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[820]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>We’re Going to Miami: The First Proving Ground for Our Self-Driving Service</title>
		<link>https://fifthlevel.ai/archives/388</link>
		<pubDate>Tue, 27 Feb 2018 15:01:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/6ea7721de0a5</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Sherif Marakby, Ford Vice President, Autonomous Vehicles and Electrification</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*BSTnbFMT1gfJLB3oXsxYjQ.jpeg" /></figure><p>Watching some of the world’s greatest athletes over the past week had me thinking about all of the individual elements that come together for the extraordinary performances we see from these competitors. It’s overwhelming to think about the dedication, commitment and the amount of time these athletes had to put in to get every little step, spin, jump — any move you can think of — just right to win a medal.</p><p>Now, I’m inspired more than ever because Ford is in a similar position as we pull together our self-driving vehicle business. We’ve spent years researching and developing self-driving technology, studying changing customer behaviors, serving some of the largest fleets in the country with help from our dealers, and working with governments big and small. Now it’s time to pull it all together and head for the finish line!</p><p>So now, we’re headed to Florida to test and prove out our business model. With the help of Miami-Dade County, we’re taking our service directly to the streets of Miami and Miami Beach.</p><p>Miami-Dade Mayor Carlos A. Giménez is a champion of innovative technology and applying it to help improve life for residents of the county. He’s on the forefront of thinking about the future of transportation, leading a county that already offers a diverse set of transportation modes ranging from ride-hailing services and rail to buses and bike sharing. He understands the potential of self-driving vehicles and how they can fit in, interact with, and enhance all of those modes and more. That’s why this collaboration makes so much sense.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fk4g8c3XR8f8%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dk4g8c3XR8f8&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fk4g8c3XR8f8%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/0d836d5d83cae0c703c710ed0f5273d7/href">https://medium.com/media/0d836d5d83cae0c703c710ed0f5273d7/href</a></iframe><p>There’s clearly a need for innovation. Miami was recently listed as the 10th most congested city in the world, according to the <a href="http://inrix.com/scorecard/">Inrix Global Traffic Scorecard</a>, and the fifth most congested city in the United States. Miami commuters spend an average of 64 hours in congestion per year during peak time periods — or nearly 10 percent of their total drive time. That’s just not right, yet it serves as a reminder that people always need to be at the center of our plan, so we’re setting out to solve for their pain points.</p><p>To understand what Miami-Dade residents would experience with self-driving vehicle service, the first part of our presence will involve pilot programs throughout the year with our partners, starting with Domino’s and Postmates. What we learn from this customer experience research will be applied to the design of our purpose-built self-driving vehicle that we plan to launch in 2021 to support the expansion of our service.</p><p>The types of questions we’re trying to answer include:</p><ul><li>Before a self-driving vehicle makes a delivery, how will employees stock it and send it off?</li><li>At the end of its journey, how will customers interact with the vehicle to retrieve their food or groceries, and how far from their homes are they willing to walk to get it?</li><li>What benefits could and should people get from a self-driving experience?</li></ul><p>Another way to think about it is to consider the costs of convenience. Today, deliveries can be made to someone’s door, though there is usually an extra charge involved. Oftentimes, drivers illegally double-park when they can’t find a space, potentially causing traffic congestion for others. A self-driving vehicle won’t need to be tipped and it won’t park illegally. So, from the outset we understand there are both hurdles and benefits to self-driving delivery in cities and we intend to learn all of these ins and outs so that we can serve people in a way that’s most intuitive and convenient. Our Domino’s pilot is already up and running in Miami, and we’re finalizing plans to launch one with Postmates in March.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Br-zhL927csDJ_AikqX4Vg.jpeg" /></figure><p>In parallel to creating the best customer experience possible, we will continue to develop the self-driving technology powering our vehicles by expanding testing in partnership with Argo AI. Running a self-driving business in any city requires a comprehensive understanding of local laws and the unique driving habits of residents, which is where Argo specializes. A new fleet of Argo vehicles is already on the streets, mapping the roads and accumulating miles that will help us improve the way they move through cities. The effort will grow throughout the year as we add vehicles and expand areas of testing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*G4bSrnC_XMq9F2eR3ifYBg.jpeg" /></figure><p>Having all these vehicles on the road means we have to face some practical realities too. There’s no doubt in my mind that we will develop fully self-driving vehicles, but one thing we don’t talk about much is the need to take care of the fleet after it spends a long day or night on the road. These vehicles will need to be maintained, repaired and cleaned, including prying sticky gum off the seats. Taking care of a vehicle — whether it drives itself or not — requires space and manpower.</p><p>That’s why we’re establishing our first autonomous vehicle operations terminal in Miami. Situated close to downtown, it will be the base from which we’ll develop our vehicle management processes and house our test fleet. The vehicles will be washed and have their sensors cleaned here; routine maintenance will be conducted, including troubleshooting problems that arise and more.</p><p>Additionally, we will work closely with our extensive dealer network in the area, looking for ways to integrate and incorporate their operations and capabilities into our terminal. A healthy dealer network is critical for support, as dealers can help with repairs and conduct parts deliveries and other services. Before thousands of self-driving vehicles can hit the streets, we have to be prepared to manage large, high-tech fleets efficiently, and the steps we’re taking in Miami represent a significant stride in that process.</p><p>After working out numerous muscles and developing different techniques to navigate a future of self-driving cars, we’re heading onto the stage in Miami-Dade to see how our business performs. By bringing all of our different development tracks together to test in unison, we’re putting ourselves in the best position to analyze our execution, determine what works well and improve what doesn’t. This way, we can quickly expand our service and take it to other cities when the time comes.</p><p>The development of self-driving vehicles hints at a whole new way of moving people and goods, but it doesn’t change our way of doing business — placing the customer above all else. We’re looking forward to connecting with the people of Miami-Dade County and becoming part of their community as we increase awareness for our efforts and build a service they can confidently rely on day in and day out.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6ea7721de0a5" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/were-going-to-miami-the-first-proving-ground-for-our-self-driving-service-6ea7721de0a5">We’re Going to Miami: The First Proving Ground for Our Self-Driving Service</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/were-going-to-miami-the-first-proving-ground-for-our-self-driving-service-6ea7721de0a5?source=rss----7192a82ca09a---4" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>388</wp:post_id>
		<wp:post_date><![CDATA[2018-02-27 15:01:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-27 15:01:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[were-going-to-miami-the-first-proving-ground-for-our-self-driving-service]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driven/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/were-going-to-miami-the-first-proving-ground-for-our-self-driving-service-6ea7721de0a5?source=rss----7192a82ca09a---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[818]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Automotive Edge Computing Consortium Shifts Connected Car Market into High Gear</title>
		<link>https://fifthlevel.ai/archives/550</link>
		<pubDate>Thu, 22 Feb 2018 13:48:18 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://aecc.org/?p=188</guid>
		<description></description>
		<content:encoded><![CDATA[<p>AT&T, DENSO, Ericsson, Intel, KDDI, NTT, NTT DOCOMO, Sumitomo Electric, Toyota, and Toyota InfoTechnology Center Form Alliance to Drive Ecosystem toward a Big Data Future<br />
WAKEFIELD, Mass., – USA – Feb. 22, 2018 – The Automotive Edge Computing Consortium (AECC) today announced it has formally launched operations as a cross-industry alliance.</p>
<p>The post <a rel="nofollow" href="https://aecc.org/automotive-edge-computing-consortium-shifts-connected-car-market-high-gear/">Automotive Edge Computing Consortium Shifts Connected Car Market into High Gear</a> appeared first on <a rel="nofollow" href="https://aecc.org">Automotive Edge Computing Consortium</a>.</p> <p><b><a href="https://aecc.org/automotive-edge-computing-consortium-shifts-connected-car-market-high-gear/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>550</wp:post_id>
		<wp:post_date><![CDATA[2018-02-22 13:48:18]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-22 13:48:18]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[automotive-edge-computing-consortium-shifts-connected-car-market-into-high-gear]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://aecc.org/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://aecc.org/automotive-edge-computing-consortium-shifts-connected-car-market-high-gear/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Some Lessons From the Waymo (Alphabet) Versus Uber Theft of Trade Secret Litigation</title>
		<link>https://fifthlevel.ai/archives/589</link>
		<pubDate>Wed, 14 Feb 2018 11:15:12 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.ipwatchdog.com/?p=93528</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Although the amount of the settlement was far less than $2.7 billion in amount sought by Waymo, the settlement apparently did include a payment from Uber of 0.34% of Uber equity—or about $244.8 million in stock based on a $72 billion valuation of Uber... Both sides had a lot riding on the outcome of the trial. In addition to the billions in damages, Waymo was seeking an injunction to prevent Uber from using any technology that may have originated from Waymo, which would have been a huge set back for Uber’s program. Indeed, during his first day of being questioned, the former CEO of Uber, Travis Kalanick, agreed that developing autonomous vehicles amounts to an “existential question” for Uber, and that the market for driverless cars is likely to be “winner-take-all.”</p>
<p>The post <a rel="nofollow" href="http://www.ipwatchdog.com/2018/02/14/waymo-uber-theft-trade-secret-litigation/id=93528/">Some Lessons From the Waymo (Alphabet) Versus Uber Theft of Trade Secret Litigation</a> appeared first on <a rel="nofollow" href="http://www.ipwatchdog.com">IPWatchdog.com | Patents &amp; Patent Law</a>.</p> <p><b><a href="http://www.ipwatchdog.com/2018/02/14/waymo-uber-theft-trade-secret-litigation/id=93528/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>589</wp:post_id>
		<wp:post_date><![CDATA[2018-02-14 11:15:12]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-14 11:15:12]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[some-lessons-from-the-waymo-alphabet-versus-uber-theft-of-trade-secret-litigation]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/tag/autonomous-vehicles/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/2018/02/14/waymo-uber-theft-trade-secret-litigation/id=93528/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Ford Picks Miami For A Major Autonomous Car Test</title>
		<link>https://fifthlevel.ai/archives/640</link>
		<pubDate>Tue, 27 Feb 2018 19:06:52 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/234385/ford-autonmous-car-test/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[In certain parts of Miami, folks can already get a pizza delivery from an autonomous vehicle. <p><b><a href="https://www.motor1.com/news/234385/ford-autonmous-car-test/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>640</wp:post_id>
		<wp:post_date><![CDATA[2018-02-27 19:06:52]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-27 19:06:52]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[ford-picks-miami-for-a-major-autonomous-car-test]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/videos/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/234385/ford-autonmous-car-test/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple likely to be &#039;all-in or all-out&#039; of self-driving cars within 2 years, analyst argues</title>
		<link>https://fifthlevel.ai/archives/729</link>
		<pubDate>Mon, 05 Mar 2018 17:11:03 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/03/05/apple-likely-to-be-all-in-or-all-out-of-self-driving-cars-within-2-years-analyst-argues</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/25056-33442-applecar-2017small-l.jpg" alt="Article Image" border="0" /> <br><br> Apple's experimentation with autonomous car technology is likely to veer away from creating a simple platform and back towards a fully self-designed vehicle, one analyst argues. <p><b><a href="https://appleinsider.com/articles/18/03/05/apple-likely-to-be-all-in-or-all-out-of-self-driving-cars-within-2-years-analyst-argues" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>729</wp:post_id>
		<wp:post_date><![CDATA[2018-03-05 17:11:03]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-05 17:11:03]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-likely-to-be-all-in-or-all-out-of-self-driving-cars-within-2-years-analyst-argues]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/03/05/apple-likely-to-be-all-in-or-all-out-of-self-driving-cars-within-2-years-analyst-argues]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[821]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>California green-lights driverless car testing statewide, operations can begin in April</title>
		<link>https://fifthlevel.ai/archives/730</link>
		<pubDate>Tue, 27 Feb 2018 03:06:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/02/26/california-green-lights-driverless-car-testing-statewide-operations-can-begin-in-april</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/24963-33189-24927-33115-applecar-testbed2-l-l.jpg" alt="Article Image" border="0" /> <br><br> As anticipated, the California Department of Motor Vehicles on Monday received approval from the California Office of Administrative Law to enact a set of regulations that will allow companies to test remotely operated autonomous vehicles on public roads. <p><b><a href="https://appleinsider.com/articles/18/02/26/california-green-lights-driverless-car-testing-statewide-operations-can-begin-in-april" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>730</wp:post_id>
		<wp:post_date><![CDATA[2018-02-27 03:06:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-27 03:06:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[california-green-lights-driverless-car-testing-statewide-operations-can-begin-in-april]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/02/26/california-green-lights-driverless-car-testing-statewide-operations-can-begin-in-april]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[817]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Self-driving test cars with remote control backup could hit California roads in April</title>
		<link>https://fifthlevel.ai/archives/731</link>
		<pubDate>Fri, 23 Feb 2018 20:14:31 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/02/23/self-driving-test-cars-with-remote-control-backup-could-hit-california-roads-in-april</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/24927-33115-applecar-testbed2-l.jpg" alt="Article Image" border="0" /> <br><br> If approved, new Californian regulations could allow companies like Apple to test their self-driving platforms with remote backup, instead of putting a human behind the wheel. <p><b><a href="https://appleinsider.com/articles/18/02/23/self-driving-test-cars-with-remote-control-backup-could-hit-california-roads-in-april" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>731</wp:post_id>
		<wp:post_date><![CDATA[2018-02-23 20:14:31]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-23 20:14:31]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[self-driving-test-cars-with-remote-control-backup-could-hit-california-roads-in-april]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/02/23/self-driving-test-cars-with-remote-control-backup-could-hit-california-roads-in-april]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[816]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Q&amp;A Session with our CTO Alexandre Haag</title>
		<link>https://fifthlevel.ai/archives/748</link>
		<pubDate>Thu, 01 Feb 2018 14:32:48 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://aid-driving.eu/?p=3725</guid>
		<description></description>
		<content:encoded><![CDATA[Der Beitrag <a href="http://aid-driving.eu/qa-session-with-our-cto-alexandre-haag/" rel="nofollow">Q&amp;A Session with our CTO Alexandre Haag</a> erschien zuerst auf <a href="http://aid-driving.eu" rel="nofollow">AID</a>.

<b><a href="http://aid-driving.eu/qa-session-with-our-cto-alexandre-haag/" target="_blank" rel="noopener">Read the original article</a></b>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>748</wp:post_id>
		<wp:post_date><![CDATA[2018-02-01 14:32:48]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-01 14:32:48]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[qa-session-with-our-cto-alexandre-haag]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/qa-session-with-our-cto-alexandre-haag/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Programming drones to fly in the face of uncertainty</title>
		<link>https://fifthlevel.ai/archives/1137</link>
		<pubDate>Mon, 12 Feb 2018 05:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/mit-csail-programming-drones-fly-face-uncertainty-0212</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Companies like Amazon have big ideas for drones that can deliver packages right to your door. But&nbsp;even putting aside the policy issues, programming drones to fly through cluttered spaces like cities is difficult. Being able to avoid obstacles while traveling at high speeds is computationally complex, especially for small drones that are limited in how much they can carry onboard for real-time processing.</p> <p>Many existing approaches rely on intricate maps that aim to tell drones exactly where they are relative to obstacles, which isn’t particularly practical in real-world settings with unpredictable objects. If their estimated location is off by even just a small margin, they can easily crash.</p> <div class="cms-placeholder-content-video"></div> <p>With that in mind, a team from MIT’s <a href="http://csail.mit.edu">Computer Science and Artificial Intelligence Laboratory</a> (CSAIL) has developed NanoMap, a system that allows drones to consistently fly 20 miles per hour through dense environments such as forests and warehouses.</p> <p>One of NanoMap’s key insights is a surprisingly simple one: The system considers the drone’s position in the world over time to be uncertain, and actually models and accounts for that uncertainty.</p> <p>“Overly confident maps won’t help you if you want drones that can operate at higher speeds in human environments,” says graduate student Pete Florence, lead author on a new related paper. “An approach that is better aware of uncertainty gets us a much higher level of reliability in terms of being able to fly in close quarters and avoid obstacles.”</p> <p>Specifically, NanoMap uses a depth-sensing system to stitch together a series of measurements about the drone’s immediate surroundings. This allows it to not only make motion plans for its current field of view, but also anticipate how it should move around in the hidden fields of view that it has already seen.</p> <p>“It’s kind of like saving all of the images you’ve seen of the world as a big tape in your head,” says Florence. “For the drone to plan motions, it essentially goes back into time to think individually of all the different places that it was in.”</p> <p>The team’s tests demonstrate the impact of uncertainty. For example, if NanoMap wasn’t modeling uncertainty and the drone drifted just 5 percent away from where it was expected to be, the drone would crash more than once every four flights. Meanwhile, when it accounted for uncertainty, the crash rate reduced to 2 percent.</p> <p>The paper was co-written by Florence and MIT Professor Russ Tedrake alongside research software engineers John Carter and Jake Ware. It was recently accepted to the IEEE International Conference on Robotics and Automation, which takes place in May in Brisbane, Australia.</p> <p>For years computer scientists have worked on algorithms that allow drones to know where they are, what’s around them, and how to get from one point to another. Common approaches such as simultaneous localization and mapping (SLAM) take raw data of the world and convert them into mapped representations.</p> <p>But the output of SLAM methods aren’t typically used to plan motions. That's where researchers often use methods like “occupancy grids,” in which many measurements are incorporated into one specific representation of the 3-D world.</p> <p>The problem is that such data can be both unreliable and hard to gather quickly. At high speeds, computer-vision algorithms can’t make much of their surroundings, forcing drones to rely on inexact data from the inertial measurement unit (IMU) sensor, which measures things like the drone’s acceleration and rate of rotation.</p> <p>The way NanoMap handles this is that it essentially doesn’t sweat the minor details. It operates under the assumption that, to avoid an obstacle, you don’t have to take 100 different measurements and find the average to figure out its exact location in space; instead, you can simply gather enough information to know that the object is in a general area.</p> <p>“The key difference to previous work is that the researchers created a map consisting of a set of images with their position uncertainty rather than just a set of images and their positions and orientation,” says Sebastian Scherer, a systems scientist at Carnegie Mellon University’s Robotics Institute. “Keeping track of the uncertainty has the advantage of allowing the use of previous images even if the robot doesn’t know exactly where it is and allows in improved planning.”</p> <p>Florence describes NanoMap as the first system that enables drone flight with 3-D data that is aware of “pose uncertainty,”&nbsp;meaning that the drone takes into consideration&nbsp;that it doesn't perfectly know its position and orientation as it moves through the world. Future iterations might also incorporate other pieces of information, such as the uncertainty in the drone’s individual depth-sensing measurements.</p> <p>NanoMap is particularly effective for smaller drones moving through smaller spaces, and works well in tandem with a second system that is focused on more long-horizon planning. (The researchers tested NanoMap last year in <a href="https://www.darpa.mil/program/fast-lightweight-autonomy">a program tied to the Defense Advanced Research Projects Agency</a>, or DARPA.)</p> <p>The team says that the system could be used in fields ranging from search and rescue and defense to package delivery and entertainment. It&nbsp;can also be applied to self-driving cars and other forms of autonomous navigation.</p> <p>“The researchers demonstrated impressive results avoiding obstacles and this work enables robots to quickly check for collisions,” says Scherer. “Fast flight among obstacles is a key capability that will allow better filming of action sequences, more efficient information gathering and other advances in the future.”</p> <p>This work was supported in part by DARPA’s Fast Lightweight Autonomy&nbsp;program.</p> <p><b><a href="http://news.mit.edu/2018/mit-csail-programming-drones-fly-face-uncertainty-0212" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1137</wp:post_id>
		<wp:post_date><![CDATA[2018-02-12 05:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-12 05:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[programming-drones-to-fly-in-the-face-of-uncertainty]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/mit-csail-programming-drones-fly-face-uncertainty-0212]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Drive.ai Welcomes Zhiyin Pan</title>
		<link>https://fifthlevel.ai/archives/1727</link>
		<pubDate>Tue, 06 Feb 2018 01:02:49 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/e2ebfc506a45</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Today we’re highlighting Zhiyin, a Drive.ai team member who in her spare time is an exercise maniac, orchid grower and water sports fanatic. She joined the team in October 2017 and leads Data Infrastructure, meaning she’s responsible for the backend infrastructure that feeds Drive.ai’s deep learning stack. As Zhiyin says, “The way I see it, everything Drive.ai does is about safety and accuracy, and that’s powered by data.” When asked why she was excited to join the Drive.ai team, she said, “I enjoy working at growing companies because I can make a big impact, which is very rewarding. Not only is Drive.ai a place where I am fulfilled and proud of the work I’m doing, but the company has a great mission. I really believe that this is a team that can make autonomous driving happen.”</p><p>When asked about her path to get to where she is today, Zhiyin says that she fell in love with the complexity and elegance of the engineering space years ago. At the beginning of her career, Zhiyin spent 7 years at Oracle as a Software Engineer. This initial experience helped her understand how a globally distributed team works together effectively. Zhiyin then spent 2 years working on her own social media networking start-up in the Chinese market, which she describes as a “super fun project” where she learned how to juggle competing priorities. Most recently, Zhiyin was the VP of Engineering at Chart.io, where she honed her skills to effectively manage an engineering team that supports the overall growth of the business. She found that being on a management track combined her passion for technical complexity with the intricacies of human nature. As Zhiyin says “I see myself as someone who has struck a balance between both technological and people skills.” Zhiyin derives part of her management philosophy from someone she looks up to, Andrew Ng, one of the company’s Board members and the professor that helped launch the initial co-founding group. “I really admire Andrew. He is very intellectual, but incredibly humble at the same time. He sparks creativity in the team.” She continues, discussing her management philosophy, “I want to enable my teammates to make the right decision by providing the right tools and resources. If the individual succeeds, so does our team — in the end, we are all successful.”</p><p>As women still form the minority group within the engineering field, Zhiyin admits that the path to obtaining a leadership position was not always smooth for her. “Female engineers are no less talented. I have often found that I am more self conscious and focus too much on my areas of improvement.” When asked about the challenges that she has personally faced, Zhiyin feels she has had to work harder than her colleagues to prove she had the same, if not better, abilities as her male peers. When asked what advice she can provide for future female engineers and leaders, Zhiyin says, “don’t be afraid to be bold or to take on additional responsibilities. The team will be better off for it.”</p><p>In her spare time, Zhiyin loves to travel with her family and spend time with her daughter. She is an avid reader, gardener, and has recently taken up a love for windsurfing. We’re excited to have you on the team, Zhiyin!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e2ebfc506a45" width="1" height="1"> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1727</wp:post_id>
		<wp:post_date><![CDATA[2018-02-06 01:02:49]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-06 01:02:49]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[drive-ai-welcomes-zhiyin-pan]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@drive.ai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@drive.ai/drive-ai-welcomes-zhiyin-pan-e2ebfc506a45?source=rss-37851f177ff8------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Our First Autonomous Vehicle Disengagement Report</title>
		<link>https://fifthlevel.ai/archives/1813</link>
		<pubDate>Mon, 29 Jan 2018 21:58:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/b77c0908bf61</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/855/0*jyBSoQ-zByjgcjzT." /></figure><p>We are pleased to release the key numbers from drive.ai’s 2017 autonomous vehicle disengagement report.</p><p><strong>2017 Disengagement Report Key Stats:</strong></p><ul><li>Total autonomous miles: 6572</li><li>Total disengagements: 151</li><li>Miles per disengagement (2016): 9.44</li><li>Miles per disengagement (2017): 65.38</li><li>Number of autonomous vehicle incidents: 0</li></ul><p>To read the report in full, <a href="https://drive.google.com/open?id=0B9lljJbJ-iZLRnJwRGZreUk2ZEgzYTVJQXVKUnpWc19DVlM4">click here</a>.</p><p>California’s laws regarding autonomous vehicle testing and deployment have been in effect for a little more than three years. Under the California Code of Regulations, Title 13, Article 3.7, all entities testing autonomous vehicles in the State of California are required to disclose certain details of their activities to the DMV.</p><p>We received our Manufacturer’s Testing Permit for the California Autonomous Vehicle Tester Program on April 19, 2016, and soon after began testing our vehicles on public roads. This year marks our first autonomous vehicle disengagement report, and we’re proud to share our progress ahead of the official report to be issued by the DMV in the coming weeks. The report we submitted this year covers our company’s autonomous vehicle operations in California from the date we received our license until the end of the reporting period, November 30, 2017.</p><p>As of the end of the reporting period, we had seven licensed autonomous vehicles operating on California public roads, and continue to further grow our fleet at a rapid clip. In the time span covered by our report, we drove a total of 6,572 autonomous miles and experienced 151 disengagements due to technology failure or when safe operation required the intervention of a safety driver. Our autonomous miles were driven on surface streets in various parts of the San Francisco Bay area with real world challenges, including traffic lights, stop signs, pedestrians, cyclists, and more.</p><p>Our autonomous miles traveled per disengagement increased dramatically over the past twenty months: from close to 3 when we first started driving autonomously on public roads to over 100 in November 2017, which also included our first ever <a href="https://medium.com/@drive.ai/what-we-learned-driving-an-autonomous-vehicle-for-24-hours-straight-587defe151bd">Autonomous Drive-a-thon</a>.</p><p>The world of driving is extraordinarily complex. Today’s production advanced driver assistance systems (ADAS), such as adaptive cruise control and traffic jam assist, have made significant progress in automating portions of driving. To advance from SAE Level 2 to SAE Level 4 automation, however, is to tackle a different class of problem. Our miles driven in California and elsewhere are helping us make that a reality by providing our systems with better real-world data on which to improve our robust, full-stack driving solution.</p><p>Safety is of the utmost importance for us at Drive.ai. Core to our mission of building the world’s best autonomous driving platform is the vision of a future with zero accidents. While we are pleased with the high numbers we have achieved in many areas, one of the numbers we are most proud of is zero: in the hundreds of hours and thousands of miles in autonomous mode we have driven to date, we have had zero safety incidents of any kind, at-fault or otherwise.</p><p>While we are excited to share the numbers from our first AV report, we’re even more excited about the progress we’ve made so far in 2018. We have our sights set on safely and rapidly deploying our technology to tackle more vehicles, more miles, and more use cases!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b77c0908bf61" width="1" height="1"> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1813</wp:post_id>
		<wp:post_date><![CDATA[2018-01-29 21:58:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-29 21:58:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[our-first-autonomous-vehicle-disengagement-report]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@drive.ai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@drive.ai/our-first-av-disengagement-report-b77c0908bf61?source=rss-37851f177ff8------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The big squeeze: How self-driving vehicles will put pressure on the car market</title>
		<link>https://fifthlevel.ai/archives/2475</link>
		<pubDate>Tue, 27 Feb 2018 12:06:29 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.driverless-future.com/?p=1132</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In the next five years the first fully self-driving cars will become available for purchase. This will happen after the first fleets of self-driving taxis appear in our cities. How will this affect the demand for private cars? We can expect consumers to react in multiple ways:</p>
<p>In the high end of the market, additional demand will be generated by price-insensitive customers who greatly value their personal time and greatly value their own life. For many affluent consumers who spend significant time at the wheel, full self-driving capability will be a must have and they will not wait until the end of the usage cycle of their current car but have high motivation to switch to a new, fully autonomous model early. People who lease their car will demand upgrades (for example via lease pull aheads) while affluent consumers who buy their own cars, will just replace their old model early. This will lead to a spike in demand for premium vehicles &#8211; which is positive for the auto industry.</p>
<p>At the same time it will produce a dent in demand in the run up until the first self-driving models become available. The more customers get the impression that reliable self-driving models will be available on the market soon, the more they will hold off on purchasing a non-self-driving model. Individuals and companies are likely to extend expiring leases on premium cars for a short time just to be sure to switch to fully-self-driving vehicles as early as possible. For any company the most rational path to take is to adopt fully self-driving cars as early as possible because this has a direct positive effect on the productivity and health of their employees.</p>
<p>It is important to recognize that this adoption path can not be incremental. Driver assistance systems are getting better and they indeed follow an incremental route. But the switch to full self-driving is a disruption: only from that moment on can the driver turn his attention away and go to sleep, go over documents, watch a movie or find other ways of using their time. For affluent people who value their time at just $50 per hour, this translates into enourmous benefits ($18250 per year with an average of 1 hour per day in a car) compared to a car model with a high performing driver assistance system.</p>
<p>These reasons have another consequence: The demand for new premium vehicles without fully self-driving capability will crash. The self-driving feature will be a critical benefit for almost every customer; only the exceptionally loyal will avoid switching from a brand that can not offer full self-driving to another premium brand with full self-driving. In this part of the market (excluding chauffeured cars and aficionado cars) competetion will be enormous. Brands which are late coming to the market will dramatically loose market share. We may see a very rapid shakeout in this part of the industry.</p>
<p>The picture looks different for more price-sensitive customers. A small part of this group will find that the obvious additional time and risk-reducing benefits of self-driving cars are reason enough to spend more on a car purchase and upgrade to a premium self-driving vehicle. This will add to the initial demand for premium self-driving cars.</p>
<p>A much larger group will find that they can not afford a premium self-driving car. This group has two major options: It can wait until self-driving capabilities trickle down to less expensive cars. Given the significant benefits of the self-driving feature this has the consequence that they will hold off on purchasing new cars in their segment until self-driving capabilities arrive. Demand for new cars in these seqments will therefore fall and OEMs will feel the pressure to accelerate the introduction of self-driving capabilities into the lower segments of the car market.</p>
<p>The other option for the more price-sensitive group is to switch to mobility services where available. Self-driving taxis are likely to provide mobility at a cost per kilometer that is not significantly higher than the total cost per kilometer of the average privately owned car without self-driving capability. Because this option will be available in many cities even before self-driving cars can be purchased many customers will already experience self-driving and its benefits. In high density urban areas, where space is at a premium, reducing the number of cars per household or even eliminating all personal cars will be the obvious solution. In many such areas the marginal costs of using a private car will be higher than using a self-driving taxi. In all areas where fleet services take hold (this will include many areas with lower density) we will see that households will reduce the number of vehicles they own.</p>
<p>For a part of this more price-sensitive group which can not afford premium self-driving vehicles, the most rational choice will be to switch to robo taxis early &#8211; even if they are more expensive than the marginal cost of using their own car &#8211; because this will allow them to use their personal time for something better than driving and increase their safety.</p>
<p>Thus even before the first fully self driving cars appear on the market, we will see a drop in demand for new vehicles caused by an increasing adoption of self-driving mobility services as well as the expectation that more affordable privately owned cars will be available in the near future. In this period, the demand for non self-driving vehicles in the lower segments must fall because some consumers are reducing the number of cars in their household by switching to self-driving mobility services and others hold off buying new cars with the expectation that more affordable self-driving cars will appear on the market in the near future.</p>
<p>This will have an effect on the used car market: As people switch to using self-driving mobility services in densely populated areas, they will sell their current cars prematurely; this will reduce prices in the used car market. A smaller group of customers will want to hold off buying a new car until self-driving features become available in their segment. This effect will be small and not be enough to counteract the price drop for used cars.</p>
<p>This will lead to a dilemma for the auto industry: because demand for cars drop and more hiqh quality used cars become available on the used car market, demand for new cars without self-driving capabilities falls. However if the auto industry rapidly switches to offering self-driving cars in the lower segments, then consumers will switch even faster to self-driving cars and cars without self-driving capability will become hard to sell. Prices for traditional cars will fall and traditional cars will depreciate much faster. OEMs that don&#8217;t offer self-driving capability will rapidly loose market share.</p>
<p>It is inevitable, therefore, that the advent of self-driving cars will squeeze demand for privately owned cars. It is not possible to rapidly roll out cheap self-driving capability in all segments. On the path to this future, demand for new cars must shrink because for some customers it is rational to hold off on purchasing a new car to wait for the availabity of the self-driving capability, for other customers it is rational to switch to using self-driving mobility services, and last but no least every price cut in self-driving technology makes the use of fleets economically more attractive compared with the use of a privately owned self-driving car.</p>
<p>Thus the auto industry is in a difficult position. As long as the advent of fully self-driving private cars is only a distant vision on the horizon, everything looks like business as usual. But when the first fleets of self-driving cars provide mobility services in an increasing number of cities across the globe over the next three years and as consumers take notice that the release of the first fully self-driving private vehicles appear imminent, then the auto industry will experience a major shakeout. Time to react will then be very short and the survival of more than one OEM will be in question!</p> <p><b><a href="http://www.driverless-future.com/?p=1132" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2475</wp:post_id>
		<wp:post_date><![CDATA[2018-02-27 12:06:29]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-27 12:06:29]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[the-big-squeeze-how-self-driving-vehicles-will-put-pressure-on-the-car-market]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?feed=rss2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?p=1132]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Self-driving electric fleet vehicles wont need large batteries: they solve the EV range problem</title>
		<link>https://fifthlevel.ai/archives/2476</link>
		<pubDate>Wed, 31 Jan 2018 17:52:39 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.driverless-future.com/?p=1118</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Neither the auto industry nor politicians have yet grasped a key implication of self-driving vehicle technology: it fundamentally changes the electric vehicle range problem &#8211; which is the key limiting factor for the adoption of electric vehicles. Batteries are currently the biggest cost factor for electric vehicles; their production is costly; their enormous weight (a Tesla Model S electric battery with a capacity of about 85kWh weighs a little more than half a ton) also reduces the energy efficiency of electric cars.</p>
<p>But the ability to drive autonomously changes the way vehicles will be used. The first self-driving vehicles won&#8217;t be available to end users; they will provide mobility services in urban areas where most trips cover only a short to medium distance. Trips above 30km will be rare (or even impossible in a given, limited urban area) and the average trip length is unlikely to significantly exceed 10km. Thus &#8211; from the perspective of individual trips &#8211; fleet vehicles won&#8217;t require enormous battery sizes.</p>
<p>This means that the classical range problem for electric vehicles &#8211; which is currently seen as the major factor limiting adoption &#8211; is no longer relevant for fleets of self-driving taxis! The operators of such vehicles will not seek to maximize battery range but will want determine the optimal battery size for their usage patterns. This depends on the distance which fleet vehicles cover during the day, the geographic and temporal distribution of trips, the installed charging infrastructure and recharge speed. If we assume that the average speed achievable in urban traffic won&#8217;t exceed 30km per hour during peak times and that all vehicles will be busy servicing customers during the 3 hour morning and afternoon peaks (without any time to recharge) this means that these vehicles need to be fully charged for a range of at least 90km before the peaks. As the peak travel period ends and demand for transportation services drops off, vehicles that are idle can then drive themselves to high capacity quick charging stations and recharge so that the fleet as a whole returns to maximum battery capacity before the afternoon peak. Thus the optimal full-capacity battery range for these vehicles should be well below 200km, possibly closer to 100 than 200km. They will be able to provide mobility services with much smaller batteries than the electric vehicles that are currently being sold to private car owners, such as Tesla, Renault Zoe and others.</p>
<p>Self-driving cars change the fundamentals of mobility and we need to consider the effects very seriously, leaving aside our intuitions and projections which are so often based on current car-based mobility. If we examine this problem more closely, it becomes obvious that the assumption that all self-driving vehicles within a fleet should be equipped with batteries of the same size is also problematic: In urban centers a large percentage of customers only request very short trips. Thus some vehicles could be equipped with extremely small batteries and the fleet management system could channel only requests for short trips to these vehicles. Requests for longer trips could be steered towards other self-driving fleet vehicles which are equipped with larger batteries. Overall, we can expect that the total fleet battery size will be much smaller than the sum of battery capacity which a similar-sized number of privately owned electric vehicles would have.</p>
<p>No auto maker or new entrant in the self-driving car space has yet presented a car model that has been engineered for fleet use from ground up (going beyond the individual car design and applying a systems perspective on the fleet and its operating infrastructure). But when that happens, the architects will need to also consider whether these cars should be equipped with an ability for rapid, fully automated, battery swapping (which may also consist of adding/removing smaller battery extension packs on the order of 25 to 50km ranges). If this were feasible in small stations in a time frame of three minutes or less, then such fleets could come very close to the theoretical minimum in fleet total battery capacity with respect to given transportation demands. This would greatly reduce capital and operational costs for such fleets, increase their cost advantage over trips with privately owned vehicles, and minimize the energy costs and environmental impact of personal transportation.</p>
<p>It is time to recognize that self-driving cars fundamentally change many aspects of mobility and that they will be the catalyst for the electric mobility of the future. With self-driving vehicles, the classic range problem of electric vehicles vanishes (and is replaced with an entirely different problem of determining the optimal battery range for a regional mobility usage pattern). This also implies that we should be careful with our projections about the adoption rate of electric vehicles. Autonomous electric taxis could dramatically accelerate the diffusion of electric vehicles and rapidly increase the share of person-kilometers traveled in electric vehicles. Auto makers, politicians and environmentalists should take notice and move their focus from more efficient batteries to rethinking and redesigning urban (and long-distance) personal mobility in the age of self-driving vehicles!</p>
<p>P.S. Because fleet-based mobility services will also be available to people who still have their own (self-driving) cars, the battery range equation will also change for them. Range will no longer be such an important factor for buying a private car when it is clear that comfortable, ubiquitous fleet services are available in all cities (locally and at the destination) and that fleet vehicles can be used for long distance trips in those rare cases where the needed range is larger than the range of the electric vehicle that was purchased.</p> <p><b><a href="http://www.driverless-future.com/?p=1118" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2476</wp:post_id>
		<wp:post_date><![CDATA[2018-01-31 17:52:39]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-31 17:52:39]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[self-driving-electric-fleet-vehicles-wont-need-large-batteries-they-solve-the-ev-range-problem]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?feed=rss2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?p=1118]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Cognata Featured on VentureBeat: 7 promising tech startups shaking up the auto industry</title>
		<link>https://fifthlevel.ai/archives/3121</link>
		<pubDate>Tue, 27 Feb 2018 05:15:26 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.cognata.com/?p=477</guid>
		<description></description>
		<content:encoded><![CDATA[<p>&#8220;The auto industry is undergoing a <a href="https://venturebeat.com/2017/11/29/auto-industry-players-outline-how-autonomous-vehicles-will-affect-society/" target="_blank" rel="noopener">massive shift</a> as it ushers in the technologies behind electric and autonomous vehicles and onboard tech designed to improve or enhance the driving experience. But to fully understand how those changes are manifesting, and how they could evolve in the future, we have to first understand the startups and entrepreneurs driving those changes.&#8221;&#8230;</p>
<p>Continue to read at VentureBeat <a href="https://venturebeat.com/2018/02/25/7-promising-tech-startups-shaking-up-the-auto-industry/">https://venturebeat.com/2018/02/25/7-promising-tech-startups-shaking-up-the-auto-industry/</a></p>
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://www.cognata.com/cognata-featured-venturebeat-7-promising-tech-startups-shaking-auto-industry/">Cognata Featured on VentureBeat: 7 promising tech startups shaking up the auto industry</a> appeared first on <a rel="nofollow" href="http://www.cognata.com">Cognata Autonomous Simulation</a>.</p> <p><b><a href="http://www.cognata.com/cognata-featured-venturebeat-7-promising-tech-startups-shaking-auto-industry/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3121</wp:post_id>
		<wp:post_date><![CDATA[2018-02-27 05:15:26]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-27 05:15:26]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[cognata-featured-on-venturebeat-7-promising-tech-startups-shaking-up-the-auto-industry]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.cognata.com/blog/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.cognata.com/cognata-featured-venturebeat-7-promising-tech-startups-shaking-auto-industry/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>openpilot port guide for Toyota models</title>
		<link>https://fifthlevel.ai/archives/3257</link>
		<pubDate>Wed, 31 Jan 2018 05:52:04 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/e5467f4b5fe6</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="https://github.com/commaai/openpilot">openpilot</a> is an open source driving agent, maintained by <a href="https://comma.ai/">comma.ai</a> and currently compatible with <a href="https://github.com/commaai/openpilot/blob/devel/README.md#supported-cars">several Honda and Toyota car models</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*POHd1YFsLlNOzUsnu0ggYg.jpeg" /></figure><p>The purpose of this post is to provide a simple how-to guide to port openpilot on currently unsupported Toyota models and trims. Making an openpilot port means you’ll have to fork the openpilot repo, change the code to incorporate the new car model or trim you want to have supported, and finally submit your pull request.</p><p>Why Toyota? Because Toyota cars are great! By default, almost every model is equipped with Toyota Safety Sense Plus (TSS-P), including the Camry and the Corolla, two of the three most sold cars in America. TSS-P means that the car has both adaptive cruise control and lane keep assist features, and therefore a by-wire way to control the longitudinal motion (gas and brake) and the lateral motion (steer). <a href="https://di-uploads-pod10.dealerinspire.com/wilsonvilletoyota/uploads/2016/11/2018-TSS-Applicability-Chart-2017.8.1.pdf">Here</a> is a handy summary of the ADAS features on each Toyota model.</p><p><strong>Tools required and dependencies</strong>: <a href="https://shop.comma.ai/products/eon-dashcam-devkit">EON</a> or <a href="https://github.com/commaai/neo">NEO</a> with latest openpilot installed, <a href="https://shop.comma.ai/products/panda-obd-ii-dongle">Panda</a>, <a href="https://shop.comma.ai/products/giraffe-toyota">Toyota Giraffe</a>, a Toyota with TSS-P, a computer. This guide is written for <a href="https://github.com/commaai/openpilot/releases/tag/v0.4.1">openpilot v0.4.1</a>.</p><p><strong>Background requirements</strong>: basic knowledge of CAN, <a href="http://socialledge.com/sjsu/index.php/DBC_Format">dbc files</a>, Python, <a href="https://danielmiessler.com/study/vim/">Vim</a>, Git/GitHub, F=ma.</p><h3>Quick process overview and git repo setup</h3><p>Toyota vehicles with TSS-P use two Electronic Control Units (ECUs) to control the car: the Driving Support Unit (DSU) controls the longitudinal motion, while the Forward Recognition Camera (FRC) controls the lateral motion. openpilot is capable of substituting those ECUs, so that the stock Adaptive Cruise Control and the Lane Keep Assist features are replaced by openpilot functionalities.</p><p>To prepare for the required SW changes, start with forking the openpilot repo on GitHub.</p><h3>Step 1 — Connect EON and Panda to vehicle’s CAN</h3><p>First things first, you need to communicate with the car by intercepting CAN traffic in a convenient location. The best spot is where the FRC is plugged in which is on the windshield behind the rear view mirror. To remove the plastic cover, you first need to unlock it by sliding it downwards parallel to the windshield.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OB0R0ERa7A2JWt7Tq9Ocxw.jpeg" /><figcaption>FRC cover</figcaption></figure><p>You will notice that the plastic cover is an assembly of two pieces. You can pull the small one out so in order to fully uncover the FRC wiring without having to remove the rear view mirror.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zw3pTeB4ZDEKRL24Y4k3Wg.jpeg" /><figcaption>Small plastic piece removed</figcaption></figure><p>Unplug the FRC connector and connect a Panda to the car’s CAN using the Toyota Giraffe. Make sure the Giraffe’s switches 1, 3, 4 are on (towards the numbers), and switch 2 is off. Switches 1 and 4 allow the FRC to be powered and connected to the vehicle’s CAN, while switch 3 powers the Panda when the car is turned on.</p><p>Now connect the Panda to the EON via mini-USB. Don’t worry about mounting the EON on the windshield yet, there is more work to do that does not require driving.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XGoaJVk_veJ-nBe4fkwp9Q.jpeg" /><figcaption>EON, Giraffe and Panda connected to the vehicle’s CAN</figcaption></figure><p>Your EON (through the Panda) now has access to the 2 CAN buses the FRC is connected to. CAN bus 0 will from now on be referred to as the <em>Powertrain CAN Bus</em>: it contains messages related to vehicle speed, steering, acceleration etc… CAN bus 1 will be referred to as the <em>Radar CAN Bus</em>, since you can find information about the radar points sent by the Radar Module and other information sent by the FRC.</p><h3>Step 2 — Determine your CAN Fingerprint</h3><p>The next step is detecting the list of CAN messages present on the <em>Powertrain CAN Bus</em>. The assumption is that every car model can be uniquely identified by the set of CAN messages on the <em>Powertrain CAN Bus</em>. Acquiring such a list is what we refer to as “fingerprinting” the car.</p><p>Use a computer to remotely SSH into the EON. First, you need to download the EON’s private key onto your computer: the key can be found in the latest <a href="https://github.com/commaai/neo/releases/download/swag/neos_v3.zip">NEOS release</a>, at neos/key/id_rsa. You can find the EON IP address in Settings-&gt;Wi-Fi Settings-&gt;(Top right menu)-&gt;Advanced.</p><p>Once you are SSH’d into the EON, type tmux ato attach to the current running tmux session.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/849/1*fa2X0fKncQm-k2YEHz5JOQ.png" /><figcaption>tmux session on EON</figcaption></figure><p>Here a <a href="https://lukaszwrobel.pl/blog/tmux-tutorial-split-terminal-windows-easily/">quick tmux guide</a> (on the EON Ctrl-b is replaced by backtick). Note: a tmux session is always launched when the EON boots up. This tmux session has privileges in accessing the EON processing power, so it’s necessary that you run openpilot from this tmux session. If you have openpilot correctly installed, you will see it running on window 0. You can create a new window by typing ` followed by c. Make sure you never close the last window in the tmux session. If the tmux session closes you have to restart your EON.</p><p>Now switch to your forked Git repo. Follow <a href="https://help.github.com/articles/configuring-a-remote-for-a-fork/">this quick guide</a> for more information.</p><p>You are ready to run the following script to acquire the car fingerprint:</p><pre>export PYTHONPATH=/data/openpilot<br>/data/openpilot/selfdrive/debug/get_fingerprint.py</pre><p>This script listens to all the CAN messages published on the <em>Powertrain CAN Bus</em> and prints out a list. While the script is running, turn the car off and on a couple of times and make sure the script is running with the car on for at least 15 consecutive seconds. You will note that the printed text has a dictionary-like structure. For example:</p><pre>number of messages: 65<br>fingerprint 36: 8, 37: 8, 170: 8, 180: 8, 186: 4, 426: 6, 452: 8, 464: 8, 466: 8, 467: 8, 547: 8, 548: 8, 552: 4, 608: 8, 610: 5, 705: 8, 800: 8, 849: 4, 896: 8, 897: 8, 900: 6, 902: 6, 905: 8, 911: 8, 916: 2, 921: 8, 933: 8, 944: 8, 945: 8, 951: 8, 955: 4, 956: 8, 979: 2, 992: 8, 998: 5, 999: 7, 1000: 8, 1001: 8, 1017: 8, 1043: 8, 1056: 8, 1059: 1, 1114: 8, 1196: 8, 1279: 8, 1552: 8, 1553: 8, 1556: 8, 1557: 8, 1561: 8, 1562: 8, 1568: 8, 1569: 8, 1570: 8, 1571: 8, 1572: 8, 1584: 8, 1589: 8, 1592: 8, 1596: 8, 1597: 8, 1600: 8, 1664: 8, 1728: 8, 1779: 8,</pre><p>The number of messages slowly increases over time. Dictionary’s key are the addresses of the messages, while the values are the lengths in bytes of the messages. Some of those messages are published at low frequency, which is why you need to wait several seconds with the car turned on to make sure that you have collected all of the messages. Failing to collect all of the messages will result in openpilot unreliably detecting the fingerprint of your car when you turn on your vehicle.</p><h3>Step 3 — Test the CAN fingerprint</h3><p>Now open data/openpilot/common/fingerprint.py. You will see a Toyota class where each attribute is a Toyota model. Make a new attribute that represents it regardless of whether you are adding a new car model or just a trim. Later on we will eventually simplify the code changes if the new trim can be considered as a sub-case of an already existing car model.</p><pre>class TOYOTA:<br> PRIUS = &quot;TOYOTA PRIUS 2017&quot;<br> RAV4H = &quot;TOYOTA RAV4 2017 HYBRID&quot;<br> RAV4 = &quot;TOYOTA RAV4 2017&quot;<br> COROLLA = &quot;TOYOTA COROLLA 2017&quot;<br> NEW_CAR = &quot;NEW CAR 2018&quot;</pre><p>Moreover, you’ll see a dictionary of fingerprints, where the key is the car model name that is supported by openpilot. Add the fingerprint dictionary that you obtained in the previous step and save the file.</p><pre>... <br> TOYOTA.COROLLA: {...<br> },<br> TOYOTA.NEW_CAR: {36: 8, 37:8, ...<br> },<br>}</pre><p>Now switch to the windows 0 of your tmux and kill the openpilot processes (Ctrl+c). You will notice that the window will disappear after you press Ctrl+c a second time. Just use any other tmux window of the same session to launch the openpilot processes again by typing:</p><pre>cd /data/openpilot/<br>./launch_openpilot.sh | grep &quot;fingerprinted&quot;</pre><p>Look at the printed output. If you see:</p><pre>fingerprinted NEW CAR 2018</pre><p>then success, openpilot uniquely recognized your car model! If not, try repeating Step 2 (it’s possible that your fingerprint collection wasn’t complete). Also, although your car has been recognized by openpilot, you will likely see some errors when running launch_openpilot.sh. This is because the code is still missing information on how to communicate with the newly added car model.</p><h3><strong>Step 4 — Try to run openpilot as if the new car model was an already supported model (i.e. you feel lucky)</strong></h3><p>Note: the success of this step does not affect the next to-do steps, but it will quickly put you on the right track and it will boost your confidence!</p><p>All Toyota car models share pretty much the same API for controlling gas, brake and steer. First try to make openpilot think that your car model is the same as an already supported one. To do so, replace the name of your car in \data\openpilot\common\fingerprint.py with the name of an already supported car and comment out the original fingerprint dictionary for such car. Which car should you try? A good rule of thumb is if your car is a hybrid model, use the Rav4 Hybrid, otherwise try the normal Rav4.</p><p><strong>FRC — lateral control</strong><br>Turn off the car and wait at least 20 seconds to clear from eventual faults built during the previous steps. Make sure that the Giraffe’s switches 1 and 4 are off. This will disconnect the stock FRC from CAN.</p><p>Restart /data/openpilot/launch_openpilot.sh and turn on your car. <br>Does the car throw any warnings? If not, openpilot is most likely correctly reproducing all the relevant messages normally published by the FRC.</p><p>Now look at the tmux session to see if there are any errors. If not, it is likely that openpilot is already capable of laterally controlling the vehicle in combination with the car’s Adaptive Cruise Control (remember, the stock DSU is still connected so openpilot won’t try to provide longitudinal control).</p><p><strong>DSU — longitudinal control</strong><br>First you need to locate the DSU. For currently supported cars, the DSU location is documented <a href="https://community.comma.ai/wiki/index.php/Toyota#Prius_.28for_openpilot.29">here</a>. In almost all the Toyota models we currently support, the DSU is easily accessible behind the glove box which can be removed by hand (note the Corolla has the DSU in an <a href="https://community.comma.ai/wiki/index.php/Toyota#Corolla_.28for_openpilot.29">inconvenient location</a>).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*A9dAEllzuHQk1NWF7iyZ6Q.jpeg" /><figcaption>DSU location in the Rav4 2017</figcaption></figure><p>Once the DSU is located, disconnect it and repeat the process described for the FRC. If you get no car and openpilot errors after turning on the car, then openpilot is already reproducing all the relevant messages normally sent by the stock DSU. For now, it’s recommended to keep the DSU disconnected.</p><h3>Step 5 — Car specific code</h3><p>At comma.ai, we realize that openpilot can scale to a lot of car models depending on whether we minimize the amount of code that is specific to each car. Vision models, planning and high level controls are all car independent, while only the low level control tuning (PID loops) and CAN read/write functions have been made car-specific.</p><p>Except for the fingerprint and the dbc file, car-specific code is contained in the path /data/openpilot/selfdrive/car. As of now we have two relevant folders, one for each car maker currently supported: /honda and /toyota.</p><p>Within each of these folders you can find the following files:</p><ul><li>carstate.py: this class reads CAN messages from the <em>Powertrain CAN Bus</em>, parses them using the dbc file and converts them in a common car state format: see CarState structure in /data/openpilot/cereal/car.capnp.</li><li>carcontroller.py: class that receives control data from the controlsd thread (such as abstracted actuators commands, see CarController in /data/openpilot/cereal/car.capnp) and packs them into CAN messages using the dbc file. CAN messages are sent both on <em>Powertrain CAN Bus</em> and <em>Radar CAN Bus</em>.</li><li>interface.py: class that contains car specific physical parameters, tuning parameters and methods to execute car state and controller updates.</li><li>radar_interface.py: very similar to carstate.py, this class reads CAN messages from the <em>Radar CAN Bus</em>, parses them and converts them in a common radar state format: see RadarState structure in /data/openpilot/cereal/car.capnp. It’s very unlikely that your unsupported car model will require changes to this file, so you can ignore it.</li><li>values.py and &lt;car_maker&gt;can.py: these 2 files are a collection of a few functions and classes mainly used by carcontroller.py. In particular,values.py includes a dictionary of static messages that carcontroller needs to send to properly simulate the disconnected ECUs (FRC and/or DSU).</li></ul><p>In the parent car folder you will find:</p><ul><li>__init__.py: this file specifies the association between car name and car interface.</li></ul><p>From now, let’s assume that the folder path is set to the Toyota car folder.</p><h3><strong>Step 6 — Changes in __init__.py</strong></h3><p>Add the new entry to the interfaces dictionary as follow:</p><pre>...<br>interfaces = {<br>...<br> TOYOTA.NEW_CAR: ToyotaInterface,<br>}<br>...</pre><h3><strong>Step 7 — Changes in interface.py</strong></h3><p>Start with adding physical car parameters for your car model. You should be able to find everything you need online: wheelbase, steering ratio, center of gravity and mass (use average between min and max curb weight).</p><p>For all the other parameters, such as control tuning, make temporarily sure they are all defined for the new car model, copying values from the most similar model.</p><h3><strong>Step 8 — Create a new dbc file for the <em>Powertrain CAN Bus</em></strong></h3><p>Fork the opendbc repo and make a new dbc file for your car model in /data/openpilot/opendbc/generator/toyota. Use the following nomenclature:</p><pre>toyota_&lt;car_model&gt;_&lt;trim(optional)&gt;_&lt;year&gt;_pt.dbc</pre><p>Start with making a copy of an existing dbc file. Again, we recommend using toyota_rav4_2017_pt.dbc if your new car model is not hybrid, otherwise use toyota_rav4_hybrid_2017_pt.dbc. Then run the generator script by typing:</p><pre>/data/openpilot/opendbc/generator/generator.py </pre><p>This combines the generic toyota dbc file with messages for a specific car model.</p><h3><strong>Step 9 — Changes in carstate.py</strong></h3><p>Associate the new car model name with the newly added powertrain dbc file in the get_can_parser function.</p><p>Also in the parse_gear_shifter function, add the new car model name to the same list as the Rav4.</p><h3><strong>Step 10 — Changes to values.py</strong></h3><p>STATIC_MSGS is a dictionary where the keys are the addresses of the messages that need to be sent by openpilot so that eventually disconnected ECUs are properly recognized. Each value in the dictionary is a tuple, where the second element is a tuple of car model names. Add the new car model name so that it appears on all the messages where you read CAR.RAV4H or CAR.RAV4, depending if the new car model is hybrid or not, respectively.</p><h3><strong>Step 11 — Test new car name addition</strong></h3><p>For Steps 5 to 10, you were properly defining a new car model name while still treating the vehicle as if it were a Rav4.</p><p>For Step 11, turn on the car with the EON connected and both FCCM and DSU disconnected.</p><p>As in Step 4, the code has to run without error: make sure that all the threads started by openpilot are properly running. If you see that some of the threads are stopped, you may have missed something between Steps 5 to 10. Fix bugs until no threads crash when you start up the car.</p><p>As in Step 4, it’s possible that the car or openpilot still reports warnings (we did not make any new car model specific changes yet). Either way, continue on to the next step.</p><h3><strong>Step 12 — Capture all the CAN messages sent by the FRC</strong></h3><p>Set the Giraffe switches 1 and 4 to on, so the stock FRC is properly connected to the CAN bus. Also, if running, kill launch_openpilot.sh. This will prevent openpilot from sending messages on CAN.</p><p>Now, start the boardd thread (needed so openpilot can read CAN messages from the Panda) and the CAN printer script. Type:</p><pre>export PYTHONPATH=/data/openpilot<br>/data/openpilot/selfdrive/boardd/boardd</pre><p>and in a separate tmux window</p><pre>export PYTHONPATH=/data/openpilot<br>cd /data/openpilot/selfdrive/debug/<br>./can_printer.py &lt;can_bus&gt; &lt;max_message_addr&gt;</pre><p>where can_bus is the CAN Bus number (0 or 1) and max_message_addr is the max message address you want printed (this is handy since there are usually too many messages and they cannot all be printed on the same terminal):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/876/1*32DeoH2FmFhbB3vY_YViZw.png" /><figcaption>can_printer.py output</figcaption></figure><p>Now turn on and off the Giraffe switch 1 and see which messages stop being updated when switch 1 is off. These are the messages sent by the FRC, and they need to be simulated by openpilot. Take note of these message addresses, their length in bytes (for example in the screenshot above, message 0x3D3 is 2 bytes, while message 0x3E0 is 8 bytes) and their data (especially if static). Repeat the process for both the <em>Powertrain CAN Bus</em> and the<em> Radar CAN Bus</em>.</p><p>Compare the list of messages you have collected with the messages present in values.py and carcontroller.py and make sure that every message is properly sent. If not, make changes to those two scripts accordingly.</p><h3><strong>Step 13 — </strong>Capture all the CAN messages sent by the<strong> DSU</strong></h3><p>Repeat what you have done in the previous step, but manually connect and disconnect the DSU instead of switching the FRC on and off.</p><p>If you have properly done all the steps up to this point, then you should be able to run launch_openpilot.sh, turn the car on with the FRC and DSU disabled, and have the car display no warnings (regardless of what the outcome was at the end of Step 4).</p><h3><strong>Step 14 — Verify car states</strong></h3><p>A simple way to test this is to use the debugging script called dump.py to print the content of a specific zmq socket. While the controlsd thread is running, just type:</p><pre>export PYTHONPATH=/data/openpilot<br>/data/openpilot/debug/dump.py carState</pre><p>The goal is to make sure the dbc file is correct and the car states are correctly interpreted from CAN in carstate.py. If you see mismatches, root cause the issue by looking at carstate.py or in the dbc file. The important car states to verify are:</p><ul><li><strong>vEgo</strong>: this is the vehicle speed in m/s. While driving the car, make sure the speedometer speed roughly matches the printed value.</li><li><strong>gearShifter</strong>: with the car turned on, move the gear shifter and make sure the printed gear enumeration matches the actual gear lever position (park, neutral, drive etc...).</li><li><strong>leftBlinkers</strong>, <strong>rightBlinker</strong>: booleans that indicate the blinker’s state.</li><li><strong>steeringAngle</strong>: from a neutral/straight position, rotate the steering wheel 360 degrees to the left. You should read 360.</li><li><strong>gas</strong>: you should read 0 when the gas pedal is released, while you should read 1 when the gas pedal if fully depressed. Note that you can test this with the car in <a href="https://www.yourmechanic.com/question/what-are-the-ignition-switch-positions">ON ignition mode</a> to avoid revving up the engine when pressing the gas pedal.</li><li><strong>gasPressed</strong>: boolean that determines if the gas pedal is pressed or released.</li><li><strong>brakePressed</strong>: boolean that determines if the brake pedal is pressed or released.</li><li><strong>steeringTorque</strong>: numerical value that represents how much torque the driver is putting on the steering wheel. This value is noisy as it’s measured by a torque transducer. Make sure the value is roughly linearly correlated to the effort that you put in turning the steering wheel. It should be positive when trying to steer left.</li><li><strong>steeringPressed</strong>: a boolean that indicates if the driver is putting any torque on the steering wheel.</li><li><strong>doorOpen</strong> and <strong>seatbeltUnbuckled</strong>: booleans that indicates if any of the doors are open and if the driver’s seat belt is unlatched, respectively.</li><li><strong>genericToggle</strong>: this boolean is used to facilitate testing for the later steps. By default, it’s arbitrarily linked with the auto high-beam toggle state. You have to have the headlights on for this to work.</li></ul><h3><strong>Step 15 — Static Safety</strong></h3><p>First, familiarize yourself with <a href="https://github.com/commaai/openpilot/blob/devel/SAFETY.md">how openpilot ensures safety</a> when actuating the car. Now make sure openpilot disengages when:</p><ul><li>brake pedal is pressed;</li><li>gas pedal is pressed;</li><li>cruise stalk is pulled (cruise control cancel);</li><li>toggle button on cruise stalk is set to off;</li><li>any door is open;</li><li>driver’s seat belt is unbuckled.</li></ul><h3><strong>Step 16 — Dynamic safety</strong></h3><p>Note that this step is the most important and you should perform these tests on closed roads/tracks. Also, it requires the highest coding skills for this guide and you should be a bit “creative” in coming up with ways to test the following steps. In the future, openpilot might include simpler ways to execute those tests.</p><p>The idea is to test openpilot by simulating detection mistakes (wrong road curvature, ghost lead detection etc…): assuming the driver is paying attention, openpilot should never respond by controlling the actuators in a way that is uncontrollable by the driver. To manually inject detection errors, you can use the convenient genericToggle boolean in carState. By default, it’s associated to the auto high beam toggle state.</p><ul><li><strong>Max steer command ramp</strong>: in carcontroller.py, change the code so that max steer command is sent when genericToggle is True. Drive on a straight road, command max steer and verify that the driver has at least 1 second before having to intervene to avoid the car going over the lane markings.<br>Repeat the test at different speeds.<br>For example, the update method of the CarController class in carcontroller.py could be modified as follows:</li></ul><pre>...<br>def update(self, sendcan, enabled, CS, frame, actuators,<br> pcm_cancel_cmd, hud_alert, audible_alert):</pre><pre>...</pre><pre> # init safety test lines<br> if CS.generic_toggle:<br> actuators.steer = 1.0<br> # end safety test lines</pre><pre> # steer torque is converted back to CAN reference (positive when steering right)<br> apply_steer = int(round(actuators.steer * STEER_MAX))<br>...</pre><ul><li><strong>Max steer command steady</strong>: in carcontroller.py, change the code so that max steer command is sent when genericToggle is True. While max steer is constantly commanded, test that the driver can easily override the openpilot command. Repeat the test at different speeds.</li><li><strong>Steer override, then release</strong>: while driving with openpilot engaged, try overriding the steer command so that the openpilot lateral control pushes against the driver’s effort. Test that the car doesn’t swirl too quickly after the driver quickly releases the steering wheel. Repeat the test at different speeds.</li><li><strong>Max acceleration</strong>: in carcontroller.py, change the code so that max positive acceleration command is sent when genericToggle is True. On a flat road, test that the vehicle acceleration always remain below 2 m/s² at any speed. You can check the acceleration value by reviewing your drive in cabana and plotting the signal ACCEL_X from the message ACCELEROMETER.</li><li><strong>Max deceleration</strong>: in carcontroller.py, change the code so that max negative acceleration (deceleration) command is sent when genericToggle is True. On a flat road, test that the vehicle acceleration always remain above -3.5 m/s² at any speed. You can check the acceleration value by reviewing your drive in cabana and plotting the signal ACCEL_X from the message ACCELEROMETER.</li></ul><p>If you are not satisfied with the outcome of any of these tests, then you need to change the safety parameters in carcontroller.py and in the panda safety code as well (see /data/openpilot/panda/boardd/safety/safety_toyota.h).</p><h3><strong>Step 17 — Tuning</strong></h3><p>If you succesfully passed the previous step, then it’s time to test the quality of your port. Focus on:</p><ul><li><strong>Longitudinal control</strong>: API on Toyota cars is pretty standard, since openpilot commands the desired longitudinal acceleration and the powertrain control module takes care of converting it into gas and brake commands. Therefore it’s very unlikely that the new car model requires a new longitudinal control tuning. If there are no apparent misbehaviors, you can skip this.</li><li><strong>Lateral control</strong>: openpilot controls the steering wheel by commanding a steering torque over CAN. Tune the closed loop control by changing the feedback gains steerKp, steerKi and the feedforward gain steerKf in interface.py. The steering wheel should feel smooth and steady on straight highway roads and be responsive as you enter turns. Change the above mentioned parameters as follows: increase them if the weight of your new car is “high”, decrease them if the steeringRatio is “high”. You can see examples from the already existing tuning.</li><li><strong>Steering override</strong>: openpilot has a timer to alert the driver when it has been too long since the last time the driver was engaged with the vehicle. The timer is reset every time the driver manually moves the steering wheel. Ensure that the torque threshold used to determine the steerOverride bit is adequate. The torque threshold should be as low as possible to detect when the driver has his hands on the steering wheel while avoiding any false positive detection.</li><li><strong>Overall quality</strong>: during highway or stop-and-go (if supported) driving, openpilot should normally require driver interventions less than once every 6 minutes, except for out-of-scope events, such as close cut-ins, tight turns, stop signs, traffic lights, etc…</li></ul><h3><strong>Step 18 — Capable of stop-and-go?</strong></h3><p><a href="https://di-uploads-pod10.dealerinspire.com/wilsonvilletoyota/uploads/2016/11/2018-TSS-Applicability-Chart-2017.8.1.pdf">Only three Toyota models</a> have Full Speed Range Dynamic Cruise Control: Prius, Camry Hybrid, C-HR. The stop-and-go functionality will therefore transfer to openpilot.</p><p>For car models without stock stop-and-go functionality, openpilot upgrades the low speed longitudinal control as follow:</p><ul><li>for non-hybrid car models, openpilot continues to slow down the vehicle to standstill instead of canceling the system when the speed drops below 28 mph, like in the stock system. However, openpilot is unable to re-accelerate the car below 20 mph with openpilot. Also, openpilot will be engage-able when above 20 mph instead of 28 mph.</li><li>For hybrid car models, openpilot should be able to do stop and go with no limitations. On the Rav4 Hybrid, for example, openpilot is able to perform stop-and-go even though the stock Adaptive Cruise Control can’t.</li></ul><p>The above cases are just a rule of thumb. You should verify that this actually applies to your car model.</p><h3><strong>Step 19 — Submit the pull request and link the test drives</strong></h3><p>Make a pull Request from your forked branch into the <a href="https://github.com/commaai/openpilot">openpilot devel branch</a>.<br>In your pull request, refer to the tests you did by linking your drives using <a href="https://community.comma.ai/cabana/">Cabana</a>. To facilitate the review of your port, link drives related to Steps 15, 16, 17 and 18.</p><p>In particular, for Step 18, link a drive where openpilot is engaged for 6 consecutive minutes. The drive must contain both highway and stop-and-go (if supported) sections.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FcWeGcTvrO40%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DcWeGcTvrO40&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FcWeGcTvrO40%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/05dfc8ed5d2e496fa78d10a63617d75b/href">https://medium.com/media/05dfc8ed5d2e496fa78d10a63617d75b/href</a></iframe><h3><strong>Step 20 — Be engaged on Slack</strong></h3><p>There is an active comma community on <a href="http://slack.comma.ai">Slack</a>. Join us to discuss about openpilot ports and follow us on twitter!</p><p>NOTE: this guide is a work in progress and it will be periodically updated, as the openpilot code evolves.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e5467f4b5fe6" width="1" height="1"> <p><b><a href="https://medium.com/@comma_ai/openpilot-port-guide-for-toyota-models-e5467f4b5fe6?source=rss-330bac69b283------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3257</wp:post_id>
		<wp:post_date><![CDATA[2018-01-31 05:52:04]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-31 05:52:04]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[openpilot-port-guide-for-toyota-models]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@comma_ai/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@comma_ai/openpilot-port-guide-for-toyota-models-e5467f4b5fe6?source=rss-330bac69b283------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The FCW implementation is completely open source, and can be found here…</title>
		<link>https://fifthlevel.ai/archives/3258</link>
		<pubDate>Sun, 28 Jan 2018 00:17:49 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/6f8b32d6580e</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The FCW implementation is completely open source, and can be found here: <a href="https://github.com/commaai/openpilot/blob/devel/selfdrive/controls/lib/planner.py#L78">https://github.com/commaai/openpilot/blob/devel/selfdrive/controls/lib/planner.py#L78</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6f8b32d6580e" width="1" height="1"> <p><b><a href="https://medium.com/@comma_ai/the-fcw-implementation-is-completely-open-source-and-can-be-found-here-6f8b32d6580e?source=rss-330bac69b283------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3258</wp:post_id>
		<wp:post_date><![CDATA[2018-01-28 00:17:49]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-28 00:17:49]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[the-fcw-implementation-is-completely-open-source-and-can-be-found-here]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@comma_ai/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@comma_ai/the-fcw-implementation-is-completely-open-source-and-can-be-found-here-6f8b32d6580e?source=rss-330bac69b283------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The number of parameters is very small in comparison to the amount of data, so overfitting should…</title>
		<link>https://fifthlevel.ai/archives/3259</link>
		<pubDate>Sun, 28 Jan 2018 00:17:04 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/48b4c3d84afe</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The number of parameters is very small in comparison to the amount of data, so overfitting should not be a problem. Just to be sure, our final dataset used for testing was about 2x larger than the dataset used for developing.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=48b4c3d84afe" width="1" height="1"> <p><b><a href="https://medium.com/@comma_ai/the-number-of-parameters-is-very-small-in-comparison-to-the-amount-of-data-so-overfitting-should-48b4c3d84afe?source=rss-330bac69b283------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3259</wp:post_id>
		<wp:post_date><![CDATA[2018-01-28 00:17:04]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-28 00:17:04]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[the-number-of-parameters-is-very-small-in-comparison-to-the-amount-of-data-so-overfitting-should]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@comma_ai/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@comma_ai/the-number-of-parameters-is-very-small-in-comparison-to-the-amount-of-data-so-overfitting-should-48b4c3d84afe?source=rss-330bac69b283------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Meet our newest self-driving vehicle: the all-electric Jaguar I-PACE</title>
		<link>https://fifthlevel.ai/archives/255</link>
		<pubDate>Tue, 27 Mar 2018 12:22:04 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/375cecc70eb8</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Today Waymo and Jaguar Land Rover are announcing an electrifying new partnership. We’re joining forces to design and engineer the world’s first premium electric fully self-driving vehicle, built for Waymo’s transportation service. This new self-driving Jaguar I-PACE blends Jaguar’s knack for innovative design with Waymo’s cutting-edge <a href="https://medium.com/waymo/recreating-the-self-driving-experience-the-making-of-the-waymo-360-video-37a80466af49">self-driving</a> <a href="https://medium.com/waymo/two-million-miles-closer-to-a-fully-autonomous-future-14eb74064e7">technology</a>, designed and developed <a href="https://medium.com/waymo/introducing-waymos-suite-of-custom-built-self-driving-hardware-c47d1714563">completely in-house</a>.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FK_dWJYgoOc8%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DK_dWJYgoOc8&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FK_dWJYgoOc8%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/f5b55f6ab5cdb1f7708ec3e77b8412bb/href">https://medium.com/media/f5b55f6ab5cdb1f7708ec3e77b8412bb/href</a></iframe><p>At Waymo, we’ve been building <a href="https://medium.com/waymo/waymo-reaches-5-million-self-driven-miles-61fba590fafe"><em>the world’s most experienced driver</em></a>: a safe, skillful and savvy chauffeur that can take people and things from A to B at the push of a button. Imagine a world where you can take a self-driving minivan to the baseball game with family, and a self-driving I-PACE home after a night out — in both cases, a car perfectly suited for your needs. That’s the world we’re building.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7gaPAs0s4YszhfQXyhawgw.jpeg" /></figure><p>The all-electric I-PACE is a natural choice for this world. This is a sleek and graceful car, so it will provide a safe <em>and</em> delightful experience for our passengers. Its size makes it ideal for city driving. Its big, fast-charge battery means it can drive all day, which is perfect for our self-driving service. And, as part of a shared fleet, we can make this premium experience accessible to everyone.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GXj5pQUrC0JnQxkuNZSYwA.jpeg" /></figure><p>We’ll add up to 20,000 I-PACEs to Waymo’s fleet in the next few years — that’s enough to drive about a <em>million</em> trips in a typical day. With this partnership, we can offer our self-driving service to many communities across the country with vehicles that are safe, quiet and eco-friendly.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OJ1pC3O5zqh_Y9l_Yj8q8A.jpeg" /></figure><p>This is just the beginning. The self-driving products of the future will be designed around passengers, not drivers. That means riders will be able to choose from a broad array of options that will match their very specific needs: one for working remotely as you commute, one for dining with friends, even one designed for napping! The ultimate goal: with Waymo as the driver, products tailored for every purpose and every trip.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iR_WidzKTyC0tIuziSKeuA.gif" /></figure><p>Reaching that goal starts with partnerships with innovative automakers and OEMs, like Jaguar Land Rover. Their team truly shares our vision of a self-driving future where roads are safer and transportation is accessible for all.</p><p>The new self-driving I-PACE will start testing in our fleet later this year, and will soon be playing a vital full-time role in Waymo’s driverless transportation service. We’re looking forward to this new partnership, and the possibilities it unlocks for Waymo’s riders across the country.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=375cecc70eb8" width="1" height="1"><hr><p><a href="https://medium.com/waymo/meet-our-newest-self-driving-vehicle-the-all-electric-jaguar-i-pace-375cecc70eb8">Meet our newest self-driving vehicle: the all-electric Jaguar I-PACE</a> was originally published in <a href="https://medium.com/waymo">Waymo</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>255</wp:post_id>
		<wp:post_date><![CDATA[2018-03-27 12:22:04]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-27 12:22:04]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[meet-our-newest-self-driving-vehicle-the-all-electric-jaguar-i-pace]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/waymo]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/waymo/meet-our-newest-self-driving-vehicle-the-all-electric-jaguar-i-pace-375cecc70eb8?source=rss----7075a35566d9---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[827]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Mission Autonomous: Ford Aims to Inspire Next Generation of Engineers with New Award in FIRST…</title>
		<link>https://fifthlevel.ai/archives/387</link>
		<pubDate>Mon, 19 Mar 2018 15:31:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/c537009df564</guid>
		<description></description>
		<content:encoded><![CDATA[<h3>Mission Autonomous: Ford Aims to Inspire Next Generation of Engineers with New Award in FIRST Robotics</h3><h4>by Craig Stephens, Chief Engineer, Controls, Ford Research &amp; Advanced Engineering</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VmEa6EPSwkusXOs21rsFpQ.jpeg" /></figure><p>One thing that still surprises me about mentoring students in the <em>FIRST</em> Robotics Competition is how they seem much older than they really are. When you listen to the students interacting with their mentors, discussing problems in detail and developing solutions, you’d think you were listening to a group of experienced engineers tackling the latest engineering roadblock.</p><p>It is remarkable that most of the people in that group are barely 17 years old. It’s motivating to help nurture the next generation of engineers and computer scientists — and to think about what these young men and women will accomplish. One day, likely sooner than we think, they will use their skills to shape advanced technology that we’ll be using on a daily basis.</p><p>Today, these students are designing, building and testing a fully functioning robot that can perform a series of complex tasks — ranging from shooting balls into a goal to balancing themselves on a beam. These robots then compete against other robots developed by equally talented young people.</p><p>Founded by Dean Kamen in 1989, <em>FIRST</em> <a href="https://www.firstinspires.org/robotics/frc">teaches students</a> fundamental skills by combining the excitement of sports with science and technology. The competitive tournament and the prospect of building a robot are enticing, but they’re just a gateway for the program’s much loftier goals: Inspiring students to think creatively and generating an interest in the fields of science, technology, engineering and math. At Ford, we’re proud to be part of that effort, with more than 250 members of our team serving as mentors in the program and by sponsoring nearly 80 high school <em>FIRST</em> teams.</p><p>This year, we wanted to go even further. So we created a new award that will be presented at the <em>FIRST</em> Championship in Houston and Detroit. Ford’s Autonomous Award celebrates the team whose robot demonstrates consistent, reliable performance during autonomous periods — or the first 15 seconds of each match. I love the fact that students will be recognized for thinking through how a robot moves on its own, knowing that tackling these challenges will help groom them for great careers — perhaps even one at Ford. After all, many of the issues students must tackle to deliver an autonomous robot involve the same types of fundamental problems that our self-driving vehicle team spends its time trying to solve.</p><p>Just like a self-driving vehicle, a <em>FIRST</em> team’s robot must make a series of determinations about its environment before it can do anything. Students must ensure their robot can orient itself on the field and understand the nature of its surroundings, including where it has to go. Next, students must program their robot so that it’s able to identify obstacles in its way and figure out how to navigate the field to accomplish its tasks. Then it needs to be able to execute those tasks while avoiding any unforeseen hazards that might pop up.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1dEp2EFSDq7XJdqcptz6Pw.jpeg" /></figure><p>Judges will be watching closely, but evaluation will be based on all these elements, not necessarily on which robot scores the most points. FIRST students aren’t programming their robots for one instance, either. They participate in multiple matches, so their robot must be able to perform in autonomous mode repeatedly and reliably for success.</p><p>In the coming years, jobs related to science, technology, engineering and math are expected to keep growing. Between 2009 and 2015, the number of STEAM jobs grew by <a href="https://www.bls.gov/careeroutlook/2014/spring/art01.pdf">more than 10 percent</a>, according to the U.S. Bureau of Labor Statistics. That was more than double the growth for non-STEAM jobs. Going forward, some of the <a href="https://www.bls.gov/spotlight/2017/science-technology-engineering-and-mathematics-stem-occupations-past-present-and-future/pdf/science-technology-engineering-and-mathematics-stem-occupations-past-present-and-future.pdf">fastest-growing</a> STEAM fields are expected to be in computer science and engineering occupations — both areas in which students can gain valuable experience programming and building a functional robot.</p><p>As an engineer, sometimes the hardest part of mentoring a <em>FIRST</em> team is having to sit on your hands and not engineer the robot yourself — you’ve got to let students take the lead. Their success is their own. They may not have degrees in engineering yet, but guiding them along the path they choose and watching what they’re capable of is truly inspiring. I can’t wait to see how they — and their robots — perform this year.</p><p>For more information about Ford’s STEAM efforts, <a href="https://social.ford.com/content/ford-steam/index.html">click here</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c537009df564" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/mission-autonomous-ford-aims-to-inspire-next-generation-of-engineers-with-new-award-in-first-c537009df564">Mission Autonomous: Ford Aims to Inspire Next Generation of Engineers with New Award in FIRST…</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/mission-autonomous-ford-aims-to-inspire-next-generation-of-engineers-with-new-award-in-first-c537009df564?source=rss----7192a82ca09a---4" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>387</wp:post_id>
		<wp:post_date><![CDATA[2018-03-19 15:31:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-19 15:31:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[mission-autonomous-ford-aims-to-inspire-next-generation-of-engineers-with-new-award-in-first]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driven/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/mission-autonomous-ford-aims-to-inspire-next-generation-of-engineers-with-new-award-in-first-c537009df564?source=rss----7192a82ca09a---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[823]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Ford Developing Autonomous Systems for Police Cars, Other Emergency Vehicles</title>
		<link>https://fifthlevel.ai/archives/586</link>
		<pubDate>Sat, 07 Apr 2018 11:15:55 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.ipwatchdog.com/?p=95218</guid>
		<description></description>
		<content:encoded><![CDATA[<p>A statement published on the official website of Ford Motor Company (NYSE:F) indicates that the company expects to have a fully autonomous car in commercial operation by 2021. Ford believes that it will be able, by that time, to produce a vehicle which meets Level 4 automation as standardized by the engineering association SAE International. Last October, Ford CEO Jim Hackett announced that Ford will bring autonomous vehicles to a test market this year. One of the strategies the company will pursue is partnering with other companies to help bring the technology into the market, such as autonomous Domino’s pizza delivery services in Miami where the company will test how consumers interact with autonomous delivery services. Ford is investing $ 1 billion into vehicle artificial intelligence firm Argo AI to develop systems that give Ford vehicles the ability to transverse an urban environment like Miami.</p>
<p>The post <a rel="nofollow" href="http://www.ipwatchdog.com/2018/04/07/ford-developing-autonomous-systems-police-cars-emergency-vehicles/id=95218/">Ford Developing Autonomous Systems for Police Cars, Other Emergency Vehicles</a> appeared first on <a rel="nofollow" href="http://www.ipwatchdog.com">IPWatchdog.com | Patents &amp; Patent Law</a>.</p> <p><b><a href="http://www.ipwatchdog.com/2018/04/07/ford-developing-autonomous-systems-police-cars-emergency-vehicles/id=95218/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>586</wp:post_id>
		<wp:post_date><![CDATA[2018-04-07 11:15:55]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-04-07 11:15:55]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[ford-developing-autonomous-systems-for-police-cars-other-emergency-vehicles]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/tag/autonomous-vehicles/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/2018/04/07/ford-developing-autonomous-systems-police-cars-emergency-vehicles/id=95218/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Patent Filings Increase for E-Cigarettes, 3-D Printing and Machine Learning</title>
		<link>https://fifthlevel.ai/archives/587</link>
		<pubDate>Fri, 23 Mar 2018 10:15:06 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.ipwatchdog.com/?p=94905</guid>
		<description></description>
		<content:encoded><![CDATA[<p>One interesting aspect of IFI CLAIMS’ most recent annual patent analysis is a list of eight areas of technology that have seen the fastest growing increases in patent applications between 2013 and 2017. To do this, IFI computed the compound annual growth rate (CAGR) of patent applications for all Cooperative Patent Classification (CPC) codes over the course of the study period to see which CPC codes were receiving the greatest number of patent applications. According to IFI’s analysis, the greatest growth in patent applications were for E-cigarettes and other technologies under the CPC code A24F for smokers’ requisites.</p>
<p>The post <a rel="nofollow" href="http://www.ipwatchdog.com/2018/03/23/patent-filing-increase-e-cigarettes-3-d-printing-machine-learning/id=94905/">Patent Filings Increase for E-Cigarettes, 3-D Printing and Machine Learning</a> appeared first on <a rel="nofollow" href="http://www.ipwatchdog.com">IPWatchdog.com | Patents &amp; Patent Law</a>.</p> <p><b><a href="http://www.ipwatchdog.com/2018/03/23/patent-filing-increase-e-cigarettes-3-d-printing-machine-learning/id=94905/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>587</wp:post_id>
		<wp:post_date><![CDATA[2018-03-23 10:15:06]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-23 10:15:06]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[patent-filings-increase-for-e-cigarettes-3-d-printing-and-machine-learning]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/tag/autonomous-vehicles/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/2018/03/23/patent-filing-increase-e-cigarettes-3-d-printing-machine-learning/id=94905/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Protecting Automotive and Mobility Innovation in 2018</title>
		<link>https://fifthlevel.ai/archives/588</link>
		<pubDate>Fri, 16 Mar 2018 10:15:15 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.ipwatchdog.com/?p=94560</guid>
		<description></description>
		<content:encoded><![CDATA[<p>As autonomous vehicle and mobility technology continues to make headlines, federal legislation is making its way through Congress with the goal of removing government hurdles for the development, testing, and rollout of innovations in this space. Although this legislation primarily implicates R&#038;D activity, IP portfolio managers at automotive OEMs and suppliers should be aware of several proposals that may ultimately impact patent filing strategies and information compartmentalization best practices in order to reduce risks from disclosure requirements that are part of the current legislation.</p>
<p>The post <a rel="nofollow" href="http://www.ipwatchdog.com/2018/03/16/protecting-automotive-mobility-innovation-2018/id=94560/">Protecting Automotive and Mobility Innovation in 2018</a> appeared first on <a rel="nofollow" href="http://www.ipwatchdog.com">IPWatchdog.com | Patents &amp; Patent Law</a>.</p> <p><b><a href="http://www.ipwatchdog.com/2018/03/16/protecting-automotive-mobility-innovation-2018/id=94560/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>588</wp:post_id>
		<wp:post_date><![CDATA[2018-03-16 10:15:15]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-16 10:15:15]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[protecting-automotive-and-mobility-innovation-in-2018]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/tag/autonomous-vehicles/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/2018/03/16/protecting-automotive-mobility-innovation-2018/id=94560/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple&#039;s heads-up display tech for self-driving cars uses AR to enhance driver safety</title>
		<link>https://fifthlevel.ai/archives/724</link>
		<pubDate>Thu, 05 Apr 2018 19:30:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/04/05/apple-working-on-ar-displays-for-the-driver-and-passengers-of-a-self-driving-car</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/25571-34974-Screen-Shot-2018-04-05-at-201504-l.jpg" alt="Article Image" border="0" /> <br><br> Apple's work on augmented reality could find a place in its ongoing automotive initiative, with the company considering ways to use AR to show information about the road ahead, including things that are out of the driver's vision. <p><b><a href="https://appleinsider.com/articles/18/04/05/apple-working-on-ar-displays-for-the-driver-and-passengers-of-a-self-driving-car" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>724</wp:post_id>
		<wp:post_date><![CDATA[2018-04-05 19:30:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-04-05 19:30:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apples-heads-up-display-tech-for-self-driving-cars-uses-ar-to-enhance-driver-safety]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/04/05/apple-working-on-ar-displays-for-the-driver-and-passengers-of-a-self-driving-car]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[829]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple nabs Google&#039;s chief of AI and search John Giannandrea to broaden Siri, self-driving car programs</title>
		<link>https://fifthlevel.ai/archives/725</link>
		<pubDate>Tue, 03 Apr 2018 21:37:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/04/03/apple-nabs-googles-chief-of-ai-and-search-john-giannandrea-to-broaden-siri-self-driving-car-programs</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/25537-34841-John_Giannandrea-l.jpg" alt="Article Image" border="0" /> <br><br> Apple has hired John Giannandrea away from Google to head up Apple's machine learning and AI strategy, and will report directly to CEO Tim Cook. <p><b><a href="https://appleinsider.com/articles/18/04/03/apple-nabs-googles-chief-of-ai-and-search-john-giannandrea-to-broaden-siri-self-driving-car-programs" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>725</wp:post_id>
		<wp:post_date><![CDATA[2018-04-03 21:37:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-04-03 21:37:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-nabs-googles-chief-of-ai-and-search-john-giannandrea-to-broaden-siri-self-driving-car-programs]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/04/03/apple-nabs-googles-chief-of-ai-and-search-john-giannandrea-to-broaden-siri-self-driving-car-programs]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[828]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Self-driving cars and &#039;Project Titan&#039; still a big focus at Apple, according to patent filings</title>
		<link>https://fifthlevel.ai/archives/726</link>
		<pubDate>Thu, 22 Mar 2018 15:19:20 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/03/22/self-driving-cars-and-project-titan-still-a-big-focus-at-apple-according-to-patent-filings</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/25368-34362-Screen-Shot-2018-03-22-at-144652-l.jpg" alt="Article Image" border="0" /> <br><br> The latest Apple patent applications published by the US Patent and Trademark Office feature a number of filings relating to its self driving car efforts, including gesture recognition for both passengers inside the vehicle and for pedestrians diverting traffic on the road. <p><b><a href="https://appleinsider.com/articles/18/03/22/self-driving-cars-and-project-titan-still-a-big-focus-at-apple-according-to-patent-filings" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>726</wp:post_id>
		<wp:post_date><![CDATA[2018-03-22 15:19:20]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-22 15:19:20]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[self-driving-cars-and-project-titan-still-a-big-focus-at-apple-according-to-patent-filings]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/03/22/self-driving-cars-and-project-titan-still-a-big-focus-at-apple-according-to-patent-filings]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[826]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple&#039;s self-driving car test fleet up to 45 vehicles navigating California roads</title>
		<link>https://fifthlevel.ai/archives/727</link>
		<pubDate>Tue, 20 Mar 2018 16:49:32 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/03/20/apples-self-driving-car-test-fleet-up-to-45-vehicles-navigating-california-roads</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/25327-34215-applecar-testbed2-l.jpg" alt="Article Image" border="0" /> <br><br> Apple is rapidly growing the number of self-driving test vehicles it has in its home state, putting it at second place there behind only General Motors' Cruise subsidiary. <p><b><a href="https://appleinsider.com/articles/18/03/20/apples-self-driving-car-test-fleet-up-to-45-vehicles-navigating-california-roads" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>727</wp:post_id>
		<wp:post_date><![CDATA[2018-03-20 16:49:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-20 16:49:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apples-self-driving-car-test-fleet-up-to-45-vehicles-navigating-california-roads]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/03/20/apples-self-driving-car-test-fleet-up-to-45-vehicles-navigating-california-roads]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[825]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Uber unlikely to blame for self-driving car fatality, says police chief</title>
		<link>https://fifthlevel.ai/archives/728</link>
		<pubDate>Tue, 20 Mar 2018 16:46:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/03/20/uber-unlikely-to-blame-for-self-driving-car-fatality-says-police-chief</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/25328-34214-uberself-l.jpg" alt="Article Image" border="0" /> <br><br> Following the death late Sunday of a woman who was struck and killed by a self-driving Uber car in Arizona, we now know more about the circumstances of the tragedy, which has a chance to shake public trust in autonomous car technology. <p><b><a href="https://appleinsider.com/articles/18/03/20/uber-unlikely-to-blame-for-self-driving-car-fatality-says-police-chief" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>728</wp:post_id>
		<wp:post_date><![CDATA[2018-03-20 16:46:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-20 16:46:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[uber-unlikely-to-blame-for-self-driving-car-fatality-says-police-chief]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/03/20/uber-unlikely-to-blame-for-self-driving-car-fatality-says-police-chief]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[824]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The Audi change of perspective</title>
		<link>https://fifthlevel.ai/archives/747</link>
		<pubDate>Mon, 19 Mar 2018 10:23:06 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://aid-driving.eu/?p=3815</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Der Beitrag <a rel="nofollow" href="http://aid-driving.eu/the-audi-change-of-perspective/">The Audi change of perspective</a> erschien zuerst auf <a rel="nofollow" href="http://aid-driving.eu">AID</a>.</p> <p><b><a href="http://aid-driving.eu/the-audi-change-of-perspective/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>747</wp:post_id>
		<wp:post_date><![CDATA[2018-03-19 10:23:06]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-19 10:23:06]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[the-audi-change-of-perspective]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/the-audi-change-of-perspective/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Depth-sensing imaging system can peer through fog</title>
		<link>https://fifthlevel.ai/archives/1135</link>
		<pubDate>Wed, 21 Mar 2018 03:59:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/depth-sensing-imaging-system-can-peer-through-fog-0321</guid>
		<description></description>
		<content:encoded><![CDATA[<p>MIT researchers have developed a system that can produce images of objects shrouded by fog so thick that human vision can’t penetrate it. It can also gauge the objects’ distance.</p> <p>An inability to handle misty driving conditions has been one of the chief obstacles to the development of autonomous vehicular navigation systems that use visible light, which are preferable to radar-based systems for their high resolution and ability to read road signs and track lane markers. So, the MIT system could be a crucial step toward self-driving cars.</p> <p>The researchers tested the system using a small tank of water with the vibrating motor from a humidifier immersed in it. In fog so dense that human vision could penetrate only 36 centimeters, the system was able to resolve images of objects and gauge their depth at a range of 57 centimeters.</p> <p>Fifty-seven centimeters is not a great distance, but the fog produced for the study is far denser than any that a human driver would have to contend with; in the real world, a typical fog might afford a visibility of about 30 to 50 meters. The vital point is that the system performed better than human vision, whereas most imaging systems perform far worse. A navigation system that was even as good as a human driver at driving in fog would be a huge breakthrough.</p> <p>“I decided to take on the challenge of developing a system that can see through actual fog,” says Guy Satat, a graduate student in the MIT Media Lab, who led the research. “We’re dealing with realistic fog, which is dense, dynamic, and heterogeneous. It is constantly moving and changing, with patches of denser or less-dense fog. Other methods are not designed to cope with such realistic scenarios.”</p> <p>Satat and his colleagues describe their system in a paper they’ll present at the International Conference on Computational Photography in May. Satat is first author on the paper, and he’s joined by his thesis advisor, associate professor of media arts and sciences Ramesh Raskar, and by Matthew Tancik, who was a graduate student in electrical engineering and computer science when the work was done.</p> <div class="cms-placeholder-content-video"></div> <p><strong>Playing the odds</strong></p> <p>Like <a href="http://news.mit.edu/2017/new-depth-sensors-could-be-sensitive-enough-self-driving-cars-1222">many</a> of the <a href="http://news.mit.edu/2016/computational-imaging-method-reads-closed-books-0909">projects</a> <a href="http://news.mit.edu/2012/camera-sees-around-corners-0321">undertaken</a> in Raskar’s Camera Culture Group, the new system uses a time-of-flight camera, which fires ultrashort bursts of laser light into a scene and measures the time it takes their reflections to return.</p> <p>On a clear day, the light’s return time faithfully indicates the distances of the objects that reflected it. But fog causes light to “scatter,” or bounce around in random ways. In foggy weather, most of the light that reaches the camera’s sensor will have been reflected by airborne water droplets, not by the types of objects that autonomous vehicles need to avoid. And even the light that does reflect from potential obstacles will arrive at different times, having been deflected by water droplets on both the way out and the way back.</p> <p>The MIT system gets around this problem by using statistics. The patterns produced by fog-reflected light vary according to the fog’s density: On average, light penetrates less deeply into a thick fog than it does into a light fog. But the MIT researchers were able to show that, no matter how thick the fog, the arrival times of the reflected light adhere to a statistical pattern known as a gamma distribution.</p> <p>Gamma distributions are somewhat more complex than Gaussian distributions, the common distributions that yield the familiar bell curve: They can be asymmetrical, and they can take on a wider variety of shapes. But like Gaussian distributions, they’re completely described by two variables. The MIT system estimates the values of those variables on the fly and uses the resulting distribution to filter fog reflection out of the light signal that reaches the time-of-flight camera’s sensor.</p> <p>Crucially, the system calculates a different gamma distribution for each of the 1,024 pixels in the sensor. That’s why it’s able to handle the variations in fog density that foiled earlier systems: It can deal with circumstances in which each pixel sees a different type of fog.</p> <p><strong>Signature shapes</strong></p> <p>The camera counts the number of light particles, or photons, that reach it every 56 picoseconds, or trillionths of a second. The MIT system uses those raw counts to produce a histogram — essentially a bar graph, with the heights of the bars indicating the photon counts for each interval. Then it finds the gamma distribution that best fits the shape of the bar graph and simply subtracts the associated photon counts from the measured totals. What remain are slight spikes at the distances that correlate with physical obstacles.</p> <p>“What’s nice about this is that it’s pretty simple,” Satat says. “If you look at the computation and the method, it’s surprisingly not complex. We also don’t need any prior knowledge about the fog and its density, which helps it to work in a wide range of fog conditions.”</p> <p>Satat tested the system using a fog chamber a meter long. Inside the chamber, he mounted regularly spaced distance markers, which provided a rough measure of visibility. He also placed a series of small objects — a wooden figurine, wooden blocks, silhouettes of letters — that the system was able to image even when they were indiscernible to the naked eye.</p> <p>There are different ways to measure visibility, however: Objects with different colors and textures are visible through fog at different distances. So, to assess the system’s performance, he used a more rigorous metric called optical depth, which describes the amount of light that penetrates the fog.</p> <p>Optical depth is independent of distance, so the performance of the system on fog that has a particular optical depth at a range of 1 meter should be a good predictor of its performance on fog that has the same optical depth at a range of 30 meters. In fact, the system may even fare better at longer distances, as the differences between photons’ arrival times will be greater, which could make for more accurate histograms.</p> <p>“Bad weather is one of the big remaining hurdles to address for autonomous driving technology,” says Srinivasa Narasimhan, a professor of computer science at Carnegie Mellon University. “Guy and Ramesh’s innovative work produces the best visibility enhancement I have seen at visible or near-infrared wavelengths and has the potential to be implemented on cars very soon.”</p>
<p><b><a href="http://news.mit.edu/2018/depth-sensing-imaging-system-can-peer-through-fog-0321" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1135</wp:post_id>
		<wp:post_date><![CDATA[2018-03-21 03:59:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-21 03:59:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[depth-sensing-imaging-system-can-peer-through-fog]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/depth-sensing-imaging-system-can-peer-through-fog-0321]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>3 Questions: The future of transportation systems</title>
		<link>https://fifthlevel.ai/archives/1136</link>
		<pubDate>Tue, 13 Mar 2018 20:40:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/3-questions-daniel-sperling-UC-Davis-future-our-transportation-systems-0313</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>Daniel Sperling is a distinguished professor of civil engineering and environmental science and policy at the University of California at Davis, where he is also founding director of the school’s Institute of Transportation Studies. Sperling, a member of the California Air Resources Board, recently gave a talk at MITEI detailing major technological and societal developments that have the potential to change transportation for the better — or worse. Following the event, Sperling spoke to MITEI about policy, science, and how to harness these change agents for the public good.</em></p> <div class="cms-placeholder-content-video"></div> <p>(Sperling's talk is also available as a <a href="https://soundcloud.com/mitenergy/three-transportation-revolutions">podcast</a>.)</p> <p><strong>Q:&nbsp;</strong>What are the downsides of the “car-centric monoculture,” as you put it, that we find ourselves living in?</p> <p><strong>A:&nbsp;</strong>Cars provide great value, which is why they are so popular. But too much of a good thing can be destructive.&nbsp;We’ve gone too far. We’ve created a transportation system made up of massive road systems and parking infrastructure that is incredibly expensive for travelers and for society to build and maintain. It is also very energy- and carbon-intensive, and disadvantages those unable to buy and drive cars.</p> <p><strong>Q:</strong>&nbsp;Can you tell me about the three transportation revolutions that you say are going to transform mobility over the next few decades?</p> <p><strong>A:&nbsp;</strong>The three revolutions are electrification, automation, and pooling. Electrification is already under way, with increasing numbers of pure battery electric vehicles, plug-in hybrid vehicles that combine batteries and combustion engines, and fuel cell&nbsp;electric vehicles that run on hydrogen. I currently own a hydrogen car (Toyota Mirai) and have owned two different battery electric cars (Nissan Leaf and Tesla).</p> <p>A second revolution, automation, is not yet under way, at least in the form of driverless cars. But it is poised to be truly transformational and disruptive for many industries — including automakers, rental cars, infrastructure providers, and transit operators. While partially automated cars are already here, true transformations await fully driverless vehicles, which are not likely to exist in significant numbers for a decade or more.</p> <p>Perhaps the most pivotal revolution, at least in terms of assuring that the automation revolution serves the public interest, is pooling, or sharing. Automation without pooling would lead to large increases in vehicle use. With pooling, though, automation would lead to reductions in vehicle use, but increases in mobility (passenger miles traveled) by mobility-disadvantaged travelers who are too poor or disabled to drive.</p> <p><strong>Q:</strong>&nbsp;You’ve mentioned that how these revolutions play out depends on which cost factor dominates — money or time. The result would either be heaven&nbsp;or hell&nbsp;for our environment and cities. Explain the nuances of that situation.</p> <p><strong>A:</strong>&nbsp;With pooled, automated and electric cars, the cost of travel would drop precipitously as a result of using cars intensively — spreading costs over 100,000 miles or more per year — having no driver costs, and having multiple riders share the cost. The monetary cost could be as little as 15 cents&nbsp;per mile, versus 60 cents&nbsp;per mile for an individually-owned automated car traveling 15,000 miles per year. The time cost of car occupants, on the other hand, is near zero because they don’t need to pay attention to driving. They can work, sleep, text, drink, and read. Thus, even if the cost of owning and operating the vehicle is substantial, the time savings would be so beneficial that many, perhaps most, would choose car ownership over subscribing to an on-demand service. In fact, most people in affluent countries would likely choose the huge time savings, worth $10, $20, or more per hour, over low travel costs. Thus, policy will be needed to assure that the public interest — environmental externalities, urban livability, access by the mobility disadvantaged — is favored over the gains of a minority of individuals.</p>
<p><b><a href="http://news.mit.edu/2018/3-questions-daniel-sperling-UC-Davis-future-our-transportation-systems-0313" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1136</wp:post_id>
		<wp:post_date><![CDATA[2018-03-13 20:40:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-13 20:40:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[3-questions-the-future-of-transportation-systems]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/3-questions-daniel-sperling-UC-Davis-future-our-transportation-systems-0313]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Uber Sticking By Self-Driving Cars Despite Recent Fatality</title>
		<link>https://fifthlevel.ai/archives/1153</link>
		<pubDate>Sat, 14 Apr 2018 07:48:54 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/239180/uber-sticking-with-self-driving-cars/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[The company wants to resume testing of its driverless fleet. <p><b><a href="https://www.motor1.com/news/239180/uber-sticking-with-self-driving-cars/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1153</wp:post_id>
		<wp:post_date><![CDATA[2018-04-14 07:48:54]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-04-14 07:48:54]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[uber-sticking-by-self-driving-cars-despite-recent-fatality]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/239180/uber-sticking-with-self-driving-cars/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>2019 Nissan Altima To Have ProPilot Assist Semi-Autonomous Tech</title>
		<link>https://fifthlevel.ai/archives/1154</link>
		<pubDate>Fri, 23 Mar 2018 15:07:13 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/237112/2019-nissan-altima-propilot-assist/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[New midsize sedan makes its debut next week in New York. <p><b><a href="https://www.motor1.com/news/237112/2019-nissan-altima-propilot-assist/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1154</wp:post_id>
		<wp:post_date><![CDATA[2018-03-23 15:07:13]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-23 15:07:13]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[2019-nissan-altima-to-have-propilot-assist-semi-autonomous-tech]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/237112/2019-nissan-altima-propilot-assist/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Dashcam Footage Of Fatal Autonomous Uber Crash Released [UPDATE]</title>
		<link>https://fifthlevel.ai/archives/1155</link>
		<pubDate>Wed, 21 Mar 2018 23:58:11 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/236613/uber-autonomous-vehicle-testing-death/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[Driver and vehicle still not believed to be at fault. <p><b><a href="https://www.motor1.com/news/236613/uber-autonomous-vehicle-testing-death/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1155</wp:post_id>
		<wp:post_date><![CDATA[2018-03-21 23:58:11]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-21 23:58:11]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[dashcam-footage-of-fatal-autonomous-uber-crash-released-update]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/236613/uber-autonomous-vehicle-testing-death/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>We need a moratorium on new public transport projects!</title>
		<link>https://fifthlevel.ai/archives/2474</link>
		<pubDate>Sat, 31 Mar 2018 12:39:32 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.driverless-future.com/?p=1139</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Across the world billions of dollars are committed every year for new public transport and road infrastructure projects: commuter rail, subways, new roads, bypasses, tunnels, bridges, etc. Committees spend years planning these projects; it often takes more than a decade until a project is implemented. Once completed, we expect the projects to yield their benefits over many, many decades. Over the last century planning and estimation processes have been refined; they work reasonably well. Unfortunately, current processes can not and do not take self-driving vehicles into account. But it is now clear that self-driving cars will fundamentally change our traffic patterns. This greatly increases the risk that public transport projects will already be obsolete at the time they are completed. In the following we will show that the most adequate action for cities and states is a temporary moratorium on new public transport projects (i.e. by systematically delaying the start of the planning phase):</p>
<p>At the current point in time self-driving car technology is not yet ready for widespread adoption but there can no longer by any doubt about its viability. Many companies are racing for implementation. Millions of kilometers are now routinely test-driven in self-driving vehicles; GM and Jaguar have started producing self-driving car models; Waymo is now operating self-driving cars without test-drivers inside the car. Anyone who performs an extensive analysis about the size of the self-driving car problem, the economic incentives for participants in the self-driving car space and the state of the industry must come to the conclusion that we are very likely to have large numbers of self-driving cars, buses, trucks and machines in our cities within the next decade (see the postscript of this article for a brief outline of key elements of such an analysis).</p>
<p>Once self-driving cars operate in cities by the thousands, we will see fundamental changes: the number of privately owned cars will fall. The higher urban density, the quicker car ownership will recede and with it parking lots. Traditional public transport will be challenged by self-driving taxis and ridesharing services. Rail-based transport solutions will suffer from their inflexibility compared to buses. The biggest problems will occur on the feeder lines; not so much on the high-capacity, high-frequency core lines. Urban and highway traffic will flow better as self-driving cars become life traffic sensors and city-wide traffic routing algorithms are applied (no, this is not science fiction, this will be a core, immanent concern of any provider of self-driving mobility services and has the benefit of being a win-win situation (identical goals) with city traffic management). We will see the distribution of traffic change significantly as trucks begin to operate 24/7, self-driving fleet vehicles are applied for delivery at night and ridesharing services increase the average occupancy per vehicle on certain routes (more likely on long distance trips as well as long commutes, less relevant for inner cities). As a consequence our road-based mobility system will change fundamentally. Of course, this will not occur over night, but the changes will greatly affect any new road infrastructure project being planned today.</p>
<p>Ideally, we would map out these changes today and then plan for the kind of mobility system which will be operating in the forties and fifties of this century. But there is too much uncertainty and too little chance to achieve widespread agreement on what this situation will look like. There is no established knowledge, and no agreement on how to determine the likely scenarios.</p>
<p>But even if we cannot yet reach agreement on what the future will look like, it should be possible to reach agreement that this future will be very, very different from the one we are planning for today. Given the uncertainties, there are three possibilities:<br />
1) Continue planning on the basis of our current processes and knowledge.<br />
2) Delay projects for a few years, hoping for improvements in our understanding of the effects of SDC technology<br />
3) Design new projects with robustness or elements targeted for self-driving car scenarios</p>
<p>The conditions under which alternative 1 is rational are very narrow: This only makes sense for projects which are unlikely to be challenged significantly by self-driving cars. New rail-based projects certainly do not fall int his category. But bypasses, highway extensions (or new highways) and most other projects also critically depend on estimates of traffic distributions which we can no longer extrapolate from today. Therefore we must balance the disadvantages of delaying the start of such a project for a few years against the advantages of fewer expenses in the near- and medium term and possibly a better system in the long term. Because we are likely to have much better ways of managing traffic flow in 20 years it is unlikely that the congestion problems which we may fear as a result of delaying a project today will actually materialize. If we do business as usual, we may find in 20 years time that a significant share of the projects we are starting today are no longer necessary and billions of dollars have been wasted.</p>
<p>Given the state of our knowledge it appears difficult to design robustness for self-driving car scenarios (Alternative 3) into new projects today.</p>
<p>Thus the only viable option is Alternative 2. If we delay new projects for a few years, we save tax payer money but don&#8217;t have to fear enormous congestion in a few decades because self-driving car technology will give us many new levers for improving traffic flow. By delaying projects, we will increase our common knowledge and shared understanding of the impact of self-driving cars. Simulations, scientific research, experiences from the first installations of fleest of self-driving cars (such as Waymo in Phoenix) will provide us with insights that we can apply for the planning and estimation of new public transport and road infrastructure projects. We will learn how traffice changes in the first cities with self-driving cars; we will understand that fleets have an impact on the way that traffic is routed and that we can use them to detect and combat congestion. We will be much more open to consider introducing new parameters into our mobility infrastructure: some lanes might be dedicated to self-driving cars only; they could be made narrower because these cars can drive with more precision. We might change the direction of inner lanes on some roads depending on travel flows or revert parking lots on the side to be used as lanes during peak hours (only self-driving vehicles would be permitted to park there at night; they would be in use during the day or have to drive themselves to another parking space before peak hours begin).</p>
<p>Thus at this point, the most rational approach for new public transport and road infrastructure projects is to put the initiative on hold! This is an action for which a consensus can be found much more easily among the various stakeholders than finding consensus to plan directly for an unknown future with self-driving cars. It also has the side-benefit of increasing the pressure on the planners to seriously consider the effects of self-driving cars. We will all be better off if we place a moratorium on new public transport and road infrastructure projects today!</p>
<p><strong>Postscript:</strong><br />
A key element of the argument presented in this article is the claim that we are likely to have very many self-driving cars, buses, trucks and machines in our cities within the next decade. This is not obvious and runs counter to the quick, intuitive assessment of many. Unfortunately, the matter is complex and requires an intensive look at the issues from multiple perspectives &#8211; technical, economic, legal, innovation theoretic. Misleading intuitions can not be eradicated with just a few sentences because they are usually based on too many self-supporting half-truths (see my paper on <a href="http://www.driverless-future.com/?page_id=774">Misconceptions of self-driving cars</a>). If you want to spend time to think the different aspects through, here is a brief outline of some of the issues (for more on this, attend one of my workshops or contact me directly):</p>
<p><strong>1. Technology</strong><br />
<strong>1.1. Problem of full self-driving has been shown as solvable.</strong><br />
1.1.1. Problem is inherently information processing in a limited, but complex domain. Interpretation is hard but can be solved with current methods.<br />
1.1.2. Known limitations (driving in snow / heavy rain etc.) are not fundamental limitations<br />
1.1.3. Self-driving car does not need general world (human-like) intelligence.<br />
1.1.4. Remote operations center can handle many of the so-called hard problems (i.e. interpreting police office hand waves)<br />
1.1.5. Having to use pre-defined maps is not a limitation for nationwide rollout (and nationwide rollout will not be the initial use-case anyway)</p>
<p><strong> 1.2. Self-driving cars will reach a state where they are much safer than the average human driver</strong><br />
1.2.1. Much better attention than human drivers<br />
1.2.2. Larger field of view than human drivers (exception: highways)<br />
1.2.3. Fast, continous learning and refinement of algorithms.<br />
1.2.4. Human drivers make many preventable accidents.<br />
1.2.5. Human is better at interpreting certain rare scenes<br />
1.2.6. Self-driving cars are better at detecting common situations early<br />
1.2.7. Vehicles have sufficient processing power and sensor mix for self-driving<br />
1.2.8. Economic usefulness of SDC technology does not require ability to operate everywhere (-&gt; technology can start early)</p>
<p><strong> 1.3. Rapid evolution of technology</strong><br />
1.3.1. Innovation process is spread across the world; involves many companies in hardware, sensors, software, mobility, etc.<br />
1.3.2. Enormous progress in AI algorithms<br />
1.3.3. Sensor mix is maturing; still rapid innovation in sensor technology and rapid fall of sensor and hardware prices<br />
1.3.4. Number of companies working on self-driving cars still increasing<br />
1.3.5. Production of first self-driving car models has already started (GM/Jaguar/not quite there yet: Tesla)</p>
<p><strong>2. Economics</strong><br />
<strong> 2.1. Disruptive potential of self-driving cars is eliminating the driver</strong><br />
2.1.1. All industries potentially affected from impact on logistics<br />
2.1.2. SDC technology challenges competitive position of many companies (not just auto industry) and countries -&gt; enormous competitive pressure<br />
2.1.3. Removing the need for a driver requires rethinking all processes in the auto / transport / mobility industry<br />
2.1.4. Diffusion of self-driving cars at a much faster pace than other auto industry innovations (won&#8217;t take decades)</p>
<p><strong> 2.2. SDCs will lead to increased use of mobility as a service</strong><br />
2.2.1. Car ownership must fall (a detailed analysis of cost/benefit/comfort associated with owning a car / calling a self-driving taxi)<br />
2.2.2. Self-driving taxis will slash costs for individual motorized mobility (but costs for privately owned SDCs will rise compared to current cars)<br />
2.2.3. Vehicle stock in developed nations will fall significantly<br />
2.2.4. Mobility as a service market exhibits network effects -&gt; first mover advantage means winner gets all -&gt; extreme race for being first<br />
2.2.5. Regulation of SDC fleets by cities or countries is very likely<br />
2.2.6. Public transport will face significant challenges from providers of self-driving mobility services<br />
2.2.7. Rail-based networks are at a disadvantage because of their low flexibility. Only high-capacity lines can remain profitable.<br />
2.2.8. SDCs will increase throughput in cities; increased congestion very unlikely (this is contrary to many intuitions)</p>
<p><strong> 2.3. SDCs will increase person-kilometers traveled but not necessarily vehicle-kilometers traveled (impacted by occupancy rate)</strong><br />
2.3.1. Mobility services are not just economically viable in high density urban areas but also in many lower density rural areas (of the United States)<br />
2.3.2. Occupancy rate for long-distance trips and longer commutes will increase<br />
2.3.3. In short, local travel passengers will be reluctant to share rides; buses will be preferred compared to taxis for ridesharing<br />
2.3.4. Self-driving long distance buses will multiply</p>
<p><strong> 2.4. High valuation of SDC-related businesses</strong><br />
2.4.1. Enormous capital inflow for all business related to self-driving car technology because of high potential gains associated with market shakeups<br />
2.4.2. High-priced human capital in self-driving car technology; rapid movement between companies (rapid transfer for knowledge from leaders to well funded followers)<br />
2.4.3. Number of cars sold will fall. OEMs will loose significant revenue. Not all OEMs will be able to survive this transition of the industry.<br />
2.4.4. Auto industry will change. Fleets will be powerful customers and heavily influence vehicle design.<br />
2.4.5. SDCs will slash delivery costs.<br />
2.4.6. Ecommerce will grow. Retail will suffer. Supermarkets will close.</p>
<p><strong> 3. Legal/political</strong><br />
<strong>  3.1. Self-driving car technology seen as key technology affecting global balance of economic and military power</strong><br />
3.1.1. Countries compete to grow/protect their own self-driving car technology and related industries<br />
3.1.2. Opposition to SDCs does not have force. Risk of job loss widely acknowledged but potential benefits to population as a whole are too large<br />
3.1.3. Regulatory bodies have a lack of knowledge and competence in rapidly evolving SDC technology;<br />
3.1.4. If perceived as necessary, regulatory approval for self-driving cars may occur rapidly<br />
3.1.5. Very little attention yet on the wider regulatory implications of self-driving cars (in cities, as a business model, as a universal service, as a competitor to public transport etc.)</p>
<p><strong> 4. Innovation</strong><br />
<strong>  4.1. Innovation process</strong><br />
4.1.1. Self-driving car technology close to end of fluid first phase of disruptive innovation processes<br />
4.1.2. Shakeout and consolidation in the SDC technology likely to be observable soon<br />
4.1.3. Self-driving car hard- and software likely to become commoditized. Not a source of long-term competitive advantage.</p>
<p><strong>  4.2. Diffusion</strong><br />
4.2.1. Self-driving car technology will be adapted very rapidly. First mover advantage for fleets. Heavy demand by affluent consumers expected.<br />
4.2.2. Catalyst for innovation in other areas. Transport, retail, autonomous machines and solutions.<br />
4.2.3. Enabler for electric vehicles. Self-driving cars will greatly increase not just the number of electric vehicles but explode the number of person kilometers traveled through self-driving electric (fleet) vehicles</p>
<p><strong>  4.3. New technology leads to new possibilities which will be discovered, embraced, regulated, mandated</strong><br />
4.3.1. Self-driving vehicles will work as traffic sensors<br />
4.3.2. Self-driving vehicles can be used to influence and control traffic<br />
4.3.3. Fleets of self-driving vehicles lead to a much better real-time understanding of traffic<br />
4.3.4. Congestion-causing effect of a trip can be determined, quantified, taxed etc.<br />
4.3.5. Building codes will reduce requirements for number of parking lots</p>
<p><strong>  4.4. Changes in attitudes</strong><br />
4.4.1. Human error in driving will no longer be tolerated (does not mean no more human driving, could just mean that SDC algorithms oversee human driving)<br />
4.4.2. Personal car ownership may be banned in some inner cities<br />
4.4.3. Preferences for privately owning a car may fall<br />
4.4.4. If global warming becomes more of a threat car ownership may be increasingly viewed critical given ubiquitous access to mobility services</p>
<p>&nbsp;</p> <p><b><a href="http://www.driverless-future.com/?p=1139" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2474</wp:post_id>
		<wp:post_date><![CDATA[2018-03-31 12:39:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-31 12:39:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[we-need-a-moratorium-on-new-public-transport-projects]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?feed=rss2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?p=1139]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Safety and Driver Attention</title>
		<link>https://fifthlevel.ai/archives/3255</link>
		<pubDate>Fri, 13 Apr 2018 20:45:22 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/2a33d3d23109</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I’m sure if you follow the self driving car space, you’ve heard about the two recent deaths in the news. The woman <a href="https://twitter.com/TempePolice/status/976585098542833664">hit by the Uber in self driving mode</a> and the man who <a href="https://www.tesla.com/blog/update-last-week%E2%80%99s-accident">drove into a guardrail in a Tesla on Autopilot</a>. Like many other deaths caused by inattentive drivers, <em>we believe both those deaths would have been prevented had the driver of the car been paying attention</em>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*kWPqETkzKKcWpOQfhdAzvw.jpeg" /><figcaption>openpilot user getting a distracted alert (real user photo, not actor)</figcaption></figure><p>When you are behind the wheel of a car, you must always be paying attention! There is no software yet that is capable of driving better than a human. But one of the dangers as the software gets better is that this is not always apparent.</p><p>Currently, our data shows that <a href="https://github.com/commaai/openpilot">openpilot</a> makes multiple minor mistakes requiring correction per drive. As long as this is the case, no one will confuse the system with a superhuman driver. But as our software improves to the point that mistakes have hours between them, this might no longer be true.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/848/1*n5ZQlYrxkFFRqPP4ujeR0w.png" /></figure><p>We believe the solution to this is <strong>driver monitoring</strong>. GM has pioneered this with Super Cruise. And from <a href="http://lexfridman.com/">Lex Fridman</a> at MIT, <a href="https://www.youtube.com/watch?v=iMOD9TYRtr8&amp;feature=youtu.be&amp;t=70">here is an example</a> of glance classification applied to the safety driver in the Uber crash. (looking forward to the open source release!)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vsbS7mCCqas9sUCH-LL6Uw.png" /><figcaption>Screengrab from video taken with EON Dashcam Devkit</figcaption></figure><p>Today, we are taking the first step toward our driver monitoring strategy. In openpilot 0.4.4, we’ve added a setting to record and upload the front camera. This setting is off by default, but if you choose to opt in, you’ll be rewarded with 0.5 extra comma points per minute, and you’ll be helping us to train our driver classification network. You’ll also be able to review the data later.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iWN8JBMG_xVGpq7Ms6z7Ng.png" /><figcaption>See the “Front Camera Record and Upload” option?</figcaption></figure><p>Remember, if you can’t see the camera, it can’t see you! When <a href="https://shop.comma.ai/products/eon-dashcam-devkit">EON</a> was designed, it was made to be flipped by switching the top and bottom piece with the supplied allen key. So flip your EON for 0.4.4, it looks better upside down anyway. If you still have a NEO, we are <a href="https://shop.comma.ai/products/neo-to-eon-upgrade">offering a trade-in</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*83sFV4CUq5qNJMycfcmyvA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2DWSRKBpyao4TIGVGRXEjQ.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ObknNGXT2VRv_A4vy6Pp_g.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AyndBROxyRLBIcZK2l0QwA.jpeg" /><figcaption>How to flip an EON, in pictures</figcaption></figure><p>You can also watch the <a href="https://www.youtube.com/watch?v=JPNzmAprWZs&amp;feature=youtu.be">EON video flip tutorial</a> by <a href="https://www.youtube.com/channel/UCeHc0xNYWSSoS1aBZ_xmDbw">VirtuallyChris</a>.</p><p>In all our future hardware, we plan to have a camera for monitoring the driver. And in the period where the statistics show our software is good enough to be trusted, but not yet superhuman, monitoring will be mandatory.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dJ2C0Y7iJVAQiP223Jm_Fg.jpeg" /><figcaption>Flipped EON (camera in the lower left now)</figcaption></figure><p><em>Like seeing real progress made in self driving cars?<br></em><a href="https://twitter.com/comma_ai">Follow us on Twitter</a>, @comma_ai</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2a33d3d23109" width="1" height="1"> <p><b><a href="https://medium.com/@comma_ai/safety-and-driver-attention-2a33d3d23109?source=rss-330bac69b283------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3255</wp:post_id>
		<wp:post_date><![CDATA[2018-04-13 20:45:22]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-04-13 20:45:22]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[safety-and-driver-attention]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@comma_ai/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@comma_ai/safety-and-driver-attention-2a33d3d23109?source=rss-330bac69b283------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Gold Star Minute</title>
		<link>https://fifthlevel.ai/archives/3256</link>
		<pubDate>Sun, 18 Mar 2018 01:18:48 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/6276bef7aec6</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Here at comma.ai, you all teach us how to drive. But a question we get a lot is, how much are we teaching the car? Are we teaching the car the right things.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/300/1*1WdhH6Z_cecs17e-B2hQ_Q.png" /><figcaption>Perhaps representative of what you might get</figcaption></figure><p>Now you can finally know! If you are doing well, just like in school, your minute of driving will get a gold star!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ltMZIytfzK5RgKXew2A0PA.png" /><figcaption>Picture from Gold Star Minute</figcaption></figure><ol><li><strong>Have a supported car</strong> — You’ll need a <a href="https://github.com/commaai/openpilot/blob/devel/README.md">car openpilot supports</a>, aka a modern Honda or Toyota. In order for our models to learn, we use the sensors in the car.</li><li><strong>Have supported hardware</strong> — To get Gold Star Minutes, you’ll need a <a href="https://shop.comma.ai/products/eon-dashcam-devkit"><em>comma EON Dashcam</em></a>. You’ll also need a <a href="https://shop.comma.ai/products/panda-obd-ii-dongle?variant=3024843505677"><em>grey panda</em></a> properly connected to an appropriate <em>giraffe</em>. Fortunately, all of these items are available in the <a href="https://shop.comma.ai/">comma.ai shop</a>.</li><li><strong>Have supported software</strong> — Your EON will need to be running openpilot or chffrplus ≥ 0.4.3.2. That’s the first one with HDR and autofocus. You don’t even have to go to the shop for this, only <a href="https://github.com/commaai">GitHub</a>.</li><li><strong>Mount your hardware properly</strong> — EON need to be mounted stably, so they don’t wobble all over the place. grey panda antenna need to see the sky. Included mounts are great. And clean your dirty windshield!</li><li><strong>Move</strong> — Move more than 100 meters in both this minute, the last minute, and the next minute. That’s an average speed of 6 miles per hour. You can almost run that fast…</li><li><strong>Don’t crash</strong> — This is obvious. We will not give gold stars if you crash.</li><li><strong>Drive in USA or Canada </strong>— Gold Star Minute program is USA and Canada only for now. Will be expanded. (note that chffrplus/openpilot will still work elsewhere)</li><li><strong>Drive over 4 minutes </strong>— The first 2 and last 2 minutes of a drive can’t be Gold Star Minutes.</li><li><strong>Things out of your control</strong> — Kalman filter didn’t init right? Government broke the GPS satellites? ORB features matched poorly? You might have done nothing wrong and still not get a Gold Star. I’m sorry life isn’t fair. We are working to make life as fair as possible.</li></ol><p>*Gold Star Minute requirements liable to change at any time. Check this medium post for updates. Just like a schoolteacher, we will try our best to give feedback to help you make more of your minutes gold star minutes!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/857/1*YiPBEp_dnI96cHXS3t-dVQ.png" /><figcaption>Mapping a Gold Star Minute</figcaption></figure><p>Gold Star Minutes won’t just be used for training models. (btw, the more you gold star, the more the model will drive like you) They will also be used for making HD maps. Soon™ you might even be able to see the maps. And eventually™, your openpilot will stop at the stop signs and stop lights from your gold star minutes.</p><p>Any questions, <a href="http://slack.comma.ai">join us on Slack</a>.</p><p>And always, <a href="https://twitter.com/comma_ai">follow us on Twitter</a> @comma_ai</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6276bef7aec6" width="1" height="1"> <p><b><a href="https://medium.com/@comma_ai/gold-star-minute-6276bef7aec6?source=rss-330bac69b283------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3256</wp:post_id>
		<wp:post_date><![CDATA[2018-03-18 01:18:48]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-18 01:18:48]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[gold-star-minute]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@comma_ai/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@comma_ai/gold-star-minute-6276bef7aec6?source=rss-330bac69b283------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Hundreds of Computer Vision Researchers Gathered in Munich to Talk Self-Driving</title>
		<link>https://fifthlevel.ai/archives/3272</link>
		<pubDate>Thu, 05 Apr 2018 21:51:16 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/8e7872f08747</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*No-UgVtnKhngbZmrOmWv5Q.jpeg" /></figure><p>In March 2018, we hosted our first event in Munich: a Computer Vision Meetup. Over two hundred computer vision researchers and engineers gathered to hear the latest from Lyft Level 5, Intel Labs, and TUM professor, Daniel Cremers.</p><p>Our Munich team is growing! To learn more and view openings, visit lyft.com/level5.</p><h3><strong>Here’s what you missed</strong></h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xbaGYKsDCEuvG4V4Ac0hcw.jpeg" /></figure><p>Holger Rapp and Wolfgang Hess from our Munich office talked about SLAM (Simultaneous localization and mapping) for self-driving cars. SLAM provides highly detailed maps and accurate localization for autonomous cars, even in GPS-deprived surroundings like cities. They dove into the fast loop closing algorithm in their Cartographer paper and how it was generalized to 3D.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*X--nUo8ahAdljI_3oPNWXQ.jpeg" /></figure><p>Matt Vitelli covered current state-of-the-art techniques for processing high-resolution camera imagery and LiDAR point clouds using a combination of classical computer vision techniques, deep learning models, and other heuristics. He then walked through a few experiments Level 5 has tackled, including training an end-to-end network for predicting steering angle from camera data, bounding box detection from high-resolution imagery, LiDAR point cloud segmentation, and fusing the data together.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EOe4ULGijMjdXjKof6TW9Q.jpeg" /></figure><p>Alexey Dosovitskiy from Intel Labs talked about how applications of deep learning in autonomous driving are complicated by both logistical and algorithmic difficulties. He introduced us to CARLA, a high-fidelity open urban driving simulator that intends to democratize research on autonomous driving.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Y_dpIPHlX0IJoAYWYRiNVQ.jpeg" /></figure><p>Lastly, Daniel Cremers, Chair for Computer Vision &amp; Artificial Intelligence at TUM, shared a number of recent developments in 3D computer vision—in particular, reconstruction from moving cameras. He shared methods for SLAM that can accurately localize the camera and recover the observed 3D world.</p><p>Big thanks to our partners at München Computer Vision &amp; Medical Image Analysis meetup group.</p><p>Learn more about Level 5 and view our openings at <a href="https://www.lyft.com/self-driving-vehicles/engineers">lyft.com/level5</a>. To hear about our future events, follow <a href="https://medium.com/u/d6f431a02b8c">Lyft Level 5</a> on Twitter.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8e7872f08747" width="1" height="1"> <p><b><a href="https://medium.com/@LyftLevel5/hundreds-of-computer-vision-researchers-gathered-in-munich-to-talk-self-driving-8e7872f08747?source=rss-d6f431a02b8c------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3272</wp:post_id>
		<wp:post_date><![CDATA[2018-04-05 21:51:16]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-04-05 21:51:16]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[hundreds-of-computer-vision-researchers-gathered-in-munich-to-talk-self-driving]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@LyftLevel5/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@LyftLevel5/hundreds-of-computer-vision-researchers-gathered-in-munich-to-talk-self-driving-8e7872f08747?source=rss-d6f431a02b8c------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Google I/O Recap: Turning self-driving cars from science fiction into reality with the help of AI</title>
		<link>https://fifthlevel.ai/archives/254</link>
		<pubDate>Tue, 08 May 2018 15:52:17 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/89dded40c63</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>By: Dmitri Dolgov, CTO and VP of Engineering</em></p><p>This morning, we took the stage at <a href="https://events.google.com/io/">Google I/O</a>, Google’s annual developer conference, to share details on how Waymo is using artificial intelligence (AI) to make fully self-driving cars a reality. If you’re familiar with our work, you know that AI and machine learning (ML) have played a critical role in moving us closer to our goal of bringing self-driving technology to everyone, everywhere.</p><p>Here’s a quick recap of what we shared during this morning’s keynote.</p><p><strong>Jump-start in AI:</strong> Many people know about Google’s early AI advances in image search and speech recognition. But did you know that Google’s AI researchers also helped give Waymo a jump-start on the road to truly self-driving cars? One example: as deep learning began to take off, our self-driving engineers worked side-by-side with the Google Brain team to apply deep nets to our pedestrian detection system. Even in those early days, the <a href="https://ai.google/research/pubs/pub43850">results were remarkable</a> — within a matter of months, we were able to reduce the error rate for pedestrian detection by 100x, making our system safer and more capable on the road.</p><p><strong>Fast forward to fully self-driving: </strong>Fast forward to 2018, Waymo’s advances in AI have helped turn self-driving cars from science fiction into reality. Today, Waymo is the only company in the world with a fleet of <a href="https://medium.com/waymo/with-waymo-in-the-drivers-seat-fully-self-driving-vehicles-can-transform-the-way-we-get-around-75e9622e829a">truly autonomous cars</a> on public roads. Members of the public in Phoenix, Arizona have already started to experience these fully self-driving rides.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FQqRMTWqhwzM%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DQqRMTWqhwzM&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FQqRMTWqhwzM%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/2f7906cef5f129ffa9440bd14da374f3/href">https://medium.com/media/2f7906cef5f129ffa9440bd14da374f3/href</a></iframe><p><strong>AI everywhere:</strong> AI plays a crucial role in nearly every part of our self-driving system. While perception is the most mature area for deep learning, we also use deep nets for everything from prediction to planning to mapping and simulation. With machine learning, we can navigate nuanced and difficult situations; maneuvering construction zones, yielding to emergency vehicles, and giving room to cars that are parallel parking. We can do this because we’ve trained our ML models using lots of different examples. To date, we’ve driven 6 million miles on public roads and observed hundreds of millions of interactions between vehicles, pedestrians and cyclists.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*i-gVOW20qfJqNkXe_ba1fw.png" /><figcaption>We use ML in nearly every part of our self-driving system, including perception, prediction, planning, and mapping. Here, we’re using ML to accurately detect pedestrians, even when they’re dressed in dinosaur costumes!</figcaption></figure><p><strong>Beyond the algorithm: </strong>It takes more than good algorithms to be able to put self-driving vehicles on the road and expand to more cities. Infrastructure plays a key role in training and testing our ML models. At Waymo, we use the TensorFlow ecosystem and Google’s data centers — including TPUs — to train our neural networks. With TPUs, we can train our nets up to 15x more efficiently. We also rigorously test our ML models in <a href="https://www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulation-facilities/537648/">simulation</a>, where we drive the equivalent of 25,000 cars all day, every day. With this robust training and testing cycle, we can rapidly improve our ML models, and quickly deploy the latest nets on our self-driving cars.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*f_ua3gCH6A4LtTHwrhFwXQ.png" /><figcaption><em>With simulation, we can rapidly test and validate our ML models, allowing us to quickly iterate and accelerate our software development process.</em></figcaption></figure><p><strong>Weatherproofing our driver:</strong> We aim to bring self-driving technology to everyone, everywhere… <em>and</em> in all weather. Driving in heavy rain or snow can be a tough task for self-driving cars and people alike, in part because visibility is limited. Raindrops and snowflakes can create a lot of noise in sensor data for a self-driving car. Machine learning helps us filter out that noise and correctly identify pedestrians, vehicles and more.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*Un2g-aLSvti8b6GdYcR18A.gif" /><figcaption><em>In this example, we’re filtering out noise in our sensors using ML, which helps our cars clearly see the road ahead.</em></figcaption></figure><p>With our years of experience, collaboration with Google AI, and powerful infrastructure, we’re getting closer than ever to a future where transportation is safer, easier and more accessible for everyone.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=89dded40c63" width="1" height="1"><hr><p><a href="https://medium.com/waymo/google-i-o-recap-turning-self-driving-cars-from-science-fiction-into-reality-with-the-help-of-ai-89dded40c63">Google I/O Recap: Turning self-driving cars from science fiction into reality with the help of AI</a> was originally published in <a href="https://medium.com/waymo">Waymo</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>254</wp:post_id>
		<wp:post_date><![CDATA[2018-05-08 15:52:17]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-08 15:52:17]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[google-i-o-recap-turning-self-driving-cars-from-science-fiction-into-reality-with-the-help-of-ai]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/waymo]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/waymo/google-i-o-recap-turning-self-driving-cars-from-science-fiction-into-reality-with-the-help-of-ai-89dded40c63?source=rss----7075a35566d9---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[832]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Returning to Detroit to Power Up Our Electrified, Self-Driving Future</title>
		<link>https://fifthlevel.ai/archives/397</link>
		<pubDate>Thu, 24 May 2018 14:01:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/fd7fb7ab7dd8</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Sherif Marakby, Ford Vice President, Autonomous Vehicles and Electrification</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*baayR3RrHEUYoO_aHKGEuQ.jpeg" /><figcaption>Employees gather for a quick huddle at Ford’s new office space in Detroit’s Corktown district.</figcaption></figure><p>All over the world, you hear cities are changing rapidly. Here in Detroit, that’s irrefutable. We’re getting to see it firsthand, with dramatic changes happening right before our eyes.</p><p>From a recently developed riverfront to urban farming efforts and new business opportunities, it would be a massive understatement to say the Motor City has been through a lot of changes in recent years. Detroit has seen more than $13 billion in new investments since 2006, according to the Detroit Regional Chamber of Commerce, and its residents are seizing the opportunity to make their city a better place.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DEv8nI8ieBxAZNxYswuyog.jpeg" /><figcaption>Ford’s Pei-Wen Hsu inside Ford’s new office space in Corktown, highlighted by large windows and open floor space.</figcaption></figure><p>You can count Ford in that group, too. Just like Detroit, the auto industry is experiencing a massive shift — paving the way for a future of electric and self-driving vehicles. Electric vehicles will play a crucial role in the way we move around in the future, easing pollution in busy cities and offering new capabilities — like an onboard electric generator, or greater torque for better performance. Likewise, self-driving vehicles present opportunities to reduce congestion, improve delivery services and expand transportation access.</p><p>The magnitude of these changes means we’re doing everything we can to ensure we’re meeting customer needs. That’s why we’re returning to Detroit ourselves, moving our electric vehicle team– Ford Team Edison — and our self-driving vehicle business team into a historic former factory in Corktown.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/838/1*8OdeNY36GeMuBgqmGopdKw.jpeg" /><figcaption>Ford’s new office space features an open floor plan and is located in a high-traffic urban district.</figcaption></figure><p>Ford is striving to deploy electric and self-driving vehicles in a way that enhances life in urban areas, so it makes sense for our teams doing the work to be located in that same environment. The daily experiences of these groups will help them develop an intimate understanding of how these vehicles will need to operate in cities, what infrastructure is necessary to support them, and ways to effectively manage curb space. Corktown is the perfect place for us to gain critical insight into these areas and kick our efforts into high gear.</p><p>Our plan entails preparing for the future by ensuring we create electrified vehicles that truly answer people’s needs and deliver what they want, starting with a Mustang-inspired battery-electric SUV launching in 2020. Our autonomous vehicle business team will be operating right alongside Ford Team Edison, working hard to deliver our first self-driving vehicles as hybrid-electric vehicles in 2021.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dunBmdxmRo3b2NBx1V-ZCA.jpeg" /><figcaption>Ford’s new office space on Michigan Avenue in Detroit’s Corktown district.</figcaption></figure><p>The move is an opportunity for us to think differently, too, by pioneering new ways of working for Ford. With a new layout plus flexible office furniture enabling employees to customize their workspace as needed, our new Corktown offices offer an environment that will foster increased collaboration within and between teams — and will serve as a test case for a work system that can be brought back to company facilities around the world.</p><p>That’s a really important part of this whole move, as it’s not just about one or two teams relocating to Detroit — it’s about how we’re going to work closely and collaborate with teams around the world going forward.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sdgJ4c6byL2UKqB4z9EsLw.jpeg" /><figcaption>The view from the top of Ford’s new office space in Corktown, looking west down Michigan Avenue, just a short drive from Ford’s World Headquarters in Dearborn.</figcaption></figure><p>But don’t take my word for it — our excitement is best described by some of our employees now working in Detroit:</p><blockquote>“Expanding our presence from Dearborn into the heart of downtown Detroit, while bringing with us the enthusiasm and excitement we share for the new technologies we’re building is incredibly rewarding. By channeling collaboration, energy and unity of purpose — the building’s unique redesign is pushing us even further to elevate our game and the innovative products we’re creating. It’s a very exciting move and I couldn’t be more proud to be back in the neighborhood.” — <strong>Thomas Walsh, Ford Autonomous Vehicle Partnerships Lead</strong></blockquote><blockquote>“As someone who’s been with Ford for almost 27 years, returning to Detroit brings back a lot of emotions. I was with the company when it was still in downtown, saw it leave in the mid-’90s, and now we’re coming back! It’s great we’re refurbishing a historic building, too, as it’s important we preserve the city’s history even as we prepare it for the future. I’m looking forward to exploring and experiencing Detroit in a more meaningful way and I know the positive vibe will be a contributing factor in the work of my team — being closer to customers is what it’s all about.” — <strong>Samantha Hoyt, Ford Manager, Cross-Vehicle Marketing and Strategy</strong></blockquote><blockquote>“I’ve spent a lot of time in Corktown on my own and with friends — I’m even thinking about making the move to Detroit myself — and it’s inspiring to see how proud everyone who’s working in the neighborhood is about what they’re doing. Now I get to be a part of that and work in a space that lets me more easily tackle issues face-to-face with my colleagues. To be part of the rebuilding process, to be able to say I work in Detroit gives me a sense of pride, too. I know the urban energy will be inspiring.” — <strong>Jarret Zablocki, Ford Hybrid and Plug-In Hybrid Strategist</strong></blockquote><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fd7fb7ab7dd8" width="1" height="1"> <p><a href="https://medium.com/@ford/returning-to-detroit-to-power-up-our-electrified-self-driving-future-fd7fb7ab7dd8?source=rss-db92c082f24a------2" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>397</wp:post_id>
		<wp:post_date><![CDATA[2018-05-24 14:01:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-24 14:01:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[returning-to-detroit-to-power-up-our-electrified-self-driving-future]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@ford]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@ford/returning-to-detroit-to-power-up-our-electrified-self-driving-future-fd7fb7ab7dd8?source=rss-db92c082f24a------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[837]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>More Than Just Miles: Our Self-Driving Vehicle Testing Helps Steer Miami Students Toward a Future…</title>
		<link>https://fifthlevel.ai/archives/437</link>
		<pubDate>Fri, 20 Apr 2018 17:31:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/692fb8b5ae4f</guid>
		<description></description>
		<content:encoded><![CDATA[<h3><strong>More Than Just Miles: Our Self-Driving Vehicle Testing Helps Steer Miami Students Toward a Future of Robotics</strong></h3><h4>By Peter Rander, President, Argo AI</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Zl7TpgwVn5IxUTxj3EG6zQ.jpeg" /><figcaption>Joel Cruz, a high school student from Miami, and Peter Rander, President, Argo AI.</figcaption></figure><p>After a year spent testing self-driving vehicles in Pittsburgh and Southeast Michigan — not to mention enduring some cold winter weather — the Argo AI fleet is enjoying a dramatic change of scenery. Among palm trees and ocean views, we’ve expanded our testing with Ford Motor Company into Miami-Dade County, with our self-driving vehicles now mapping roads and accumulating valuable test miles on the streets of Miami and Miami Beach. As we further increase our areas of testing throughout the year, we will continue to add vehicles to our autonomous vehicle fleet.</p><p>We look forward to working with the local government supporting our efforts, including Miami-Dade Mayor Carlos Giménez, who has been a staunch advocate of innovative ideas and self-driving technology. At the same time, we thought it was important to share our news more directly with other parts of the community in South Florida. And who better to engage with than a group that shares our passion for robotics?</p><p>That’s why we spent an afternoon with a group involved in FIRST Robotics — students, their mentors and teachers. We’ve been supportive of FIRST Robotics in Pittsburgh, and through our partnership with Ford and its involvement with the program, we’ve connected with multiple school teams to reveal that our latest robots — our self-driving vehicles — are now being tested in and around their own neighborhoods.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uj-UUcrlh5sRMgDb7BdAkQ.jpeg" /><figcaption>An Argo AI test vehicle in Miami.</figcaption></figure><p>While it was great sharing our news with members of the media, the most thrilled reactions came from these students. It’s a testament to how excited they are about this technology that when a test vehicle arrived at the scene promising a spontaneous pizza party, they didn’t even budge — all these students wanted to talk about was how the car’s various sensors worked and how deliveries could be handled in the future.</p><p>After that, it was my turn to be impressed by their amazing accomplishments. One of the coolest things I saw from these students was a demo of a robot that quickly picked up and stacked small traffic cones. As you watch a robot move and try to perform its intended function, what really hits you is just how many questions these students first had to ask themselves and solve for. There’s no cookie-cutter solution to picking up and stacking anything, so you gain a true appreciation for the fact that they clearly had to entertain numerous options before determining what their approach to building a robot that could efficiently pick up and stack cones would be.</p><p>Upon hearing that we would be visiting, some of these students even developed their own self-driving car. Within a few days, they had inserted a Raspberry Pi circuit board into a remote control car to program it, then added a couple of ultrasonic sensors to enable the test car to detect obstacles in its path. After some testing and tuning, they had developed a self-driving car that would stop when it found something in its path and then steer out of the way of the obstacle.</p><p>I was pleasantly surprised to see how excited these students, their teachers, coaches and mentors actually were about robotics, and my time with them was truly heartening. Back when I was in high school, I barely even knew what a robot was except that it was science fiction — a cool, futuristic idea that hinted at a fascinating future. Nowadays, students are putting together makeshift robots in a couple of days and diving headfirst into the field to solve interesting challenges on their own.</p><p>That’s a far cry from even 10 years ago, when it was impressive just to see a robot follow a line painted on the floor. Seeing how passionate these young men and women are reminds me that part of our mission at Argo AI is to inspire others — whether they’re students in robotics or interested members of the communities in which we operate — to learn more about how automation can improve their lives. That these students managed to put together their own driver-less car is motivating, even to those in the field like me.</p><p>Similarly, I can only hope that seeing Argo AI’s test vehicles now driving themselves on local streets will serve as an inspiration for students in Florida wondering what they have to offer the world.</p><p><em>Special thanks to Sandra Contreras, regional director of </em><a href="https://firstinflorida.org/"><em>FIRST in South Florida</em></a><em>, for helping coordinate our visit with the student teams. And thanks to Hector Escobedo, engineering teacher and FIRST mentor, and to the staff at </em><a href="http://terrawolves.com/"><em>TERRA Environmental Research Institute</em></a><em> for hosting the visit at their amazing high school.</em></p><p><em>And of course, best of luck to all the student teams that joined us that day: RamTech 59 from Miami Coral Park Senior High School, Team LIFE from Terra Environmental Research Institute and The Ninjineers from American Heritage School.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=692fb8b5ae4f" width="1" height="1"> <p><a href="https://medium.com/@ArgoAI/more-than-just-miles-our-self-driving-vehicle-testing-helps-steer-miami-students-toward-a-future-692fb8b5ae4f?source=rss-ba51b2aa6fa1------2" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>437</wp:post_id>
		<wp:post_date><![CDATA[2018-04-20 17:31:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-04-20 17:31:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[more-than-just-miles-our-self-driving-vehicle-testing-helps-steer-miami-students-toward-a-future]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@ArgoAI]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@ArgoAI/more-than-just-miles-our-self-driving-vehicle-testing-helps-steer-miami-students-toward-a-future-692fb8b5ae4f?source=rss-ba51b2aa6fa1------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[830]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Ride Along with Drive.ai’s Self-Driving Car in Texas At Dawn</title>
		<link>https://fifthlevel.ai/archives/520</link>
		<pubDate>Wed, 16 May 2018 15:01:46 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/5c3cdbc3e7e7</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Last week, Drive.ai <a href="https://medium.com/@drive.ai/self-driving-cars-are-here-drive-ai-is-launching-in-texas-e095668854ae">announced</a> that we will be launching an on-demand self-driving car service in Frisco, Texas later this year in July. While our service will initially launch with safety drivers, we are excited to share today a new video demonstrating the technical capabilities of our self-driving system to operate without a driver behind the wheel. In this uncut video, one of our cars drives through private and public roads, encountering many of the real-world scenarios that they will see on our initial route.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fmj4QLsemKEY%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dmj4QLsemKEY&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fmj4QLsemKEY%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/cdf995db0d1b93af560a8e3658e2a851/href">https://medium.com/media/cdf995db0d1b93af560a8e3658e2a851/href</a></iframe><p>The video also features an augmented reality visualization in the bottom right corner, showcasing how our perception system uses sensor data to accurately identify objects such as cars, pedestrians, and cyclists — all at different distances and speeds. Using this data, we are able to plan our course (visualized as a “red carpet”) in a safe manner while maximizing rider comfort.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/497/1*54r4-IcRpYrxXOaiMt5p0Q.gif" /></figure><p>We are excited to bring our self-driving technology to Texas and we look forward to sharing more details with you as we get closer to launch!</p><p>A couple of highlights from the video:</p><ul><li>0:37 As the car proceeds to drive on public roads by crossing a 6-lane intersection, our system looks both ways for a long distance to understand what is happening and how to proceed.</li><li>1:19 Cyclist does a double-take on seeing no one in the driver’s seat, yet still offers a friendly wave!</li><li>1:53 We encounter a roundabout — this often requires the car to negotiate traffic merging from multiple points.</li><li>3:11 Low-angle sunlight can make it difficult to see. Our system fuses the inputs from multiple sensors to ensure accurate detection and tracking.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5c3cdbc3e7e7" width="1" height="1"> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>520</wp:post_id>
		<wp:post_date><![CDATA[2018-05-16 15:01:46]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-16 15:01:46]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[ride-along-with-drive-ais-self-driving-car-in-texas-at-dawn]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@drive.ai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@drive.ai/ride-along-with-drive-ais-self-driving-car-in-texas-at-dawn-5c3cdbc3e7e7?source=rss-37851f177ff8------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[834]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>AECC Board Members Meet to Discuss Strategic Roadmap</title>
		<link>https://fifthlevel.ai/archives/549</link>
		<pubDate>Wed, 25 Apr 2018 19:34:43 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://aecc.org/?p=289</guid>
		<description></description>
		<content:encoded><![CDATA[<p>On April 25, 2018, the Board of Directors and key members of the Automotive Edge Computing Consortium met in Tokyo, Japan at the Intel offices to discuss a number of key initiatives. Member organization representation included Toyota, Intel, Ericsson, KDDI, DENSO, Dell, AT&T, Toyota ITC, NTT DOCOMO, Sumitomo, and Nippon Telegraph. Discussions ranged from improving user experience, to a long-term strategic roadmap. Our next all member meeting will be in June in Tokyo, Japan.</p>
<p>The post <a rel="nofollow" href="https://aecc.org/aecc-board-members-meet-to-discuss-strategic-roadmap/">AECC Board Members Meet to Discuss Strategic Roadmap</a> appeared first on <a rel="nofollow" href="https://aecc.org">Automotive Edge Computing Consortium</a>.</p> <p><b><a href="https://aecc.org/aecc-board-members-meet-to-discuss-strategic-roadmap/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>549</wp:post_id>
		<wp:post_date><![CDATA[2018-04-25 19:34:43]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-04-25 19:34:43]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[aecc-board-members-meet-to-discuss-strategic-roadmap]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://aecc.org/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://aecc.org/aecc-board-members-meet-to-discuss-strategic-roadmap/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Waymo Self-Driving Minivan Involved In Arizona Crash</title>
		<link>https://fifthlevel.ai/archives/639</link>
		<pubDate>Fri, 04 May 2018 22:45:16 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/241216/waymo-autonomous-minivan-crash-arizona/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[The Waymo van wasn't at fault, according to police. <p><b><a href="https://www.motor1.com/news/241216/waymo-autonomous-minivan-crash-arizona/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>639</wp:post_id>
		<wp:post_date><![CDATA[2018-05-04 22:45:16]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-04 22:45:16]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[waymo-self-driving-minivan-involved-in-arizona-crash]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/videos/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/241216/waymo-autonomous-minivan-crash-arizona/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple&#039;s self-driving shuttle project to use Volkswagen vans after fruitless negotiations with BMW, others [u]</title>
		<link>https://fifthlevel.ai/archives/710</link>
		<pubDate>Wed, 23 May 2018 22:43:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/05/23/apples-self-driving-shuttle-project-to-use-volkswagen-vans-report-says</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/26159-36778-180523-Transporter-l.jpg" alt="Article Image" border="0" /> <br><br> After rebuffed attempts to partner with major carmakers over an ambitious electric vehicle project, Apple has reportedly inked a deal with Volkswagen to convert a number of T6 Transporter vans into self-driving shuttles for company employees. <p><b><a href="https://appleinsider.com/articles/18/05/23/apples-self-driving-shuttle-project-to-use-volkswagen-vans-report-says" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>710</wp:post_id>
		<wp:post_date><![CDATA[2018-05-23 22:43:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-23 22:43:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apples-self-driving-shuttle-project-to-use-volkswagen-vans-after-fruitless-negotiations-with-bmw-others-u]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/05/23/apples-self-driving-shuttle-project-to-use-volkswagen-vans-report-says]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[836]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>&#039;Apple Glasses&#039; AR headset could launch in 2021, says Gene Munster</title>
		<link>https://fifthlevel.ai/archives/711</link>
		<pubDate>Thu, 17 May 2018 17:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/05/17/apple-glasses-ar-headset-could-launch-in-2021-says-gene-munster</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/26084-36585-25814-35731-25305-34172-24101-31203-magicleapsuite-l-l-l-l.jpg" alt="Article Image" border="0" /> <br><br> Apple could release a highly rumored augmented reality wearable in 2021, according to a note from Loup Ventures analyst Gene Munster discussing product categories the iPhone producer could pursue, with the rumored 'Apple Glasses' predicted to sell over 10 million units in its first year. <p><b><a href="https://appleinsider.com/articles/18/05/17/apple-glasses-ar-headset-could-launch-in-2021-says-gene-munster" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>711</wp:post_id>
		<wp:post_date><![CDATA[2018-05-17 17:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-17 17:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-glasses-ar-headset-could-launch-in-2021-says-gene-munster]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/05/17/apple-glasses-ar-headset-could-launch-in-2021-says-gene-munster]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[835]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple grows self-driving test fleet to 55 vehicles as project remains in shadows</title>
		<link>https://fifthlevel.ai/archives/712</link>
		<pubDate>Mon, 14 May 2018 20:06:20 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/05/14/apple-grows-self-driving-test-fleet-to-55-vehicles-as-project-remains-in-shadows</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/26035-36466-applecar-testbed2-l.jpg" alt="Article Image" border="0" /> <br><br> Apple now has 55 autonomous test vehicles and 83 authorized drivers, California's Department of Motor Vehicles has confirmed, indicating that work on the company's mysterious car platform is accelerating. <p><b><a href="https://appleinsider.com/articles/18/05/14/apple-grows-self-driving-test-fleet-to-55-vehicles-as-project-remains-in-shadows" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>712</wp:post_id>
		<wp:post_date><![CDATA[2018-05-14 20:06:20]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-14 20:06:20]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-grows-self-driving-test-fleet-to-55-vehicles-as-project-remains-in-shadows]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/05/14/apple-grows-self-driving-test-fleet-to-55-vehicles-as-project-remains-in-shadows]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[833]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple meets with California DMV officials to discuss autonomous vehicles</title>
		<link>https://fifthlevel.ai/archives/723</link>
		<pubDate>Wed, 02 May 2018 00:57:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/05/01/apple-meets-with-california-dmv-officials-to-discuss-autonomous-vehicles</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/25883-36001-21053-23609-170421-Automated-l-l.jpg" alt="Article Image" border="0" /> <br><br> A handful of high-ranking California Department of Motor Vehicles officials met with Apple employees in April to discuss facets of the company's ongoing autonomous car program, according to a report on Tuesday. <p><b><a href="https://appleinsider.com/articles/18/05/01/apple-meets-with-california-dmv-officials-to-discuss-autonomous-vehicles" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>723</wp:post_id>
		<wp:post_date><![CDATA[2018-05-02 00:57:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-02 00:57:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-meets-with-california-dmv-officials-to-discuss-autonomous-vehicles]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/05/01/apple-meets-with-california-dmv-officials-to-discuss-autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[831]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Fleet of autonomous boats could service some cities, reducing road traffic</title>
		<link>https://fifthlevel.ai/archives/1129</link>
		<pubDate>Wed, 23 May 2018 18:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/fleet-autonomous-boats-service-cities-reducing-road-traffic-0523</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The future of transportation in waterway-rich cities such as Amsterdam, Bangkok, and Venice — where canals run alongside and under bustling streets and bridges — may include autonomous boats that ferry goods and people, helping clear up road congestion.</p> <p>Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Senseable City Lab in the Department of Urban Studies and Planning (DUSP), have taken a step toward that future by designing a fleet of autonomous boats that offer high maneuverability and precise control. The boats can also be rapidly 3-D printed using a low-cost printer, making mass manufacturing more feasible.</p> <p>The boats could be used to taxi people around and to deliver goods, easing street traffic. In the future, the researchers also envision the driverless boats being adapted to perform city services overnight, instead of during busy daylight hours, further reducing congestion on both roads and canals.</p> <p>“Imagine shifting some of infrastructure services that usually take place during the day on the road — deliveries, garbage management, waste management — to the middle of the night, on the water, using a fleet of autonomous boats,” says CSAIL Director Daniela Rus, co-author on a paper describing the technology that’s being presented at this week’s IEEE International Conference on Robotics and Automation.</p> <p>Moreover, the boats — rectangular 4-by-2-meter hulls equipped with sensors, microcontrollers, GPS modules, and other hardware — could be programmed to self-assemble into floating bridges, concert stages, platforms for food markets, and other structures in a matter of hours. “Again, some of the activities that are usually taking place on land, and that cause disturbance in how the city moves, can be done on a temporary basis on the water,” says Rus, who is the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science.</p> <p>The boats could also be equipped with environmental sensors to monitor a city’s waters and gain insight into urban and human health.</p> <p>Co-authors on the paper are: first author Wei Wang, a joint postdoc in CSAIL and the Senseable City Lab; Luis A. Mateos and Shinkyu Park, both DUSP postdocs; Pietro Leoni, a research fellow, and Fábio Duarte, a research scientist, both in DUSP and the Senseable City Lab; Banti Gheneti, a graduate student in the Department of Electrical Engineering and Computer Science; and Carlo Ratti, a principal investigator and professor of the practice in the DUSP and director of the MIT Senseable City Lab.</p> <div class="cms-placeholder-content-video"></div> <p><strong>Better design and control</strong></p> <p>The work was conducted as part of the “<a href="http://senseable.mit.edu/roboat/">Roboat</a>” project, a collaboration between the MIT Senseable City Lab and the Amsterdam Institute for Advanced Metropolitan Solutions (AMS). In 2016, as part of the project, the researchers <a href="http://news.mit.edu/2016/autonomous-fleet-amsterdam-roboat-0919">tested</a> a prototype that cruised around the city’s canals, moving forward, backward, and laterally along a preprogrammed path.</p> <p>The ICRA paper details several important new innovations: a rapid fabrication technique, a more efficient and agile design, and advanced trajectory-tracking algorithms that improve control, precision docking and latching, and other tasks.&nbsp;</p> <p>To make the boats, the researchers 3-D-printed a rectangular hull with a commercial printer, producing 16 separate sections that were spliced together. Printing took around 60 hours. The completed hull was then sealed by adhering several layers of fiberglass.</p> <p>Integrated onto the hull are a power supply, Wi-Fi antenna, GPS, and a minicomputer and microcontroller. For precise positioning, the researchers incorporated an indoor ultrasound beacon system and outdoor real-time kinematic GPS modules, which allow for centimeter-level localization, as well as an inertial measurement unit (IMU) module that monitors the boat’s yaw and angular velocity, among other metrics.</p> <p>The boat is a rectangular shape, instead of the traditional kayak or catamaran shapes, to allow the vessel to move sideways and to attach itself to other boats when assembling other structures. Another simple yet effective design element was thruster placement. Four thrusters are positioned in the center of each side, instead of at the four corners, generating forward and backward forces. This makes the boat more agile and efficient, the researchers say.</p> <p>The team also developed a method that enables the boat to track its position and orientation more quickly and accurately. To do so, they developed an efficient version of a nonlinear model predictive control (NMPC) algorithm, generally used to control and navigate robots within various constraints.</p> <p>The NMPC and similar algorithms have been used to control autonomous boats before. But typically those algorithms are tested only in simulation or don’t account for the dynamics of the boat. The researchers instead incorporated in the algorithm simplified nonlinear mathematical models that account for a few known parameters, such as drag of the boat, centrifugal and Coriolis forces, and added mass due to accelerating or decelerating in water. The researchers also used an identification algorithm that then identifies any unknown parameters as the boat is trained on a path.</p> <p>Finally, the researchers used an efficient predictive-control platform to run their algorithm, which can rapidly determine upcoming actions and increases the algorithm’s speed by two orders of magnitude over similar systems. While other algorithms execute in about 100 milliseconds, the researchers’ algorithm takes less than 1 millisecond.</p> <p><strong>Testing the waters</strong></p> <p>To demonstrate the control algorithm’s efficacy, the researchers deployed a smaller prototype of the boat along preplanned paths in a swimming pool and in the Charles River. Over the course of 10 test runs, the researchers observed average tracking errors — in positioning and orientation —&nbsp;smaller than tracking errors of traditional control algorithms.</p> <p>That accuracy is thanks, in part, to the boat’s onboard GPS and IMU modules, which determine position and direction, respectively, down to the centimeter. The NMPC algorithm crunches the data from those modules and weighs various metrics to steer the boat true. The algorithm is implemented in a controller computer and regulates each thruster individually, updating every 0.2 seconds.</p> <p>“The controller considers the boat dynamics, current state of the boat, thrust constraints, and reference position for the coming several seconds, to optimize how the boat drives on the path,” Wang says. “We can then find optimal force for the thrusters that can take the boat back to the path and minimize errors.”</p> <p>The innovations in design and fabrication, as well as faster and more precise control algorithms, point toward feasible driverless boats used for transportation, docking, and self-assembling into platforms, the researchers say.</p> <p>“Having swarms of robots in the canals of Amsterdam is a great idea,” says Javier Alonso-Mora, an assistant professor in the Cognitive Robotics Department at Delft University of Technology in the Netherlands, who was not involved in the research. “Twenty percent of the surface in the Netherlands is water, and robots can be an efficient mode of transportation and logistics. This is a first step in that direction, with a very nice prototype that is able to move in all directions and connect with other boats to build temporal structures. Together with the team at MIT we are now looking at the next steps in autonomy, including multirobot coordination and urban navigation.”</p> <p>A next step for the work is developing adaptive controllers to account for changes in mass and drag of the boat when transporting people and goods. The researchers are also refining the controller to account for wave disturbances and stronger currents.</p> <p>“We actually found that the Charles River has much more current than in the canals in Amsterdam,” Wang says. “But there will be a lot of boats moving around, and big boats will bring big currents, so we still have to consider this.”</p> <p>The work was supported by a grant from AMS.</p>
<p><b><a href="http://news.mit.edu/2018/fleet-autonomous-boats-service-cities-reducing-road-traffic-0523" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1129</wp:post_id>
		<wp:post_date><![CDATA[2018-05-23 18:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-23 18:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[fleet-of-autonomous-boats-could-service-some-cities-reducing-road-traffic]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/fleet-autonomous-boats-service-cities-reducing-road-traffic-0523]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Making driverless cars change lanes more like human drivers do</title>
		<link>https://fifthlevel.ai/archives/1130</link>
		<pubDate>Wed, 23 May 2018 03:59:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/driverless-cars-change-lanes-like-human-drivers-0523</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In the field of self-driving cars, algorithms for controlling lane changes are an important topic of study. But most existing lane-change algorithms have one of two drawbacks: Either they rely on detailed statistical models of the driving environment, which are difficult to assemble and too complex to analyze on the fly; or they’re so simple that they can lead to impractically conservative decisions, such as never changing lanes at all.</p> <p>At the International Conference on Robotics and Automation tomorrow, researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) will present a new lane-change algorithm that splits the difference. It allows for more aggressive lane changes than the simple models do but relies only on immediate information about other vehicles’ directions and velocities to make decisions.</p> <p>“The motivation is, ‘What can we do with as little information as possible?’” says Alyssa Pierson, a postdoc at CSAIL and first author on the new paper. “How can we have an autonomous vehicle behave as a human driver might behave? What is the minimum amount of information the car needs to elicit that human-like behavior?”</p> <p>Pierson is joined on the paper by Daniela Rus, the Viterbi Professor of Electrical Engineering and Computer Science; Sertac Karaman, associate professor of aeronautics and astronautics; and Wilko Schwarting, a graduate student in electrical engineering and computer science.</p> <p>“The optimization solution will ensure navigation with lane changes that can model an entire range of driving styles, from conservative to aggressive, with safety guarantees,” says Rus, who is the director of CSAIL.</p> <p>One standard way for autonomous vehicles to avoid collisions is to calculate buffer zones around the other vehicles in the environment. The buffer zones describe not only the vehicles’ current positions but their likely future positions within some time frame. Planning lane changes then becomes a matter of simply staying out of other vehicles’ buffer zones.</p> <p>For any given method of computing buffer zones, algorithm designers must prove that it guarantees collision avoidance, within the context of the mathematical model used to describe traffic patterns. That proof can be complex, so the optimal buffer zones are usually computed in advance. During operation, the autonomous vehicle then calls up the precomputed buffer zones that correspond to its situation.</p> <p>The problem is that if traffic is fast enough and dense enough, precomputed buffer zones may be too restrictive. An autonomous vehicle will fail to change lanes at all, whereas a human driver would cheerfully zip around the roadway.</p> <p>With the MIT researchers’ system, if the default buffer zones are leading to performance that’s far worse than a human driver’s, the system will compute new buffer zones on the fly — complete with proof of collision avoidance.</p> <p>That approach depends on a mathematically efficient method of describing buffer zones, so that the collision-avoidance proof can be executed quickly. And that’s what the MIT researchers developed.</p> <p>They begin with a so-called Gaussian distribution — the familiar bell-curve probability distribution. That distribution represents the current position of the car, factoring in both its length and the uncertainty of its location estimation.</p> <p>Then, based on estimates of the car’s direction and velocity, the researchers’ system constructs a so-called logistic function. Multiplying the logistic function by the Gaussian distribution skews the distribution in the direction of the car’s movement, with higher speeds increasing the skew.</p> <p>The skewed distribution defines the vehicle’s new buffer zone. But its mathematical description is so simple — using only a few equation variables — that the system can evaluate it on the fly.</p> <p>The researchers tested their algorithm in a simulation including up to 16 autonomous cars driving in an environment with several hundred other vehicles.</p> <p>“The autonomous vehicles were not in direct communication but ran the proposed algorithm in parallel without conflict or collisions,” explains Pierson.&nbsp;“Each car used a different risk threshold that produced a different driving style, allowing us to create conservative and aggressive drivers. Using the static, precomputed buffer zones would only allow for conservative driving, whereas our dynamic algorithm allows for a broader range of driving styles.”</p> <p>This project was supported, in part, by the Toyota Research Institute and the Office of Naval Research.</p>
<p><b><a href="http://news.mit.edu/2018/driverless-cars-change-lanes-like-human-drivers-0523" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1130</wp:post_id>
		<wp:post_date><![CDATA[2018-05-23 03:59:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-23 03:59:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[making-driverless-cars-change-lanes-more-like-human-drivers-do]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/driverless-cars-change-lanes-like-human-drivers-0523]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Albatross robot takes flight</title>
		<link>https://fifthlevel.ai/archives/1131</link>
		<pubDate>Fri, 18 May 2018 04:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/albatross-robot-takes-flight-0518</guid>
		<description></description>
		<content:encoded><![CDATA[<p>MIT engineers have designed a robotic glider that can skim along the water’s surface, riding the wind like an albatross while also surfing the waves like a sailboat.</p> <p>In regions of high wind, the robot is designed to stay aloft, much like its avian counterpart. Where there are calmer winds, the robot can dip a keel into the water to ride like a highly efficient sailboat instead.</p> <p>The robotic system, which borrows from both nautical and biological designs, can cover a given distance using one-third as much wind as an albatross and traveling 10 times faster than a typical sailboat. The glider is also relatively lightweight, weighing about 6 pounds. The researchers hope that in the near future, such compact, speedy robotic water-skimmers may be deployed in teams to survey large swaths of the ocean.</p> <p>“The oceans remain vastly undermonitored,” says Gabriel Bousquet, a former postdoc in MIT’s Department of Aeronautics and Astronautics, who led the design of the robot as part of his graduate thesis. “In particular, it’s very important to understand the Southern Ocean and how it is interacting with climate change. But it’s very hard to get there. We can now use the energy from the environment in an efficient way to do this long-distance travel, with a system that remains small-scale.”</p> <p>Bousquet will present details of the robotic system this week at IEEE’s International Conference on Robotics and Automation, in Brisbane, Australia. His collaborators on the project are Jean-Jacques Slotine, professor of mechanical engineering and information sciences and of brain sciences; and Michael Triantafyllou, the Henry L. and Grace Doherty Professor in Ocean Science and Engineering.</p> <div class="cms-placeholder-content-video"></div> <p><strong>The physics of speed</strong></p> <p>Last year, Bousquet, Slotine, and Triantafyllou published a study on <a href="http://news.mit.edu/2017/engineers-identify-key-albatross-marathon-flight-1011">the dynamics of albatross flight</a>, in which they identified the mechanics that enable the tireless traveler to cover vast distances while expending minimal energy. The key to the bird’s marathon voyages is its ability to ride in and out of high- and low-speed layers of air.</p> <p>Specifically, the researchers found the bird is able to perform a mechanical process called a “transfer of momentum,” in which it takes momentum from higher, faster layers of air, and by diving down transfers that momentum to lower, slower layers, propelling itself without having to continuously flap its wings.</p> <p>Interestingly, Bousquet observed that the physics of albatross flight is very similar to that of sailboat travel. Both the albatross and the sailboat transfer momentum in order to keep moving. But in the case of the sailboat, that transfer occurs not between layers of air, but between the air and water.</p> <p>“Sailboats take momentum from the wind with their sail, and inject it into the water by pushing back with their keel,” Bousquet explains. “That’s how energy is extracted for sailboats.”</p> <p><img alt="" src="/sites/mit.edu.newsoffice/files/albatross-robot.gif" /></p> <p class="caption"><span style="font-size:10px;">An albatross glider, designed by MIT engineers, skims the Charles River.</span></p> <p>Bousquet also realized that the speed at which both an albatross and a sailboat can travel depends upon the same general equation, related to the transfer of momentum. Essentially, both the bird and the boat can travel faster if they can either stay aloft easily or interact with two layers, or mediums, of very different speeds.</p> <p>The albatross does well with the former, as its wings provide natural lift, though it flies between air layers with a relatively small difference in windspeeds. Meanwhile, the sailboat excels at the latter, traveling between two mediums of very different speeds — air versus water — though its hull creates a lot of friction and prevents it from getting much speed.&nbsp; Bousquet wondered: What if a vehicle could be designed to perform well in both metrics, marrying the high-speed qualities of both the albatross and the sailboat?</p> <p>“We thought, how could we take the best from both worlds?” Bousquet says.</p> <p><strong>Out on the water</strong></p> <p>The team drafted a design for such a hybrid vehicle, which ultimately resembled an autonomous glider with a 3-meter wingspan, similar to that of a typical albatross. They added a tall, triangular sail, as well as a slender, wing-like keel. They then performed some mathematical modeling to predict how such a design would travel.</p> <p>According to their calculations, the wind-powered vehicle would only need relatively calm winds of about 5 knots to zip across waters at a velocity of about 20 knots, or 23 miles per hour.</p> <p>“We found that in light winds you can travel about three to 10 times faster than a traditional sailboat, and you need about half as much wind as an albatross, to reach 20 knots,” Bousquet says. “It’s very efficient, and you can travel very fast, even if there is not too much wind.”</p> <p>The team built a prototype of their design, using a glider airframe designed by Mark Drela, professor of aeronautics and astronautics at MIT. To the bottom of the glider they added a keel, along with various instruments, such as GPS, inertial measurement sensors, auto-pilot instrumentation, and ultrasound, to track the height of the glider above the water.</p> <p>“The goal here was to show we can control very precisely how high we are above the water, and that we can have the robot fly above the water, then down to where the keel can go under the water to generate a force, and the plane can still fly,” Bousquet says.</p> <p>The researchers decided to test this “critical maneuver” — the act of transitioning between flying in the air and dipping the keel down to sail in the water. Accomplishing this move doesn’t necessarily require a sail, so Bousquet and his colleagues decided not to include one in order to simplify preliminary experiments.</p> <p>In the fall of 2016, the team put its design to the test, launching the robot from the MIT Sailing Pavilion out onto the Charles River. As the robot lacked a sail and any mechanism to get it started, the team hung it from a fishing rod attached to a whaler boat. With this setup, the boat towed the robot along the river until it reached about 20 miles per hour, at which point the robot autonomously “took off,” riding the wind on its own.</p> <p>Once it was flying autonomously, Bousquet used a remote control to give the robot a “down” command, prompting it to dip low enough to submerge its keel in the river. Next, he adjusted the direction of the keel, and observed that the robot was able to steer away from the boat as expected. He then gave a command for the robot to fly back up, lifting the keel out of the water.</p> <p>“We were flying very close to the surface, and there was very little margin for error — everything had to be in place,” Bousquet says. “So it was very high stress, but very exciting.”</p> <p>The experiments, he says, prove that the team’s conceptual device can travel successfully, powered by the wind and the water. Eventually, he envisions fleets of such vehicles autonomously and efficiently monitoring large expanses of the ocean.</p> <p>“Imagine you could fly like an albatross when it’s really windy, and then when there’s not enough wind, the keel allows you to sail like a sailboat,” Bousquet says. “This dramatically expands the kinds of regions where you can go.”</p> <p>This research was supported, in part, by the Link Ocean Instrumentation fellowship.</p>
<p><b><a href="http://news.mit.edu/2018/albatross-robot-takes-flight-0518" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1131</wp:post_id>
		<wp:post_date><![CDATA[2018-05-18 04:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-18 04:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[albatross-robot-takes-flight]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/albatross-robot-takes-flight-0518]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Researchers develop virtual-reality testing ground for drones</title>
		<link>https://fifthlevel.ai/archives/1132</link>
		<pubDate>Thu, 17 May 2018 04:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/virtual-reality-testing-ground-drones-0517</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Training drones to fly fast, around even the simplest obstacles, is a crash-prone exercise that can have engineers repairing or replacing vehicles with frustrating regularity.</p> <p>Now MIT engineers have developed a new virtual-reality training system for drones that enables a vehicle to “see” a rich, virtual environment while flying in an empty physical space.</p> <p>The system, which the team has dubbed “Flight Goggles,” could significantly reduce the number of crashes that drones experience in actual training sessions. It can also serve as a virtual testbed for any number of environments and conditions in which researchers might want to train fast-flying drones.</p> <p>“We think this is a game-changer in the development of drone technology, for drones that go fast,” says Sertac Karaman, associate professor of aeronautics and astronautics at MIT. “If anything, the system can make autonomous vehicles more responsive, faster, and more efficient.”</p> <p>Karaman and his colleagues will present details of their virtual training system at the IEEE International Conference on Robotics and Automation next week. Co-authors include Thomas Sayre-McCord, Winter Guerra, Amado Antonini, Jasper Arneberg, Austin Brown, Guilherme Cavalheiro, Dave McCoy, Sebastian Quilter, Fabian Riether, Ezra Tal, Yunus Terzioglu, and Luca Carlone of MIT’s Laboratory for Information and Decision Systems, along with Yajun Fang of MIT’s Computer Science and Artificial Intelligence Laboratory, and Alex Gorodetsky of Sandia National Laboratories.</p> <div class="cms-placeholder-content-video"></div> <p><strong>Pushing boundaries</strong></p> <p>Karaman was initially motivated by a new, extreme robo-sport: competitive drone racing, in which remote-controlled drones, driven by human players, attempt to out-fly each other through an intricate maze of windows, doors, and other obstacles. Karaman wondered: Could an autonomous drone be trained to fly just as fast, if not faster, than these human-controlled vehicles, with even better precision and control?</p> <p>“In the next two or three years, we want to enter a drone racing competition with an autonomous drone, and beat the best human player,” Karaman says. To do so, the team would have to develop an entirely new training regimen.</p> <p>Currently, training autonomous drones is a physical task: Researchers fly drones in large, enclosed testing grounds, in which they often hang large nets to catch any careening vehicles. They also set up props, such as windows and doors, through which a drone can learn to fly. When vehicles crash, they must be repaired or replaced, which delays development and adds to a project’s cost.</p> <p>Karaman says testing drones in this way can work for vehicles that are not meant to fly fast, such as drones that are programmed to slowly map their surroundings. But for fast-flying vehicles that need to process visual information quickly as they fly through an environment, a new training system is necessary.</p> <p>“The moment you want to do high-throughput computing and go fast, even the slightest changes you make to its environment will cause the drone to crash,” Karaman says. “You can’t learn in that environment. If you want to push boundaries on how fast you can go and compute, you need some sort of virtual-reality environment.”</p> <p><strong>Flight Goggles</strong></p> <p>The team’s new virtual training system comprises a motion capture system, an image rendering program, and electronics that enable the team to quickly process images and transmit them to the drone.</p> <p>The actual test space — a hangar-like gymnasium in MIT’s new <a href="http://news.mit.edu/2017/building-31-robots-drones-0925">drone-testing facility in Building 31</a> — is lined with motion-capture cameras that track the orientation of the drone as it’s flying.</p> <p>With the image-rendering system, Karaman and his colleagues can draw up photorealistic scenes, such as a loft apartment or a living room, and beam these virtual images to the drone as it’s flying through the empty facility. &nbsp;&nbsp;&nbsp;</p> <p>“The drone will be flying in an empty room, but will be ‘hallucinating’ a completely different environment, and will learn in that environment,” Karaman explains.</p> <p>The virtual images can be processed by the drone at a rate of about 90 frames per second — around three times as fast as the human eye can see and process images. To enable this, the team custom-built circuit boards that integrate a powerful embedded supercomputer, along with an inertial measurement unit and a camera. They fit all this hardware into a small, 3-D-printed nylon and carbon-fiber-reinforced drone frame.&nbsp;</p> <p><strong>A crash course</strong></p> <p>The researchers carried out a set of experiments, including one in which the drone learned to fly through a virtual window about twice its size. The window was set within a virtual living room. As the drone flew in the actual, empty testing facility, the researchers beamed images of the living room scene, from the drone’s perspective, back to the vehicle. As the drone flew through this virtual room, the researchers tuned a navigation algorithm, enabling the drone to learn on the fly.</p> <p>Over 10 flights, the drone, flying at around 2.3 meters per second (5 miles per hour), successfully flew through the virtual window 361 times, only “crashing” into the window three times, according to positioning information provided by the facility’s motion-capture cameras. Karaman points out that, even if the drone crashed thousands of times, it wouldn’t make much of an impact on the cost or time of development, as it’s crashing in a virtual environment and not making any physical contact with the real world.</p> <p>In a final test, the team set up an actual window in the test facility, and turned on the drone’s onboard camera to enable it to see and process its actual surroundings. Using the navigation algorithm that the researchers tuned in the virtual system, the drone, over eight flights, was able to fly through the real window 119 times, only crashing or requiring human intervention six times.</p> <p>“It does the same thing in reality,” Karaman says. “It’s something we programmed it to do in the virtual environment, by making mistakes, falling apart, and learning. But we didn’t break any actual windows in this process.”</p> <p>He says the virtual training system is highly malleable. For instance, researchers can pipe in their own scenes or layouts in which to train drones, including detailed, drone-mapped replicas of actual buildings — something the team is considering doing with MIT’s Stata Center. The training system may also be used to test out new sensors, or specifications for existing sensors, to see how they might handle on a fast-flying drone.</p> <p>“We could try different specs in this virtual environment and say, ‘If you build a sensor with these specs, how would it help a drone in this environment?’’ Karaman says.</p> <p>The system can also be used to train drones to fly safely around humans. For instance, Karaman envisions splitting the actual test facility in two, with a drone flying in one half, while a human, wearing a motion-capture suit, walks in the other half. The drone would “see” the human in virtual reality as it flies around its own space. If it crashes into the person, the result is virtual, and harmless.</p> <p>“One day, when you’re really confident, you can do it in reality, and have a drone flying around a person as they’re running, in a safe way,” Karaman says. “There are a lot of mind-bending experiments you can do in this whole virtual reality thing. Over time, we will showcase all the things you can do.”</p> <p>This research was supported, in part, by U.S. Office of Naval Research, MIT Lincoln Laboratory, and the NVIDIA Corporation.</p>
<p><b><a href="http://news.mit.edu/2018/virtual-reality-testing-ground-drones-0517" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1132</wp:post_id>
		<wp:post_date><![CDATA[2018-05-17 04:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-17 04:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[researchers-develop-virtual-reality-testing-ground-for-drones]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/virtual-reality-testing-ground-drones-0517]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Fundamental equations guide marine robots to optimal sampling sites</title>
		<link>https://fifthlevel.ai/archives/1133</link>
		<pubDate>Thu, 10 May 2018 03:59:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/fundamental-equations-guide-marine-robots-optimal-sampling-sites-0510</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Observing the world’s oceans is increasingly a mission assigned to autonomous underwater vehicles (AUVs) — marine robots that are designed to drift, drive, or glide through the ocean without any real-time input from human operators. Critical questions that AUVs can help to answer are where, when, and what to sample for the most informative data, and how to optimally reach sampling locations.</p> <p>MIT engineers have now developed systems of mathematical equations that forecast the most informative data to collect for a given observing mission, and the best way to reach the sampling sites.</p> <p>With their method, the researchers can predict the degree to which one variable, such as the speed of ocean currents at a certain location, reveals information about some other variable, such as temperature at some other location — a quantity called “mutual information.” If the degree of mutual information between two variables is high, an AUV can be programmed to go to certain locations to measure one variable, to gain information about the other. &nbsp;</p> <p>The team used their equations and an ocean model they developed, called &nbsp;Multidisciplinary Simulation, Estimation, and Assimilation Systems (MSEAS), in sea experiments to successfully forecast fields of mutual information and guide actual AUVs.</p> <p>“Not all data are equal,” says Arkopal Dutt, a graduate student in MIT’s Department of Mechanical Engineering. “Our criteria … allow the autonomous machines to pinpoint sensor locations and sampling times where the most informative measurements can be made.”</p> <p>To determine how to safely and efficiently reach ideal sampling destinations, the researchers developed a way to help AUVs use the uncertain ocean’s activity, by forecasting out a “reachability front” — a dynamic three-dimensional region of the ocean that an AUV would be guaranteed to reach within a certain time, given the AUV’s power constraints and the ocean’s currents. The team’s method enables a vehicle to surf currents that would bring it closer to its destination, and avoid those that would throw it off track.</p> <p>When the researchers compared their reachability forecasts with the routes of actual AUVs observing a region of the Arabian Sea, they found their predictions matched where the vehicles were able to navigate, over long periods of time.</p> <p>Ultimately, the team’s methods should help vehicles explore the ocean in an intelligent, energy-efficient manner.</p> <p>“Autonomous marine robots are our scouts, braving the rough seas to collect data for us,” says mechanical engineering graduate student Deepak Subramani. “Our math equations help the scouts reach the desired locations and reduce their energy usage by intelligently using the ocean currents.”</p> <p>The researchers, led by Pierre Lermusiaux, professor of mechanical engineering and ocean science and engineering at MIT, have laid out their results in a paper soon to appear in a volume of the book series, “The Sea,” published by the <em>Journal of Marine Research.</em></p> <p>In addition to Dutt and Subramani, Lermusiaux’s team includes Jing Lin, Chinmay Kulkarni, Abhinav Gupta, Tapovan Lolla, Patrick Haley, Wael Hajj Ali, Chris Mirabito, and Sudip Jana, all from the Department of Mechanical Engineering.</p> <p><strong>Quest for the most informative data </strong></p> <p>To validate their approach, the researchers showed that they could successfully predict the measurements that were the most informative for a varied set of goals. For example, they forecast the observations that were optimal for testing scientific hypotheses, learning if the ocean model equations themselves are correct or not, estimating parameters of marine ecosystems, and detecting the presence of coherent structures in the ocean. They confirmed that their optimal observations were 50 to 150 percent more informative than an average observation.</p> <p>To reach the optimal observing locations, AUVs must navigate through the ocean. Traditionally, planning paths for robots has been done in relatively static environments. But planning through the ocean is a different story, as strong currents and eddies can constantly change, be uncertain, and push a vehicle off its preplanned course.</p> <p>The MIT team thus developed path-planning algorithms from fundamental principles with the ocean in mind. They modified an existing equation, known as the Hamilton-Jacobi equation, to determine an AUV’s reachability front, or the furthest perimeter a vehicle is guaranteed to reach in a given amount of time. The equation is based on three main variables: time, a vehicle’s specific propulsion constraints, and advection, or the transport by the dynamic ocean currents — a variable which the group predicts by using its MSEAS ocean model.</p> <p>With the new system, the AUVs can map out the feasible most informative paths and adapt their sampling plans as the uncertain ocean’s currents shift over time. In a first large, open-ocean test, the team calculated probabilistic reachability fronts and the most informative paths for autonomous floats and gliders in the Indian Ocean, as part of the <a href="http://mseas.mit.edu/Research/NASCar-OPS/index.html">Northern Arabian Sea Circulation-autonomous research (NASCar) initiative</a> of the Office of Naval Research (ONR).</p> <p>Over several months, the researchers, working out of their MIT offices, provided daily reachability forecasts to the ONR team to help guide the underwater vehicles, collecting optimal observations along the way.</p> <p>“It was basically not much sleeping,” Lermusiaux recalls. “The forecasts were three to seven days out, and we would assimilate data and update every day. We did quite well. On average, the gliders and floats ended up where desired and within the probabilistic areas that we predicted.”</p> <p><strong>A moment of truth pays off</strong></p> <p>Lermusiaux and his colleagues also utilized their systems to plan “time-optimal paths” — trajectories that would get an AUV to a certain location in the shortest amount of time, given the forecast ocean current conditions.</p> <p>With colleagues from the MIT Lincoln Laboratory and Woods Hole Oceanographic Institution, they tested these time-optimal paths in real time by holding “races” between identical propelled AUVs, off the coast of Martha’s Vineyard. In each race, one AUV’s course was determined by the team’s time-optimal path, while another AUV followed a path with the shortest distance to the same destination.</p> <p>“It was tense — who will win?” Subramani recalls. “This was the moment of truth for us, after all those years of theoretical development with math equations and proofs.”</p> <p>The team’s work paid off. In every race, the AUV operating under the team’s forecast reached its destination first, performing about 15 percent faster than the competing AUV. The team’s forecast helped the winning AUV to avoid strong currents that at times acted to block the other AUV.</p> <p>“It was amazing,” Kulkarni says. “Even though physically the two paths were only less than a mile apart, following our predictions gave up to a 15 percent reduction in travel times. It shows our paths are truly time optimal.”</p> <p>Among other applications, Lermusiaux, as a member of MIT’s Tata Center for Technology and Design, will be applying his ocean forecasting methods to help guide observations off the coast of India, where the vehicles will be tasked with monitoring fisheries to provide a potentially low-cost management system.</p> <p>“AUVs are not very fast, and their autonomy is not infinite, so you really have to take into account the currents and their uncertainties, and model things rigorously,” Lermusiaux says. “Machine intelligence for these autonomous systems comes from rigorously deriving and merging governing differential equations and principles with control theory, information theory, and machine learning.”</p> <p>This research was funded, in part, by the Office of Naval Research, the MIT Lincoln Laboratory, the MIT Tata Center, and the National Science Foundation.</p>
<p><b><a href="http://news.mit.edu/2018/fundamental-equations-guide-marine-robots-optimal-sampling-sites-0510" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1133</wp:post_id>
		<wp:post_date><![CDATA[2018-05-10 03:59:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-10 03:59:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[fundamental-equations-guide-marine-robots-to-optimal-sampling-sites]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/fundamental-equations-guide-marine-robots-optimal-sampling-sites-0510]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Self-driving cars for country roads</title>
		<link>https://fifthlevel.ai/archives/1134</link>
		<pubDate>Mon, 07 May 2018 04:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/self-driving-cars-for-country-roads-mit-csail-0507</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Navigating roads less traveled in self-driving cars is a difficult task. One reason is that there aren’t many places where self-driving cars can actually drive. Companies like Google only test their fleets <a href="https://avsincities.bloomberg.org/" target="_blank">in major cities</a> where they’ve spent countless hours meticulously labeling the exact 3-D positions of lanes, curbs, off-ramps, and stop signs.</p> <p>“The cars use these maps to know where they are and what to do in the presence of new obstacles like pedestrians and other cars,” says Daniela Rus, director of MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL). “The need for dense 3-D maps limits the places where self-driving cars can operate.”</p> <p>Indeed, if you live along the <a href="http://tradingeconomics.com/united-states/roads-paved-percent-of-total-roads-wb-data.html" target="_blank">millions of miles of U.S. roads</a> that are unpaved, unlit, or unreliably marked, you’re out of luck. Such streets are often much more complicated to map, and get a lot less traffic, so companies aren’t incentivized to develop 3-D maps for them anytime soon. From California’s Mojave Desert to Vermont’s White Mountains, there are huge swaths of America that self-driving cars simply aren’t ready for.</p> <p>One way around this is to create systems advanced enough to navigate without these maps. In an important first step, Rus and colleagues at CSAIL have developed <a href="https://toyota.csail.mit.edu/sites/default/files/documents/papers/ICRA2018_AutonomousVehicleNavigationRuralEnvironment.pdf" target="_blank">MapLite</a>, a framework that allows self-driving cars to drive on roads they’ve never been on before without 3-D maps.</p> <div class="cms-placeholder-content-video"></div> <p>MapLite combines simple GPS data that you’d find on Google Maps with a series of sensors that observe the road conditions. In tandem, these two elements allowed the team to autonomously drive on multiple unpaved country roads in Devens, Massachusetts, and reliably detect the road more than 100 feet in advance. (As part of a collaboration with the Toyota Research Institute, researchers used a Toyota Prius that they outfitted with a range of LIDAR and IMU sensors.)</p> <p>“The reason this kind of ‘map-less’ approach hasn’t really been done before is because it is generally much harder to reach the same accuracy and reliability as with detailed maps,” says CSAIL graduate student Teddy Ort, who was a lead author on a related paper about the system. “A system like this that can navigate just with on-board sensors shows the potential of self-driving cars being able to actually handle roads beyond the small number that tech companies have mapped.”</p> <p>The paper, which will be presented in May at the International Conference on Robotics and Automation (ICRA) in Brisbane, Australia, was co-written by Ort, Rus, and PhD graduate Liam Paull, who is now an assistant professor at the University of Montreal.</p> <p>For all the progress that has been made with self-driving cars, their navigation skills still pale in comparison to humans’. Consider how you yourself get around: If you’re trying to get to a specific location, you probably plug an address into your phone and then consult it occasionally along the way, like when you approach intersections or highway exits.</p> <p>However, if you were to move through the world like most self-driving cars, you’d essentially be staring at your phone the whole time you’re walking. Existing systems still rely heavily on maps, only using sensors and vision algorithms to avoid dynamic objects like pedestrians and other cars.</p> <p>In contrast, MapLite uses sensors for all aspects of navigation, relying on GPS data only to obtain a rough estimate of the car’s location. The system first sets both a final destination and what researchers call a “local navigation goal,” which has to be within view of the car. Its perception sensors then generate a path to get to that point, using LIDAR to estimate the location of the road’s edges. MapLite can do this without physical road markings by making basic assumptions about how the road will be relatively more flat than the surrounding areas.</p> <p>“Our minimalist approach to mapping enables autonomous driving on country roads using local appearance and semantic features such as the presence of a parking spot or a side road,” says Rus.</p> <p>The team developed a system of models that are “parameterized,” which means that they describe multiple situations that are somewhat similar. For example, one model might be broad enough to determine what to do at intersections, or what to do on a specific type of road.</p> <p>MapLite differs from other map-less driving approaches that rely more on machine learning by training on data from one set of roads and then being tested on other ones.</p> <p>“At the end of the day we want to be able to ask the car questions like ‘how many roads are merging at this intersection?’” says Ort. “By using modeling techniques, if the system doesn’t work or is involved in an accident, we can better understand why.”</p> <p>MapLite still has some limitations. For example, it isn’t yet reliable enough for mountain roads, since it doesn’t account for dramatic changes in elevation. As a next step, the team hopes to expand the variety of roads that the vehicle can handle. Ultimately they aspire to have their system reach comparable levels of performance and reliability as mapped systems but with a much wider range.</p> <p>“I imagine that the self-driving cars of the future will always make some use of 3-D maps in urban areas,” says Ort. “But when called upon to take a trip off the beaten path, these vehicles will need to be as good as humans at driving on unfamiliar roads they have never seen before. We hope our work is a step in that direction.”</p> <p>This project was supported, in part, by the National Science Foundation and the Toyota Research Initiative.</p> <p><b><a href="http://news.mit.edu/2018/self-driving-cars-for-country-roads-mit-csail-0507" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1134</wp:post_id>
		<wp:post_date><![CDATA[2018-05-07 04:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-07 04:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[self-driving-cars-for-country-roads]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/self-driving-cars-for-country-roads-mit-csail-0507]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>18-Month Ban For Leaving Driver Seat Vacant While On Tesla Autopilot</title>
		<link>https://fifthlevel.ai/archives/1152</link>
		<pubDate>Wed, 02 May 2018 17:00:02 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/240936/18-month-ban-vacant-driver-seat/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[There apparently are some individuals in this world who don’t care much for their life or the lives of others and, unfortunately, some of those individuals are driving on roads. <p><b><a href="https://www.motor1.com/news/240936/18-month-ban-vacant-driver-seat/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1152</wp:post_id>
		<wp:post_date><![CDATA[2018-05-02 17:00:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-02 17:00:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[18-month-ban-for-leaving-driver-seat-vacant-while-on-tesla-autopilot]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/240936/18-month-ban-vacant-driver-seat/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Self-Driving Cars Are Here: Drive.ai is Launching in Texas</title>
		<link>https://fifthlevel.ai/archives/1338</link>
		<pubDate>Mon, 07 May 2018 18:02:19 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/e095668854ae</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bZlC6c4jtMyJUDQ_." /></figure><p>Drive.ai began in 2015 to transform the relationship between people and transportation. Since then, we’ve built a full stack self-driving system from the ground up, with cutting-edge deep learning technology at its core. Now, it’s time to put our technology on the road and add real value to peoples’ lives with self-driving vehicles.</p><p><strong>Howdy, Frisco</strong></p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FYJe8ELIU7_E%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYJe8ELIU7_E&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FYJe8ELIU7_E%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/037b132424715a150fb156844d0d61bb/href">https://medium.com/media/037b132424715a150fb156844d0d61bb/href</a></iframe><p>Starting in July 2018, we’re launching an on-demand self-driving car service in Frisco, Texas. This on-demand self-driving program is the first of its kind in Texas and solves the ‘last mile’ transit problem: moving between areas that are too far to walk but too close to drive (and often too crowded to find parking).</p><p>Using our app, members of the Frisco community will be able to call Drive.ai’s self-driving vehicles on-demand. When the pilot launches, our vehicles will drive on public roads between HALL Park offices and The Star, connecting busy offices with popular retail, dining, and entertainment options. At launch, the service will be administered by the Denton County Transportation Authority and operated in conjunction with <a href="http://www.hallgroup.com/">The Hall Group</a>, <a href="http://friscostation.com/">Frisco Station Partners</a>, and <a href="https://www.thestarinfrisco.com/">The Star</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/501/0*3El6mTbaXOppZRPR." /></figure><p>In our initial rides, passengers will be joined by a safety driver, closely monitoring the vehicles’ operation. We plan to soon remove the safety driver in favor of a chaperone in the passenger seat, who will educating riders about the self-driving experience and attend to the vehicle. And eventually, we will offer passenger-only rides. Throughout every phase of the program, our self-driving vehicles will be connected to our tele-choice technology, providing the ability to call to a remote operator should extra assistance be needed.</p><p><strong>Three Key Elements for the Public Deployment of Self-driving Cars</strong></p><p>Frisco, Texas is one of the <a href="http://money.cnn.com/gallery/real_estate/2017/06/02/fastest-growing-cities-census/2.html">fastest growing cities</a> in the country. Self-driving can benefit this city <em>today</em> by connecting residents and local businesses without increasing traffic. But it’s essential that public self-driving programs are deployed in a safe and thoughtful way.</p><p>Together with our partners, we’ve focused on three key elements to bring a positive self-driving experience to the City of Frisco:</p><h3><strong>1. Technology: industry-leading AI and deep learning</strong></h3><p>Drive.ai was founded by a group of AI engineers, many coming directly out of Andrew Ng’s research group Stanford University’s Artificial Intelligence Lab. From day one, we recognized that getting self-driving vehicles on public roads would require deep learning being built into a full stack system.</p><p>That’s why we built our entire self-driving system from the ground up: the mapping, perception, motion planning, localization, fleet management, tele-choice, communications, and mobile app. Deep learning is built into every aspect of Drive.ai.</p><p>Our deep learning-first approach enables us to push self-driving forward across a range of geographies and vehicles, solving the biggest mobility issues that cities and their residents face today.</p><h3><strong>2. Partnerships: deploying with public and private partners</strong></h3><p>As our team — and the self-driving industry — has grown in the past couple of years, we’ve thought carefully about where Drive.ai might expand. Since day one, we have been committed to working with businesses and policymakers to make self-driving vehicles a reality, safely and quickly. The ability to collaborate with local stakeholders is a huge consideration.</p><p>That’s why we’re excited about partnering with the Frisco Transportation Management Association for this deployment. Local partners best understand the transportation challenges within their community, and where self-driving programs can add the most value. By working with public and private partners, we can get residents to the places they want to go while being respectful of local roadwork, special events, and emergency response protocol.</p><h3><strong>3. Safety: people-centric safety</strong></h3><p>Self-driving vehicles are a new technology with unique strengths and limitations. We’ve thought carefully about how to introduce a safe, smooth experience for riders, pedestrians, and other drivers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/820/0*01SZ8YsP0oSll5KN." /></figure><p>The bright orange Drive.ai vehicles are clearly labeled “Self-Driving Vehicle” for easy recognition. We developed distinctive color schemes, unique signage, and exterior communication panels that effectively communicate and convey our vehicles’ intended actions with pedestrians and other drivers on the roads.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/410/0*D5uAeB2s0N4LZUPK." /></figure><p>We’ve also limited our driving service to a geo-fenced area so that all vehicle routes can be carefully mapped and predictably managed by our locally-based expert team. As we work towards launching the service in Frisco, we are excited to engage with and educate the public about our service and self-driving vehicles, through programs we craft with our partners.</p><h3><strong>This Is Big</strong></h3><p>Today’s announcement is bigger than just Frisco. It’s about moving the transportation industry forward, and deploying a Level 4 self-driving system to improve the state of mobility <em>today.</em></p><p>You can stay up-to-date on all our progress by visiting the brand-new <a href="http://drive.ai">drive.ai website</a>. You can also learn more about our Frisco program by visiting RideFriscoAV.com</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e095668854ae" width="1" height="1"> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1338</wp:post_id>
		<wp:post_date><![CDATA[2018-05-07 18:02:19]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-07 18:02:19]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[self-driving-cars-are-here-drive-ai-is-launching-in-texas]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@drive.ai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@drive.ai/self-driving-cars-are-here-drive-ai-is-launching-in-texas-e095668854ae?source=rss-37851f177ff8------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Uber shuts down self-driving operation in Arizona after fatal crash</title>
		<link>https://fifthlevel.ai/archives/2501</link>
		<pubDate>Wed, 23 May 2018 19:38:45 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.theguardian.com/technology/2018/may/23/uber-shuts-down-self-driving-operation-in-arizona-two-months-after-fatal-crash</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The company will focus research efforts on Pittsburgh and continue to test in San Francisco</p><p>Uber is to shut down its self-driving car programme in Arizona after one of its cars <a href="https://arstechnica.com/cars/2018/03/uber-self-driving-car-hits-and-kills-pedestrian/">killed a pedestrian</a> there in March.</p><p> <span>Related: </span><a href="https://www.theguardian.com/technology/2018/mar/19/uber-self-driving-car-kills-woman-arizona-tempe">Self-driving Uber kills Arizona woman in first fatal crash involving pedestrian</a> </p> <a href="https://www.theguardian.com/technology/2018/may/23/uber-shuts-down-self-driving-operation-in-arizona-two-months-after-fatal-crash">Continue reading...</a> <p><b><a href="https://www.theguardian.com/technology/2018/may/23/uber-shuts-down-self-driving-operation-in-arizona-two-months-after-fatal-crash" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2501</wp:post_id>
		<wp:post_date><![CDATA[2018-05-23 19:38:45]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-23 19:38:45]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[uber-shuts-down-self-driving-operation-in-arizona-after-fatal-crash]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/self-driving-cars/rss]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/2018/may/23/uber-shuts-down-self-driving-operation-in-arizona-two-months-after-fatal-crash]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Cognata Appoints New VP of Sales Alon Podhurst</title>
		<link>https://fifthlevel.ai/archives/3006</link>
		<pubDate>Wed, 25 Apr 2018 21:26:24 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.cognata.com/?p=498</guid>
		<description></description>
		<content:encoded><![CDATA[<p class="responsiveNews">Cognata Ltd. today announced that Alon Podhurst has joined the company as Vice President of Sales. In this position, Alon is responsible for the commercial leadership of Cognata, developing new business opportunities with key players within the autonomous vehicle ecosystem and creating customized solutions to meet customer needs.</p>
<p class="responsiveNews">Prior to joining Cognata, Alon served as Vice President of Sales at Windward, a maritime data and analytics company where he led the company’s sales team, developed new business opportunities and built key relationships with executive decision-makers. Prior to that Alon served as Vice President of Sales at Ecoppia, the world’s leading provider of autonomous, water-free cleaning technology for utility-scale solar installations. Alon held executive sales positions at Telmap, an Intel company that developed mobile location-based services, where he oversaw sales for Europe, the Middle East and Africa, and with Comverse Technologies, Inc.</p>
<p class="responsiveNews">“Alon’s established record of facilitating long-term and fruitful business relationships with industry leaders and rapidly building and scaling commercial teams is a natural fit,” said Danny Atsmon, CEO of Cognata. “We are excited to be growing our executive team to meet customer demand, as Cognata rapidly expands its commercial partnerships with major players in the autonomous vehicle field.”</p>
<p class="responsiveNews">Cognata is transforming the world of autonomous vehicles, providing companies with simulated road tests to enable them to get self-driving vehicles to market faster &#8212; and safely. The company combines artificial intelligence, deep learning and computer vision in a simulation platform to enable autonomous vehicle developers to shave years off the costly process of road-testing. Cognata can recreate cities anywhere in the world, allowing a dramatically expanded range of testing scenarios beyond the current limited geographies, to the great benefit of any OEM and autonomous vehicle manufacturer.</p>
<p class="responsiveNews">“Joining the Cognata team as autonomous vehicles are rapidly becoming a reality all over the world provides me with the opportunity to make a lasting impact on this new industry and help get autonomous vehicles on the road faster,” said Alon Podhurst, Vice President of Sales of Cognata.</p>
<p>&nbsp;</p>
<p><img data-attachment-id="493" data-permalink="http://www.cognata.com/home/cognata-team-240x240_alon-1/" data-orig-file="https://i2.wp.com/www.cognata.com/wp-content/uploads/2018/04/cognata-team-240X240_alon-1.jpg?fit=240%2C240" data-orig-size="240,240" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="cognata-team-240X240_alon 1" data-image-description="" data-medium-file="https://i2.wp.com/www.cognata.com/wp-content/uploads/2018/04/cognata-team-240X240_alon-1.jpg?fit=240%2C240" data-large-file="https://i2.wp.com/www.cognata.com/wp-content/uploads/2018/04/cognata-team-240X240_alon-1.jpg?fit=240%2C240" class="size-full wp-image-493 alignnone" src="https://i2.wp.com/www.cognata.com/wp-content/uploads/2018/04/cognata-team-240X240_alon-1.jpg?resize=240%2C240" alt="" width="240" height="240" srcset="https://i2.wp.com/www.cognata.com/wp-content/uploads/2018/04/cognata-team-240X240_alon-1.jpg?w=240 240w, https://i2.wp.com/www.cognata.com/wp-content/uploads/2018/04/cognata-team-240X240_alon-1.jpg?resize=150%2C150 150w" sizes="(max-width: 240px) 100vw, 240px" data-recalc-dims="1" /></p>
<p class="responsiveNews">About Cognata<br />
Cognata provides a fast lane to autonomous driving with its testing and evaluation solution for self-driving vehicles—a realistic automotive simulation platform where virtual cars travel virtual roads in virtual cities, all remarkably true to real-world conditions. Led by CEO Danny Atsmon, a widely respected expert in ADAS and deep learning, Cognata brings the disruptive potential of artificial intelligence, deep learning, and computer vision to the autonomous driving simulation world. Cognata’s simulated testing and evaluation environment shaves years off the validation time by generating fast, highly accurate results, and eliminates the safety concerns, high costs, and limited scalability of road-testing in the physical world. Cognata was founded in 2016 by a team of experts in deep learning, autonomous vehicles and computer vision. The company is headquartered in Rehovot, Israel, close to the Weizmann Institute of Science. For more information, visit <a href="http://www.cognata.com/" rel="nofollow">http://www.cognata.com</a>.</p>
<p>The post <a rel="nofollow" href="http://www.cognata.com/cognata-appoints-new-vp-sales-alon-podhurst/">Cognata Appoints New VP of Sales Alon Podhurst</a> appeared first on <a rel="nofollow" href="http://www.cognata.com">Cognata Autonomous Simulation</a>.</p> <p><b><a href="http://www.cognata.com/cognata-appoints-new-vp-sales-alon-podhurst/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3006</wp:post_id>
		<wp:post_date><![CDATA[2018-04-25 21:26:24]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-04-25 21:26:24]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[cognata-appoints-new-vp-of-sales-alon-podhurst]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.cognata.com/blog/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.cognata.com/cognata-appoints-new-vp-sales-alon-podhurst/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Waymo’s early rider program, one year in</title>
		<link>https://fifthlevel.ai/archives/253</link>
		<pubDate>Wed, 13 Jun 2018 18:56:10 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/3a788f995a9c</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Ariel rides after school. Neha hops to the grocery store. Barbara and Jim zip around town while kicking back.</p><p>They’re all part of the Waymo early rider program we <a href="https://medium.com/waymo/apply-to-be-part-of-waymos-early-rider-program-5fd996c7a86f">launched last April</a>. Today, over 400 riders with diverse transportation needs use Waymo every day, at any time, to ride all around the Phoenix area. Their feedback helps us understand how fully self driving cars fit into their daily lives.</p><p>One year in, our early rider program and our extensive on-road testing is helping us build the world’s most experienced driver. In fact, our fleet of cars across the U.S. is now driving more than 24,000 miles daily; that’s the equivalent of an around the world road trip! Here’s a quick report on how our riders use Waymo, what we’ve learned, and what’s next.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F3HrN12WG-2Q%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D3HrN12WG-2Q&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F3HrN12WG-2Q%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/342a18b6a1ab4dce1739b034ee847832/href">https://medium.com/media/342a18b6a1ab4dce1739b034ee847832/href</a></iframe><p><strong>Where are we going?</strong></p><p>Our applicants come from all corners of the community: senior citizens, high school students, families with young children, people living with disabilities or without a driver’s license. All said they wanted to try truly self-driving technology, and many mentioned their hopes for a future of safer streets and roadways.</p><p>Our early riders go everywhere under the Arizona sun: on daily trips to work and school, on rides to doctors and dentists, on shopping trips to the local Chandler Fashion Center Mall. They hop in Waymo vehicles to visit restaurants and bars or catch a movie at the AMC Ahwatukee. Along the way they do homework, catch up on emails, read books, or even just daydream and check out the local scenery.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hljrx61g5fQWvM7ZmeuyFQ.jpeg" /></figure><p><strong>Who’s riding?</strong></p><p>The households who participate in our program have family members aged 9 to 69. Some don’t drive because they’re too young; others lack a license for medical reasons. One rider is a former Peace Corps volunteer who doesn’t own a car, and another bikes to work every day but needs Waymo for weekend errands.</p><p>Meet a few of our Waymo early riders:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RmXcLp8fdTUY3_jtb7N7PA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VXkFnsrchwhgkQbHZ55D7w.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QpU8HVINg_ZaMi1ec3lHTg.jpeg" /></figure><ol><li><em>Neha has avoided driving ever since witnessing a fatal car crash years ago. She says it’s “mind-blowing” that she can now take self-driving cars to run errands, get groceries, and commute back and forth to work.</em></li><li><em>Barbara says she’d do more knitting on her self-driving rides if she wasn’t so busy checking out the local sights with her husband Jim. Though she’s lived in the same neighborhood for 20 years, she missed a lot while she was focused on driving — even including a nearby park she never noticed before.</em></li><li><em>Chris, Marie, Miranda, and Ariel are a one-car family, so self-driving cars help the parents keep up with their kids’ busy lives. Their two teenagers often ride Waymo to their after-school activities.</em></li></ol><p><strong>What we’re learning</strong></p><p>As some of the first people in the world to use self-driving vehicles for their everyday transportation needs, our early riders are helping shape this technology. Thanks to their feedback, we’re refining the rider experience to make sure that:</p><p><strong>Riding from point A to point B is simple. </strong>Drives ought to feel natural, and pickups and dropoffs should feel intuitive. In Arizona’s sunny high temperatures, nobody wants to carry grocery bags a block down the street. We’re working to give our vehicles the smarts to arrive as close as possible to the best exit doors wherever our riders want to go.</p><p><strong>Contacting us is easy</strong>. We’re learning a lot about the questions passengers have for their driver when the driver is the car itself. We’re building systems to let riders ask questions or get assistance at any time with the tap of a button inside our vehicle or through our app. They contact us on a wide range of topics — everything from “how do I play my own music in your car” and “what route will we take to my destination” to “I left my sunglasses, how can I get them back” and “can I bring my service animal with me” (the answer: of course).</p><p><strong>Ride for every reason. </strong>Our early riders are a diverse group, from veteran commuters to middle-school students, to busy parents with small children. They help us understand all kinds of reasons people need rides … and even help us determine how our cars should gently wake up napping riders at their destination!</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FEi5tCVlFVlA%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DEi5tCVlFVlA&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FEi5tCVlFVlA%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/0b8f79779096dacb03fae9f956277778/href">https://medium.com/media/0b8f79779096dacb03fae9f956277778/href</a></iframe><p>Our early rider program <a href="https://waymo.com/apply/">continues to take applications</a>; if you live in the area, drop us a line. You might become the next Waymo early rider, helping to bring our self-driving service to even more people in the future.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3a788f995a9c" width="1" height="1"><hr><p><a href="https://medium.com/waymo/waymos-early-rider-program-one-year-in-3a788f995a9c">Waymo’s early rider program, one year in</a> was originally published in <a href="https://medium.com/waymo">Waymo</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>253</wp:post_id>
		<wp:post_date><![CDATA[2018-06-13 18:56:10]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-13 18:56:10]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[waymos-early-rider-program-one-year-in]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/waymo]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/waymo/waymos-early-rider-program-one-year-in-3a788f995a9c?source=rss----7075a35566d9---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[839]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Let’s Give Them Something to Taco ‘Bout: Enabling Self-Driving Food Delivery With Postmates</title>
		<link>https://fifthlevel.ai/archives/386</link>
		<pubDate>Mon, 11 Jun 2018 13:01:03 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/6d1358bdbf87</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Alexandra Ford English, Ford Autonomous Vehicle Business Team</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iZP9WA5VMAOst3AyWqfiQg.jpeg" /></figure><p>It’s a familiar feeling. The day inches towards dinner and your stomach makes a simple, direct plea to your heart: You want tacos. That hand pressed corn tortilla, the tangy salsa and fresh cilantro — you’re getting hungry already!</p><p>But what if ordering tacos did more than just satisfy the gremlin in your gut? What if it actually…shaped the future?</p><p><a href="https://medium.com/self-driven/why-teaming-with-postmates-will-help-ford-expand-on-demand-delivery-to-everyone-c11739e76ba4">Ford is working with Postmates</a>, an on-demand delivery platform, to operate a self-driving delivery service.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VEp6-NHBoGeGKgkbEG-l6A.jpeg" /></figure><p>Research vehicles for our business pilots are designed to appear as self-driving, however, they are manually driven by an experienced driver. The focus of our research is on the first and last mile of the delivery experience. We are <a href="https://medium.com/self-driven/were-going-to-miami-the-first-proving-ground-for-our-self-driving-service-6ea7721de0a5">developing our self-driving technology in separate test vehicles</a>.</p><p>Our Postmates pilot is currently underway in Miami and Miami Beach with more than 70 businesses participating, including local favorites like Coyo Taco. For residents in the area, when you order tacos — or almost anything, really — through Postmates, you may be given the option to have your items delivered by a self-driving research vehicle.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F9DY0LKOdE3M%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D9DY0LKOdE3M&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F9DY0LKOdE3M%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/208890e831599b84dec5151ff6eb9378/href">https://medium.com/media/208890e831599b84dec5151ff6eb9378/href</a></iframe><p>What does that mean? It’s easier to show than to tell, so let’s examine the future of food delivery enabled by self-driving technology.</p><p>Some things don’t change — when your meal is ready to be delivered, a restaurant employee will place it in the vehicle. (Surprise! You ordered tacos.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZOlCzJMgpQ_s7_aU_q7vQA.jpeg" /></figure><p>We designed a Transit Connect for this pilot program with a locker system to secure your food and allow us to serve multiple customers on one delivery route.</p><p>Additionally, services like Postmates must deliver an assortment of products from sushi restaurants to hardware stores. Therefore, the rear and passenger-side lockers are different sizes to allow us to test optimal vehicle configuration. Ultimately, we are testing how businesses and consumers interact with a self-driving vehicle.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*I6iCgsFBUikAyqpllvK5HA.jpeg" /><figcaption>This Ford Transit Connect self-driving research vehicle features three lockers — one on the passenger side and two in the back — that can deliver both food and goods.</figcaption></figure><p>After the restaurant employee types his access code into the screen, one of the lockers will automatically open so that he can place the food inside. Each locker has two cup holders so that you don’t have to worry about losing half your beverage in transit.</p><p>When the vehicle arrives at its destination, the customer receives a text notification indicating the delivery is ready for pickup.</p><p>Upon meeting the vehicle at the curb, consumers enter an access code into the touch screen and the appropriate locker will open. Audio prompts direct the interaction and lights will illuminate the designated locker. We’re making interactions with the vehicle as easy as possible through various sensory technologies built into the Transit Connect.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*0RH3jnCnI5mnpwS4m30ypg.gif" /></figure><p>This is our first self-driving research vehicle modified specifically to test a variety of interfaces — the touch screen, the locker system, the external audio system— to inform the design of <a href="https://medium.com/self-driven/coming-to-a-city-near-you-test-driving-our-autonomous-vehicle-business-27a05a2b082e">our purpose-built self-driving vehicle</a> that’s scheduled to arrive in 2021.</p><p>Ultimately, through our partnership with Postmates, we’re testing methods for efficient deliveries to help local businesses expand their reach and provide a seamless experience to customers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LmLmmdEaUHSMStqNVcCsrQ.jpeg" /></figure><p>If you have the opportunity to check out the self-driving experience, jump at the chance to contribute to the future of delivery. And equally important — don’t forget to enjoy your meal!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6d1358bdbf87" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/lets-give-them-something-to-taco-bout-enabling-self-driving-food-delivery-with-postmates-6d1358bdbf87">Let’s Give Them Something to Taco ‘Bout: Enabling Self-Driving Food Delivery With Postmates</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/lets-give-them-something-to-taco-bout-enabling-self-driving-food-delivery-with-postmates-6d1358bdbf87?source=rss----7192a82ca09a---4" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>386</wp:post_id>
		<wp:post_date><![CDATA[2018-06-11 13:01:03]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-11 13:01:03]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[lets-give-them-something-to-taco-bout-enabling-self-driving-food-delivery-with-postmates]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driven/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/lets-give-them-something-to-taco-bout-enabling-self-driving-food-delivery-with-postmates-6d1358bdbf87?source=rss----7192a82ca09a---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[838]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>On a mission to build the uncrashable car</title>
		<link>https://fifthlevel.ai/archives/545</link>
		<pubDate>Mon, 11 Jun 2018 17:05:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/mit-alumnus-ryan-eustice-building-an-uncrashable-car-0611</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Ryan Eustice’s interest in self-driving cars began 12,500 feet below the surface of the Atlantic. As a PhD student in the joint MIT-Woods Hole Oceanographic Institution Program, Eustice focused on creating technologies for underwater vehicles to map and understand their environments.</p> <p>“That’s how I got into this line of work originally,” explains Eustice, who is currently senior vice president of automated driving at Toyota Research Institute and associate professor at the University of Michigan. “From an engineering perspective, the focus would be on helping the robot better navigate and understand its surroundings.”</p> <p>At MIT and Woods Hole, Eustice would deploy robots on field cruises to take pictures or make a map of the seafloor using cameras, sonar, or LIDAR — an acronym for light detection and ranging. That map would then be used by a geologist or marine biologist for their research purposes. A breakthrough in his career came in 2004, when he had the opportunity to send one of his robots to the site of the Titanic wreck, 12,500 feet below the water’s surface off the coast of Newfoundland. “I was able to produce a very accurate reconstruction and map of the wreckage using the downward-looking camera imagery the robot collected.”</p> <p>Professor John Leonard, who served as Eustice’s co-advisor while he was a PhD student, found Eustice’s work ethic infectious. One day, Leonard was facing a deadline to write some paragraphs for the literature review of one of Eustice’s important papers. “I said I would try to write a few paragraphs —&nbsp;and Ryan said ‘Do or do not, there is no try,’” recalls Leonard, in reference to the famous quip by <em>Star Wars</em> character Yoda. “I stayed late that night and wrote the paragraphs before going home.”</p> <p>After receiving his PhD, Eustice made his way back to his home-state of Michigan. He was offered a faculty position in the University of Michigan’s Department of Naval Architecture and Marine Engineering, where he continued his work on underwater robotics. “I’ve been using some of the same technology that went into mapping the Titanic,” Eustice explains. “I’m looking at how robots can be deployed near naval ships so they can do inspection tasks or map the below-water portion of the hull.”</p> <p>Shortly after arriving in Michigan, Eustice was asked to apply the technology he was building for underwater vehicles to cars. In 2007, the U.S. Defense Advanced Research Projects Agency (DARPA) announced their Urban Challenge to build an autonomous vehicle that can drive and navigate everyday traffic scenarios. The team from Ford Motor Company, a few towns over from Ann Arbor, were looking for someone with expertise in mapping, navigation, and LIDAR technologies. Eustice fit the bill.</p> <p>The Ford Motor Company team finished as finalists in the 2007 DARPA Urban Challenge. Eustice continued to work with them for nearly a decade, before joining <a href="https://www.tri.global/" target="_blank">Toyota Research Institute</a> in 2016. At Toyota, Eustice leads a team developing a sensor-rich car built around artificial intelligence. Like many companies around the world, part of the team’s research is focused on what they call “chauffeur mode” —&nbsp;where the human is the passenger and the car is fully capable to drive itself.</p> <p>But according to Eustice, this kind of automation has multiple applications. “We are working on a technology stack that gets us to a full automation scenario, but at the same time we see a tremendous opportunity to use that technology in a different way,” says Eustice. “Fundamentally, we want to build an un-crashable car.”</p> <p>With fully autonomous vehicles, the human has to be somewhat alert since the car is unable to handle all individually rare but collectively common scenarios that happen in day-to-day driving —&nbsp;a mattress flipping off a car in front of you, or a crossing guard motioning for you to stop, for example. In those situations, human drivers need to remain alert in the event they have to take over steering control. Humans are expected to watch the AI.</p> <p>But Eustice and his team are developing technologies that flip that equation. “With ‘guardian mode’ we say, ‘Well let’s imagine a system where we have AI guard the human,’” Eustice explains. It’s a subtle change but has profound ramifications that can augment the human driver.</p> <p>Eustice and his team have outfitted test cars with 360-degree sensing around the vehicle, using similar technologies he worked with as a graduate student at MIT. But instead of mapping oceanic environments, he now has one particularly lofty ambition,&nbsp;“to develop a car that is incapable of causing a crash.”</p>
<p><b><a href="http://news.mit.edu/2018/mit-alumnus-ryan-eustice-building-an-uncrashable-car-0611" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>545</wp:post_id>
		<wp:post_date><![CDATA[2018-06-11 17:05:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-11 17:05:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[on-a-mission-to-build-the-uncrashable-car]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/mit-alumnus-ryan-eustice-building-an-uncrashable-car-0611]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Audi And Airbus To Work On Air-Taxi Project Testing In Germany</title>
		<link>https://fifthlevel.ai/archives/634</link>
		<pubDate>Wed, 20 Jun 2018 22:37:30 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/249793/audi-airbus-air-taxi-testing/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[It’s almost as cool as The Jetsons cartoon. <p><b><a href="https://www.motor1.com/news/249793/audi-airbus-air-taxi-testing/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>634</wp:post_id>
		<wp:post_date><![CDATA[2018-06-20 22:37:30]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-20 22:37:30]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[audi-and-airbus-to-work-on-air-taxi-project-testing-in-germany]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/249793/audi-airbus-air-taxi-testing/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Check Out Radio-Controlled Chevy Corvette Guy Built For $4,000</title>
		<link>https://fifthlevel.ai/archives/635</link>
		<pubDate>Thu, 14 Jun 2018 18:54:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/249007/remonte-controlled-chevy-corvette-build/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[Forget models, this guy turned a real C6 Corvette into an RC car, and it's his daily driver. <p><b><a href="https://www.motor1.com/news/249007/remonte-controlled-chevy-corvette-build/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>635</wp:post_id>
		<wp:post_date><![CDATA[2018-06-14 18:54:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-14 18:54:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[check-out-radio-controlled-chevy-corvette-guy-built-for-4000]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/249007/remonte-controlled-chevy-corvette-build/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Ford&#039;s Fake Autonomous Transit Connect Is Ready To Deliver Dinner</title>
		<link>https://fifthlevel.ai/archives/636</link>
		<pubDate>Mon, 11 Jun 2018 18:48:15 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/245402/ford-autonomous-transit-connect-test/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[Ford wants to know how people interact with autonomous cars, even if this van isn't autonomous. <p><b><a href="https://www.motor1.com/news/245402/ford-autonomous-transit-connect-test/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>636</wp:post_id>
		<wp:post_date><![CDATA[2018-06-11 18:48:15]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-11 18:48:15]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[fords-fake-autonomous-transit-connect-is-ready-to-deliver-dinner]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/245402/ford-autonomous-transit-connect-test/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Land Rover Developing Autonomous Tech For Off-Roading</title>
		<link>https://fifthlevel.ai/archives/637</link>
		<pubDate>Sun, 03 Jun 2018 08:41:13 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/243855/land-rover-autonomous-off-road/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[The system is being co-developed with the University of Birmingham. <p><b><a href="https://www.motor1.com/news/243855/land-rover-autonomous-off-road/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>637</wp:post_id>
		<wp:post_date><![CDATA[2018-06-03 08:41:13]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-03 08:41:13]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[land-rover-developing-autonomous-tech-for-off-roading]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/243855/land-rover-autonomous-off-road/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>What Is This GLE Hiding Behind Its Six Mercedes Badges?</title>
		<link>https://fifthlevel.ai/archives/638</link>
		<pubDate>Sun, 27 May 2018 16:00:02 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/243205/gle-behind-six-mercedes-badges-spy/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[What better way to hide amazing tech than using six badges. <p><b><a href="https://www.motor1.com/news/243205/gle-behind-six-mercedes-badges-spy/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>638</wp:post_id>
		<wp:post_date><![CDATA[2018-05-27 16:00:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-27 16:00:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[what-is-this-gle-hiding-behind-its-six-mercedes-badges]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/videos/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/243205/gle-behind-six-mercedes-badges-spy/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple recruits senior Waymo engineer &amp; NASA veteran for self-driving car project</title>
		<link>https://fifthlevel.ai/archives/709</link>
		<pubDate>Fri, 15 Jun 2018 19:28:48 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/06/15/apple-recruits-senior-waymo-engineer-nasa-veteran-for-self-driving-car-project</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/26464-37655-applecar-testbed2-l.jpg" alt="Article Image" border="0" /> <br><br> The latest addition to Apple's self-driving car team is Jaime Waydo, previously a senior engineer at Alphabet's Waymo as well as NASA's Jet Propulsion Laboratory. <p><b><a href="https://appleinsider.com/articles/18/06/15/apple-recruits-senior-waymo-engineer-nasa-veteran-for-self-driving-car-project" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>709</wp:post_id>
		<wp:post_date><![CDATA[2018-06-15 19:28:48]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-15 19:28:48]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-recruits-senior-waymo-engineer-nasa-veteran-for-self-driving-car-project]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/06/15/apple-recruits-senior-waymo-engineer-nasa-veteran-for-self-driving-car-project]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[840]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>State of the Art in Autonomous Driving</title>
		<link>https://fifthlevel.ai/archives/746</link>
		<pubDate>Tue, 19 Jun 2018 10:08:25 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://aid-driving.eu/?p=3855</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Der Beitrag <a rel="nofollow" href="http://aid-driving.eu/state-of-the-art-in-autonomous-driving/">State of the Art in Autonomous Driving</a> erschien zuerst auf <a rel="nofollow" href="http://aid-driving.eu">AID</a>.</p> <p><b><a href="http://aid-driving.eu/state-of-the-art-in-autonomous-driving/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>746</wp:post_id>
		<wp:post_date><![CDATA[2018-06-19 10:08:25]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-19 10:08:25]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[state-of-the-art-in-autonomous-driving]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/state-of-the-art-in-autonomous-driving/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Rolls-Royce Shunning Semi-Autonomous Tech</title>
		<link>https://fifthlevel.ai/archives/1150</link>
		<pubDate>Tue, 29 May 2018 07:58:08 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/243349/rolls-royce-avoid-semi-autonomous-tech/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[It wants chauffeurs to earn their keep. <p><b><a href="https://www.motor1.com/news/243349/rolls-royce-avoid-semi-autonomous-tech/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1150</wp:post_id>
		<wp:post_date><![CDATA[2018-05-29 07:58:08]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-29 07:58:08]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[rolls-royce-shunning-semi-autonomous-tech]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/243349/rolls-royce-avoid-semi-autonomous-tech/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>VW And Apple Allegedly Partner On Electric, Autonomous Transporters</title>
		<link>https://fifthlevel.ai/archives/1151</link>
		<pubDate>Thu, 24 May 2018 20:41:07 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/243022/vw-apple-autonomous-electric-transporter/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[VW tried to team up with companies like BMW and Mercedes-Benz before winding up with VW. <p><b><a href="https://www.motor1.com/news/243022/vw-apple-autonomous-electric-transporter/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1151</wp:post_id>
		<wp:post_date><![CDATA[2018-05-24 20:41:07]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-24 20:41:07]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[vw-and-apple-allegedly-partner-on-electric-autonomous-transporters]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/243022/vw-apple-autonomous-electric-transporter/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The positive risk profile of self-driving cars</title>
		<link>https://fifthlevel.ai/archives/2473</link>
		<pubDate>Thu, 31 May 2018 19:46:33 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.driverless-future.com/?p=1147</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The two recent fatal accidents with self-driving cars by Uber and Tesla have not led to the major backlash which many people had predicted. While this does not come as a surprise (the predictions ignored the long history of technical innovations, where accidents have rarely slowed or even halted the advance of a technology), nevertheless, the two harrowing accidents increase the concern of the public and of regulators about the safety of self-driving cars.</p>
<p>Therefore this is the right time to perform a more careful analysis of the risk profile of this technology. As we will show in the following, the specific forms of risk, accident scenarios, and risk mitigation strategies for self-driving cars differ very significantly from other technologies that have been developed over the last centuries. To illustrate the differences, we will examine three key aspects of the risk profile of self-driving car technologies and contrast them with established technologies:</p>
<p><strong>1) One- or two sided distribution of safety outcomes</strong><br />
Self-driving cars are an unusual product from the perspective of safety-related outcomes. Practically every product comes with the risk that it&#8217;s use may inflict harm under some circumstances. For most products the safety related outcomes are either harm (negative outcome) or no effect. A much smaller group of products can also lead to positive safety-related outcomes &#8211; their use increases safety. A self-driving car will prevent some accidents (positive outcome) or cause accidents (negative outcome); this two-sided distribution of safety outcomes contrasts with other product categories such as microwaves, coffee machines or electric drills which have only one-sided safety outcomes. From one perspective, products with two-sided safety distributions are preferable over products with one-sided distributions. But they present a challenge for risk analysis and for ethical considerations because uncertainty about the distribution of negative outcomes may need to be balanced against the certainty of positive outcomes. Delaying the use of self-driving cars for too long may cause harm (accidents that would not have happened).</p>
<p>In the health sector, this dilemma is a well-known problem for the approval of medical treatments. And the US Food and Drug Administration (FDA) has worked hard to balance both sides of the distribution (both by speeding up the approval process and by enabling critically ill patients to get access to experimental treatments in certain cases). But self-driving cars differ from medical treatments in a very positive way: Whereas the expected positive effects of a treatment often do not materialize (uncertainty on the positive part of the distribution), there is much more certainty about the positive safety outcomes of self-driving cars (accident prevention) and we already have statistical data for the safety benefits of some driver assistance systems.</p>
<p>Thus any legislative effort for regulating the approval of self-driving cars, needs to consider both sides of the distribution of safety outcomes.</p>
<p><strong>2) Alignment of safety goals with development goals</strong><br />
For most products, safety is not an innate part or consequence of the development process. Over the last century we have learned the hard way that a large body of laws and regulations are needed (which then lead to well thought out internal processes) to ensure that safety is adequately addressed in all phases of the development process.</p>
<p>However, the situation is different for self-driving cars. For anyone developing an autonomous vehicle, the primary and overarching development goal of self-driving cars is to be able to operate the vehicle safely at all times. Driving as such is <strong>NOT</strong> the primary goal, it is a secondary concern because just navigating the car on the road and keeping control of speed and direction is only a very small part of the development problem.<br />
The internal state of the car at any given moment is most important, because the car needs to constantly monitor its environment, identify road signs, traffic lights, predict actions of other traffic participants,  etc. Therefore the main concern of development teams is to make sure that the car has a complete and accurate internal representation (of state and probable behavior) of what is going on around it. The key metrics in the development process are not just driving errors but their much earlier cause &#8211; shortcomings in sensing, interpretation, prediction. Thus the development of self-driving cars is a constant and intensive search for failures, potential errors, potential flaws. As a consequence, even in the absence of any safety regulations, it would not be possible to develop a self-driving car for the market without being constantly focused on safety. Of course, this is not a guarantee that no mistakes will be made. And this is not a guarantee that the development process will lead to absolutely flawless vehicles (that is not possible). But the technology of self-driving cars is one of only very few technologies where safety issues are inherently the primary focus of development.</p>
<p><strong>3) Efficiency of recall process for defective products</strong><br />
Self-driving cars are almost unique in another, third dimension of risk: For most technologies it is difficult to prevent harm once a defective model is released to the public (and this has important implications for regulation). Once an Espresso machine, a drug or another product reaches the hands of thousands or millions of users it is very difficult to ensure that a defective product model will not lead repeatedly to harm somewhere. Recalls take time and rarely reach all owners. Again, the situation is very different for self-driving cars. They incorporate wireless communication and update mechanisms that allow the near-instant grounding of defective vehicles models. A worst-case scenario where a flaw is discovered after tens of thousands of vehicles have been released to public roads is not realistic: when accidents point to the flaw, the other cars on the road will quickly be grounded and thus further accidents will be prevented from happening. Of course this does not mean that standards for approving self-driving cars should be lax but rather that we should keep the likely risk scenarios in perspective, when we consider regulations for self-driving cars.</p>
<p>In summary, the risk profile of self-driving cars is quite unusual because it is positive on the following three dimensions:<br />
&#8212; With self-driving cars, safety is the primary development objective and focus, it is an inherent part of the development process and can never be just an afterthought or constraint of the development process<br />
&#8212; Self-driving cars have double-sided safety outcomes: Besides the risk of failure, they also increase the safety of passengers. Keeping self-driving cars off the road for to long because of worries about accidents may be harmful<br />
&#8212; Self-driving cars allow instant grounding of defective models; defects can not harm large groups of customers</p>
<p>In the public and regulatory discourse we need to do justice to the unique risk characteristics of self-driving cars!</p>
<p>P.S. For more on self-driving car safety and how (not) to determine statistically whether self-driving cars are safe, see my earlier post on Misconceptions of Self-Driving cars: <a href="http://www.driverless-future.com/?page_id=983">Misconception 7: To convince us that they are safe, self-driving cars must drive hundreds of millions of miles</a></p>
<p>&nbsp;</p> <p><b><a href="http://www.driverless-future.com/?p=1147" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2473</wp:post_id>
		<wp:post_date><![CDATA[2018-05-31 19:46:33]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-31 19:46:33]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[the-positive-risk-profile-of-self-driving-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?feed=rss2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?p=1147]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Driver was streaming The Voice when Uber self-driving car crashed, say police</title>
		<link>https://fifthlevel.ai/archives/2496</link>
		<pubDate>Fri, 22 Jun 2018 08:32:46 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.theguardian.com/technology/2018/jun/22/driver-was-streaming-the-voice-when-uber-self-driving-car-crashed-say-police</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Rafaela Vasquez looked up half a second before Arizona crash that killed woman, report says</p><p>The “safety” driver behind the wheel of a self-driving Uber that hit and killed a pedestrian was streaming the television show The Voice on her phone at the time of the crash, police have said.</p><p>The collision that killed Elaine Herzberg, 49, who was crossing the road at night in Tempe, Arizona, was “entirely avoidable”, a police report said, if Rafaela Vasquez had been paying attention.</p> <a href="https://www.theguardian.com/technology/2018/jun/22/driver-was-streaming-the-voice-when-uber-self-driving-car-crashed-say-police">Continue reading...</a> <p><b><a href="https://www.theguardian.com/technology/2018/jun/22/driver-was-streaming-the-voice-when-uber-self-driving-car-crashed-say-police" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2496</wp:post_id>
		<wp:post_date><![CDATA[2018-06-22 08:32:46]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-22 08:32:46]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[driver-was-streaming-the-voice-when-uber-self-driving-car-crashed-say-police]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/self-driving-cars/rss]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/2018/jun/22/driver-was-streaming-the-voice-when-uber-self-driving-car-crashed-say-police]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Motorists &#039;are being misled by autonomous driving aids&#039; - report</title>
		<link>https://fifthlevel.ai/archives/2497</link>
		<pubDate>Mon, 11 Jun 2018 23:01:34 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.theguardian.com/technology/2018/jun/12/motorists-misled-autonomous-driving-aids-tesla-nissan-report</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Tesla and Nissan among carmakers criticised for setting ‘unrealistic expectations’</p><p>The marketing of driving assistance features such as Autopilot, ProPilot and others as “autonomous” is setting unrealistic expectations and causing dangerous driving, according to insurers and vehicle safety researchers.</p><p>In <a href="https://www.abi.org.uk/globalassets/files/publications/public/motor/2018/06/thatcham-research-assisted-and-automated-driving-definitions-summary-june-2018.pdf">a report</a>, Thatcham Research and the Association of British Insurers (ABI) say that drivers are being lulled into a false sense of security by the marketing of new driver assistance features making their way into cars and costing upwards of £20,000.</p> <a href="https://www.theguardian.com/technology/2018/jun/12/motorists-misled-autonomous-driving-aids-tesla-nissan-report">Continue reading...</a> <p><b><a href="https://www.theguardian.com/technology/2018/jun/12/motorists-misled-autonomous-driving-aids-tesla-nissan-report" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2497</wp:post_id>
		<wp:post_date><![CDATA[2018-06-11 23:01:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-11 23:01:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[motorists-are-being-misled-by-autonomous-driving-aids-report]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/self-driving-cars/rss]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/2018/jun/12/motorists-misled-autonomous-driving-aids-tesla-nissan-report]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Tesla fatal crash: &#039;autopilot&#039; mode sped up car before driver killed, report finds</title>
		<link>https://fifthlevel.ai/archives/2498</link>
		<pubDate>Thu, 07 Jun 2018 23:19:05 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.theguardian.com/technology/2018/jun/07/tesla-fatal-crash-silicon-valley-autopilot-mode-report</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Findings about crash in Silicon Valley raise fresh concerns about limits of Elon Musk’s technology<br></p><p>A Tesla driving in “autopilot” mode crashed in March when the vehicle sped up and steered into a concrete barrier, according to a new report on the fatal collision, raising fresh concerns about Elon Musk’s technology.<br></p><p>The National Transportation Safety Board (NTSB) <a href="https://www.ntsb.gov/investigations/AccidentReports/Pages/HWY18FH011-preliminary.aspx">said</a> that four seconds before the 23 March <a href="https://www.theguardian.com/technology/2018/mar/31/tesla-car-crash-autopilot-mountain-view">crash on a highway in Silicon Valley</a>, which killed Walter Huang, 38, the car stopped following the path of a vehicle in front of it. Three seconds before the impact, it sped up from 62mph to 70.8mph, and the car did not brake or steer away, the NTSB said.</p><p> <span>Related: </span><a href="https://www.theguardian.com/technology/2018/jun/04/elon-musk-visionary-space-travel-fell-back-down-to-earth">Elon Musk: as business fortunes dip, he starts a war with the media</a> </p> <a href="https://www.theguardian.com/technology/2018/jun/07/tesla-fatal-crash-silicon-valley-autopilot-mode-report">Continue reading...</a> <p><b><a href="https://www.theguardian.com/technology/2018/jun/07/tesla-fatal-crash-silicon-valley-autopilot-mode-report" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2498</wp:post_id>
		<wp:post_date><![CDATA[2018-06-07 23:19:05]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-07 23:19:05]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[tesla-fatal-crash-autopilot-mode-sped-up-car-before-driver-killed-report-finds]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/self-driving-cars/rss]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/2018/jun/07/tesla-fatal-crash-silicon-valley-autopilot-mode-report]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Emergency brake was disabled on self-driving Uber that killed woman</title>
		<link>https://fifthlevel.ai/archives/2500</link>
		<pubDate>Thu, 24 May 2018 17:52:24 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.theguardian.com/technology/2018/may/24/emergency-brake-was-disabled-on-self-driving-uber-that-killed-woman</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Federal investigation finds emergency braking system was not enabled in SUV that hit Arizona pedestrian</p><p>A federal investigation into a self-driving Uber SUV that <a href="https://www.theguardian.com/technology/2018/mar/19/uber-self-driving-car-kills-woman-arizona-tempe">hit and killed a pedestrian in March</a> has found that the vehicle’s emergency braking system was disabled.</p><p>The preliminary report, issued by the National Transportation Safety Board, said on Thursday that while the vehicle’s guidance system had spotted the woman about six seconds before hitting her, emergency braking manoeuvres were not enabled in order to “reduce the potential for erratic vehicle behavior”.</p><p> <span>Related: </span><a href="https://www.theguardian.com/technology/2018/mar/19/uber-self-driving-car-kills-woman-arizona-tempe">Self-driving Uber kills Arizona woman in first fatal crash involving pedestrian</a> </p> <a href="https://www.theguardian.com/technology/2018/may/24/emergency-brake-was-disabled-on-self-driving-uber-that-killed-woman">Continue reading...</a> <p><b><a href="https://www.theguardian.com/technology/2018/may/24/emergency-brake-was-disabled-on-self-driving-uber-that-killed-woman" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2500</wp:post_id>
		<wp:post_date><![CDATA[2018-05-24 17:52:24]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-24 17:52:24]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[emergency-brake-was-disabled-on-self-driving-uber-that-killed-woman]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/self-driving-cars/rss]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/2018/may/24/emergency-brake-was-disabled-on-self-driving-uber-that-killed-woman]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Chip upgrade helps miniature drones navigate</title>
		<link>https://fifthlevel.ai/archives/2535</link>
		<pubDate>Wed, 20 Jun 2018 04:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/novel-chip-upgrade-helps-miniature-drones-navigate-0620</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Researchers at MIT, who last year <a href="http://news.mit.edu/2017/miniaturizing-brain-smart-drones-0712">designed a tiny computer chip</a> tailored to help honeybee-sized drones navigate, have now shrunk their chip design even further, in both size and power consumption.</p> <p>The team, co-led by Vivienne Sze, associate professor in MIT's Department of Electrical Engineering and Computer Science (EECS), and Sertac Karaman, the Class of 1948 Career Development Associate Professor of Aeronautics and Astronautics, built a fully customized chip from the ground up, with a focus on reducing power consumption and size while also increasing processing speed.</p> <p>The new computer chip, named “<a href="http://navion.mit.edu" target="_blank">Navion</a>,” which they are <a href="http://navion.mit.edu/2018_vlsi_navion.pdf" target="_blank">presenting this week</a> at the Symposia on VLSI Technology and Circuits, is just 20 square millimeters — about the size of a LEGO minifigure’s footprint — and consumes just 24 milliwatts of power, or about 1 one-thousandth the energy required to power a lightbulb.</p> <p>Using this tiny amount of power, the chip is able to process in real-time camera images at up to 171 frames per second, as well as inertial measurements, both of which it uses to determine where it is in space. The researchers say the chip can be integrated into “nanodrones” as small as a fingernail, to help the vehicles navigate, particularly in remote or inaccessible places where global positioning satellite data is unavailable.</p> <p>The chip design can also be run on any small robot or device that needs to navigate over long stretches of time on a limited power supply.</p> <p>“I can imagine applying this chip to low-energy robotics, like flapping-wing vehicles the size of your fingernail, or lighter-than-air vehicles like weather balloons, that have to go for months on one battery,” says Karaman, who is a member of the Laboratory for Information and Decision Systems and the Institute for Data, Systems, and Society at MIT. “Or imagine medical devices like a little pill you swallow, that can navigate in an intelligent way on very little battery so it doesn’t overheat in your body. The chips we are building can help with all of these.”</p> <p>Sze and Karaman’s co-authors are EECS graduate student Amr Suleiman, who is the lead author; EECS graduate student Zhengdong Zhang; and Luca Carlone, who was a research scientist during the project and is now an assistant professor in MIT’s Department of Aeronautics and Astronautics.</p> <p><strong>A flexible chip</strong></p> <p>In the past few years, multiple research groups have engineered miniature drones small enough to fit in the palm of your hand. Scientists envision that such tiny vehicles can fly around and snap pictures of your surroundings, like mosquito-sized photographers or surveyors, before landing back in your palm, where they can then be easily stored away.</p> <p>But a palm-sized drone can only carry so much battery power, most of which is used to make its motors fly, leaving very little energy for other essential operations, such as navigation, and, in particular, state estimation, or a robot’s ability to determine where it is in space. &nbsp;</p> <p>“In traditional robotics, we take existing off-the-shelf computers and implement [state estimation] algorithms on them, because we don’t usually have to worry about power consumption,” Karaman says. “But in every project that requires us to miniaturize low-power applications, we have to now think about the challenges of programming in a very different way.”</p> <pre>
</pre> <p>In their previous work, Sze and Karaman began to address such issues by combining algorithms and hardware in a single chip. Their initial design was implemented on a field-programmable gate array, or FPGA, a commercial hardware platform that can be configured to a given application. The chip was able to perform state estimation using 2 watts of power, compared to larger, standard drones that typically require 10 to 30 watts to perform the same tasks. Still, the chip’s power consumption was greater than the total amount of power that miniature drones can typically carry, which researchers estimate to be about 100 milliwatts.</p> <p>To shrink the chip further, in both size and power consumption, the team decided to build a chip from the ground up rather than reconfigure an existing design. “This gave us a lot more flexibility in the design of the chip,” Sze says.</p> <p><strong>Running in the world</strong></p> <p>To reduce the chip’s power consumption, the group came up with a design to minimize the amount of data — in the form of camera images and inertial measurements — that is stored on the chip at any given time. The design also optimizes the way this data flows across the chip.</p> <p>“Any of the images we would’ve temporarily stored on the chip, we actually compressed so it required less memory,” says Sze, who is a member of the Research Laboratory of Electronics at MIT. The team also cut down on extraneous operations, such as the computation of zeros, which results in a zero. The researchers found a way to skip those computational steps involving any zeros in the data. “This allowed us to avoid having to process and store all those zeros, so we can cut out a lot of unnecessary storage and compute cycles, which reduces the chip size and power, and increases the processing speed of the chip,” Sze says.</p> <p>Through their design, the team was able to reduce the chip’s memory from its previous 2 megabytes, to about 0.8 megabytes. The team tested the chip on previously collected datasets generated by drones flying through multiple environments, such as office and warehouse-type spaces.</p> <p>“While we customized the chip for low power and high speed processing, we also made it sufficiently flexible so that it can adapt to these different environments for additional energy savings,” Sze says. “The key is finding the balance between flexibility and efficiency.” The chip can also be reconfigured to support different cameras and inertial measurement unit (IMU) sensors.</p> <p>From these tests, the researchers found they were able to bring down the chip’s power consumption from 2 watts to 24 milliwatts, and that this was enough to power the chip to process images at 171 frames per second — a rate that was even faster than what the datasets projected.</p> <p>The team plans to demonstrate its design by implementing its chip on a miniature race car. While a screen displays an onboard camera’s live video, the researchers also hope to show the chip determining where it is in space, in real-time, as well as the amount of power that it uses to perform this task. Eventually, the team plans to test the chip on an actual drone, and ultimately on a miniature drone.</p> <p>This research was supported, in part, by the Air Force Office of Scientific Research, and by the National Science Foundation.</p>
<p><b><a href="http://news.mit.edu/2018/novel-chip-upgrade-helps-miniature-drones-navigate-0620" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2535</wp:post_id>
		<wp:post_date><![CDATA[2018-06-20 04:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-20 04:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[chip-upgrade-helps-miniature-drones-navigate]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/novel-chip-upgrade-helps-miniature-drones-navigate-0620]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Keeping data fresh for wireless networks</title>
		<link>https://fifthlevel.ai/archives/2536</link>
		<pubDate>Tue, 05 Jun 2018 03:59:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/keeping-data-fresh-wireless-networks-0605</guid>
		<description></description>
		<content:encoded><![CDATA[<p>For wireless networks that share time-sensitive information on the fly, it’s not enough to transmit data quickly. That data also need to be fresh. Consider the many sensors in your car. While it may take less than a second for most sensors to transmit a data packet to a central processor, the age of that data may vary, depending on how frequently a sensor is relaying readings.</p> <p>In an ideal network, these sensors should be able to transmit updates constantly, providing the freshest, most current status for every measurable feature, from tire pressure to the proximity of obstacles. But there’s only so much data that a wireless channel can transmit without completely overwhelming the network.</p> <p>How, then, can a constantly updating network — of sensors, drones, or data-sharing vehicles — minimize the age of the information that it receives at any moment, while at the same time avoiding data congestion?</p> <p>Engineers in MIT’s Laboratory for Information and Decision Systems are tackling this question and have come up with a way to provide the freshest possible data for a simple wireless network.</p> <p>The researchers say their method may be applied to simple networks, such as multiple drones that transmit position coordinates to a single control station, or sensors in an industrial plant that relay status updates to a central monitor. Eventually, the team hopes to tackle even more complex systems, such as networks of vehicles that wirelessly share traffic data.</p> <p>“If you are exchanging congestion information, you would want that information to be as fresh as possible,” says Eytan Modiano, professor of aeronautics and astronautics and a member of MIT’s Laboratory for Information and Decision Systems. “If it’s dated, you might make the wrong decision. That’s why the age of information is important.”</p> <p>Modiano and his colleagues presented their method in a paper at IEEE’s International Conference on Computation Communications (Infocom), where it won a Best Paper Award. The paper will appear online in the future. The paper’s lead author is graduate student Igor Kadota; former graduate student Abhishek Sinha is also a co-author.</p> <p><strong>Keeping it fresh</strong></p> <p>Traditional networks are designed to maximize the amount of data that they can transmit across channels, and minimize the time it takes for that data to reach its destination. Only recently have researchers considered the age of the information — how fresh or stale information is from the perspective of its recipient.</p> <p>“I first got excited about this problem, thinking in the context of UAVs — unmanned aerial vehicles that are moving around in an environment, and they need to exchange position information to avoid collisions with one another,” Modiano says. “If they don’t exchange this information often enough, they might collide. So we stepped back and started looking at the fundamental problem of how to minimize age of information in wireless networks.”</p> <p>In this new paper, Modiano’s team looked for ways to provide the freshest possible data to a simple wireless network. They modeled a basic network, consisting of a single data receiver, such as a central control station, and multiple nodes, such as several data-transmitting drones.</p> <p>The researchers assumed that only one node can transmit data over a wireless channel at any given time. The question they set out to answer: Which node should transmit data at which time, to ensure that the network receives the freshest possible data, on average, from all nodes?</p> <p>“We are limited in bandwidth, so we need to be selective about what and when nodes are transmitting,” Modiano says. “We say, how do we minimize age in this simplest of settings? Can we solve this? And we did.”</p> <p><strong>An optimal age</strong></p> <p>The team’s solution lies in a simple algorithm that essentially calculates an “index” for each node at any given moment. A node’s index is based on several factors: the age, or freshness of the data that it’s transmitting; the reliability of the channel over which it is communicating; and the overall priority of that node.</p> <p>“For example, you may have a more expensive drone, or faster drone, and you’d like to have better or more accurate information about that drone. So, you can set that one with a high priority,” Kadota explains.</p> <p>Nodes with a higher priority, a more reliable channel, and older data, are assigned a higher index, versus nodes that are relatively low in priority, communicating over spottier channels, with fresher data, which are labeled with a lower index.</p> <p>A node’s index can change from moment to moment. At any given moment, the algorithm directs the node with the highest index to transmit its data to the receiver. In this prioritizing way, the team found that the network is guaranteed to receive the freshest possible data on average, from all nodes, without overloading its wireless channels.</p> <p>The team calculated a lower bound, meaning an average age of information for the network that is fresher than any algorithm could ever achieve. They found that the team’s algorithm performs very close to this bound, and that it is close to the best that any algorithm could do in terms of providing the freshest possible data for a simple wireless network.</p> <p>“We came up with a fundamental bound that says, you cannot possibly have a lower age of information than this value ­— no algorithm could be better than this bound — and then we showed that our algorithm came close to that bound,” Modiano says. “So it’s close to optimal.”</p> <p>The team is planning to test its index scheme on a simple network of radios, in which one radio may serve as a base station, receiving time-sensitive data from several other radios. Modiano’s group is also developing algorithms to optimize the age of information in more complex networks.</p> <p>“Our future papers will look beyond just one base station, to a network with multiple base stations, and how that interacts,” Modiano says. “And that will hopefully solve a much bigger problem.”</p> <p>This research was funded, in part, by the National Science Foundation (NSF) and the Army Research Office (ARO).</p>
<p><b><a href="http://news.mit.edu/2018/keeping-data-fresh-wireless-networks-0605" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2536</wp:post_id>
		<wp:post_date><![CDATA[2018-06-05 03:59:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-05 03:59:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[keeping-data-fresh-for-wireless-networks]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/keeping-data-fresh-wireless-networks-0605]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>A New Kind of Proving Ground: How We’re Reimagining the Future of Mobility in Detroit</title>
		<link>https://fifthlevel.ai/archives/2543</link>
		<pubDate>Tue, 19 Jun 2018 19:30:38 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/35fc20e30343</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Bill Ford, Executive Chairman, Ford Motor Company</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ds39LH5PPhHUiqFJZ3yRvQ.jpeg" /><figcaption>As Ford Motor Company ushers in its next era, it is building a new campus in Detroit’s Corktown neighborhood. The new Corktown campus will be a hub where Ford and its partners will work on autonomous and electric vehicles, and design urban mobility services and solutions that includes smart, connected vehicles, roads, parking and public transit.</figcaption></figure><p>After opening in 1913, Michigan Central Station quickly became the Midwestern equivalent of Ellis Island. It’s where dreamers in search of new jobs and new opportunities first set foot in Detroit. It was the place where we shipped troops off to war and where we welcomed them back home. The grand hall was large and majestic, and as a child I remember thinking only Detroit could make a statement like this.</p><p>Once the last train pulled out of the station 30 years ago, however, it became a place where hope left. The station became a symbol of Detroit’s hard times, a monument to the city’s struggles.</p><p>At Ford, we think it’s time for that to change. It’s time for Michigan Central Station to be restored and to remake this station into a place of possibility once again.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F0nLn5vq325o%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D0nLn5vq325o&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F0nLn5vq325o%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/7af4fc1614b63027ddc5512c4a2e3fd9/href">https://medium.com/media/7af4fc1614b63027ddc5512c4a2e3fd9/href</a></iframe><p>As the new owner of Michigan Central Station, we have big plans for the station and Corktown. This is not a symbolic gesture, but a bet on the future — one that envisions its role in the 21st century to be just as important to the American economy as Highland Park, Willow Run and the Rouge were in the 20th century.</p><p>That’s a big claim. But the automotive industry is going through some big changes, and just as Detroit has to reimagine what it is going to be, we have to do the same at Ford. We still design, build and sell the best cars and trucks in the world, but we must do more. My great-grandfather believed that Ford Motor Company had two missions: to make cars affordable so that everybody had the opportunity to own one, and to make people’s lives better. With more than 1 billion cars on the road globally, it’s safe to say we’ve succeeded with one mission. But are we still meeting the second mission?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*gIBg-2YG_NCmY1wWjEHjYQ.gif" /><figcaption>A look at Michigan Central Station today, and a rendering of what it could look like when it reopens in 2022.</figcaption></figure><p>I’ve always believed that mobility makes freedom and progress possible. It allows us to live and work wherever we want. Yet mobility is getting harder to come by. In the future, even more people are expected to live in urban areas and city transportation systems are already strained.</p><p>The urgency has never been more apparent. We’re living through a new revolution today — an information revolution that’s even faster and more disruptive than that of the industrial age. A host of new technologies is redefining what’s possible for mobility — from new strides in electric and self-driving vehicles to ways that can make it easier to orchestrate traffic.</p><p>This is where we believe that Michigan Central Station — and Corktown — can play a crucial role. In this thriving Detroit neighborhood, we’re creating a proving ground where Ford and our technology partners can design and test services and solutions focused on improving people’s lives. After all, the urban streets of Corktown are where we’ll find those unexpected real-world challenges we just can’t fake in a lab.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UUFM3-9Otyy27LEUlZBZHA.jpeg" /><figcaption>Bill Ford at Michigan Central Station.</figcaption></figure><p>We’ve already moved 220 people from our autonomous and electric vehicle teams into a former factory in Corktown. Eventually, we are going to bring all of our mobility teams to this neighborhood — all with one mission: to make people’s lives better.</p><p>This will be the kind of campus that helps Ford fast-track its transformation, where fresh thinking and new development complement all the work currently underway in Dearborn and around the world.</p><p>Just as Henry Ford’s assembly line revolutionized the industry, we’re reimagining mobility. We are determined to be the company that interprets and harnesses these emergent forces in ways that improve lives and make cities cleaner and less congested, even as millions more people call them home.</p><p>That means smart cars, but also smart roads, smart parking and smart public transit systems, and ways for them all to talk to one another. Imagine new ways of making it easier for people who don’t own cars to get to their jobs; ways that make it easier for the poor and the elderly to get the food and health care they need to live better, longer, more independent lives.</p><p>We want the best startups, the smartest talent — the kind of thinkers, engineers and problem solvers who see things differently — to come and partner with us here in Detroit to get this done.</p><p>And of course, we want the community actively involved. Michigan Central Station — a place that many suggested should be torn down — can be a beacon of development, opportunity and possibility all over again. Our plan includes renovating the grand hall to make it as majestic as it once was. We want local shops and restaurants alongside all the inventors and dreamers. We are planning a modern workspace in the train station tower and will restore it all in an environmentally friendly way.</p><p>When my great-great-grandfather William Ford immigrated to America from Cork during Ireland’s potato famine, this part of the city was coined Corktown. It was for all the immigrants, like him, who came here with little more than hope — hope that this was a place where if you worked at it, you could make something of yourself, that you could make something better for your kids.</p><p>This was a place where you could imagine what’s possible. That’s what we want to do all over again, right here in Corktown — to reimagine what is possible.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=35fc20e30343" width="1" height="1"> <p><a href="https://medium.com/@ford/a-new-kind-of-proving-ground-how-were-reimagining-the-future-of-mobility-in-detroit-35fc20e30343?source=rss-db92c082f24a------2" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2543</wp:post_id>
		<wp:post_date><![CDATA[2018-06-19 19:30:38]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-19 19:30:38]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[a-new-kind-of-proving-ground-how-were-reimagining-the-future-of-mobility-in-detroit]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@ford]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@ford/a-new-kind-of-proving-ground-how-were-reimagining-the-future-of-mobility-in-detroit-35fc20e30343?source=rss-db92c082f24a------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The Journey Begins</title>
		<link>https://fifthlevel.ai/archives/3</link>
		<pubDate>Thu, 05 Jul 2018 15:44:27 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://fifthleveldotai.wordpress.com/?p=3</guid>
		<description></description>
		<content:encoded><![CDATA[Thanks for joining me!
<blockquote>Good company in a journey makes the way seem shorter. — Izaak Walton</blockquote>
<img class="wp-image-7 size-full" src="https://twentysixteendemo.files.wordpress.com/2015/11/post.png" alt="post" width="1000" height="563" />]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3</wp:post_id>
		<wp:post_date><![CDATA[2018-07-05 15:44:27]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-05 15:44:27]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[the-journey-begins]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_headstart_post]]></wp:meta_key>
			<wp:meta_value><![CDATA[_hs_first_post]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[sharing_disabled]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_skip_all_services]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_elasticsearch_indexed_on]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-09-27 15:44:27]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_elasticsearch_indexed_on]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-09-27 15:44:27]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-09-27]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-09-19]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-09-05]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[842]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_headstart_post]]></wp:meta_key>
			<wp:meta_value><![CDATA[_hs_first_post]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[sharing_disabled]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_skip_all_services]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_elasticsearch_indexed_on]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-09-27 15:44:27]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_elasticsearch_indexed_on]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-09-27 15:44:27]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-09-27]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-09-19]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-09-05]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[842]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The Story of the Model 3</title>
		<link>https://fifthlevel.ai/archives/143</link>
		<pubDate>Thu, 19 Jul 2018 05:44:05 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/7693b623d8f8</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img src="https://cdn-images-1.medium.com/max/1001/1*iLjMNXMYNiSHPraYHBpnrw.jpeg" alt="" /></figure>
<em>Bloomberg </em>published a terrific long-form piece last week entitled, <a href="https://www.bloomberg.com/news/features/2018-07-12/how-tesla-s-model-3-became-elon-musk-s-version-of-hell">“Hell for Elon Musk Is a Midsize Sedan”</a>.

The piece covers everything from Musk’s personal work style, to Tesla’s strategy of vertical integration, to the triumphs and failures on the way to finally hitting their 5,000 cars weekly goal at the end of June. Although the article goes on to question how sustainable that success really is.
<blockquote>“In early June, at Tesla’s annual meeting, Musk tried to project calm, but at times seemed close to tears. “This is like — I tell you — the most excruciatingly hellish several months that I have ever had,” he said, before noting that Tesla’s assembly lines were being further upgraded, making the company “very likely” to hit the weekly goal of 5,000. He also revealed he’d asked employees to build a third general assembly line that would be “dramatically better than Lines 1 and 2.” That sounded even more alien-dreadnoughty.”</blockquote>
I’ve had some difficulty pairing the massive success of the Model 3 as a product with the tremendous manufacturing struggles Tesla has experienced getting the car out the door. This piece helped put that together for me.

<img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7693b623d8f8" width="1" height="1" />

<hr />

<a href="https://medium.com/self-driving-cars/the-story-of-the-model-3-7693b623d8f8">The Story of the Model 3</a> was originally published in <a href="https://medium.com/self-driving-cars">Self-Driving Cars</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>143</wp:post_id>
		<wp:post_date><![CDATA[2018-07-19 05:44:05]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-19 05:44:05]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[the-story-of-the-model-3]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="post_tag" nicename="tesla"><![CDATA[Tesla]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driving-cars]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driving-cars/the-story-of-the-model-3-7693b623d8f8?source=rss----bdc6e635b3c0---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[847]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple And Zoox Show How Stealth And Learning Are In Tension On Self-Driving Cars</title>
		<link>https://fifthlevel.ai/archives/291</link>
		<pubDate>Wed, 18 Jul 2018 18:31:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b4e653164aaf9287cf3c3af</guid>
		<description></description>
		<content:encoded><![CDATA[New revelations from the secretive self-driving efforts of Apple and Zoox are reminders of that secrecy protects innovation, but transparency can lead to faster learning. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>291</wp:post_id>
		<wp:post_date><![CDATA[2018-07-18 18:31:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-18 18:31:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-and-zoox-show-how-stealth-and-learning-are-in-tension-on-self-driving-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/07/18/for-self-driving-companies-like-apple-stealth-and-learning-are-often-in-tension/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Here&#039;s Why The Autonomous Mustang At Goodwood Had So Much Trouble</title>
		<link>https://fifthlevel.ai/archives/631</link>
		<pubDate>Mon, 16 Jul 2018 22:39:18 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/252759/why-autonomous-mustang-goodwood-trouble/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[Mechanical failures and bad advice conspired to make the early runs look terrible. <p><b><a href="https://www.motor1.com/news/252759/why-autonomous-mustang-goodwood-trouble/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>631</wp:post_id>
		<wp:post_date><![CDATA[2018-07-16 22:39:18]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-16 22:39:18]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[heres-why-the-autonomous-mustang-at-goodwood-had-so-much-trouble]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/252759/why-autonomous-mustang-goodwood-trouble/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>First-Ever Autonomous Hillclimb At Goodwood – In A 1965 Mustang</title>
		<link>https://fifthlevel.ai/archives/632</link>
		<pubDate>Wed, 11 Jul 2018 07:00:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/251937/autonomous-1965-ford-mustang-goodwood/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[Classic 'Stang might not be obvious choice for conversion to autonomous control, but it’s gonna make the first-ever attempt at Festival of Speed hillclimb. <p><b><a href="https://www.motor1.com/news/251937/autonomous-1965-ford-mustang-goodwood/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>632</wp:post_id>
		<wp:post_date><![CDATA[2018-07-11 07:00:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-11 07:00:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[first-ever-autonomous-hillclimb-at-goodwood-in-a-1965-mustang]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/251937/autonomous-1965-ford-mustang-goodwood/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Roborace To Attempt Autonomous Goodwood Hill Run</title>
		<link>https://fifthlevel.ai/archives/633</link>
		<pubDate>Wed, 27 Jun 2018 05:43:37 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/250342/roborace-autonomous-goodwood-hill-run/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[The electric car will be the first driverless car to run up the hill. <p><b><a href="https://www.motor1.com/news/250342/roborace-autonomous-goodwood-hill-run/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>633</wp:post_id>
		<wp:post_date><![CDATA[2018-06-27 05:43:37]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-27 05:43:37]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[roborace-to-attempt-autonomous-goodwood-hill-run]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/250342/roborace-autonomous-goodwood-hill-run/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple self-driving car fleet grows to 66 vehicles in California</title>
		<link>https://fifthlevel.ai/archives/704</link>
		<pubDate>Wed, 18 Jul 2018 19:14:55 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/07/18/apple-self-driving-car-fleet-grows-to-66-vehicles-in-california</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/26821-38809-26035-36466-applecar-testbed2-l-l.jpg" alt="Article Image" border="0" /> <br><br> Apple is continuing to increase the number of autonomous vehicles it has on the roads for testing, reportedly registering more vehicles with the Californian Department of Motor Vehicles to bring the fleet up to 66 self-driving cars operating in the state. <p><b><a href="https://appleinsider.com/articles/18/07/18/apple-self-driving-car-fleet-grows-to-66-vehicles-in-california" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>704</wp:post_id>
		<wp:post_date><![CDATA[2018-07-18 19:14:55]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-18 19:14:55]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-self-driving-car-fleet-grows-to-66-vehicles-in-california]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/07/18/apple-self-driving-car-fleet-grows-to-66-vehicles-in-california]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[846]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Former Apple employee pleads not guilty to stealing self-driving car trade secrets</title>
		<link>https://fifthlevel.ai/archives/705</link>
		<pubDate>Tue, 17 Jul 2018 01:55:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/07/16/former-apple-employee-pleads-not-guilty-to-stealing-self-driving-car-trade-secrets</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/26787-38700-Apple-Car-l.jpg" alt="Article Image" border="0" /> <br><br> Former Apple employee Xiaolang Zhang, who worked on the company's self-driving car initiative, on Monday entered a plea of not guilty after federal prosecutors indicted the engineer on one count of trade secret theft. <p><b><a href="https://appleinsider.com/articles/18/07/16/former-apple-employee-pleads-not-guilty-to-stealing-self-driving-car-trade-secrets" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>705</wp:post_id>
		<wp:post_date><![CDATA[2018-07-17 01:55:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-17 01:55:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[former-apple-employee-pleads-not-guilty-to-stealing-self-driving-car-trade-secrets]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/07/16/former-apple-employee-pleads-not-guilty-to-stealing-self-driving-car-trade-secrets]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[845]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Podcast discusses the new MacBook Pro, alleged industrial espionage, and iOS USB Restricted Mode</title>
		<link>https://fifthlevel.ai/archives/706</link>
		<pubDate>Fri, 13 Jul 2018 13:10:17 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/07/13/podcast-discusses-the-new-macbook-pro-alleged-industrial-espionage-and-ios-usb-restricted-mode</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/26741-38558-Apple_macbook_pro_update_True_Tone_Technology_07122018-l.jpg" alt="Article Image" border="0" /> <br><br> This week on the AppleInsider Podcast, Victor and Andrew discuss the new MacBook Pro, allegations of industrial espionage as an engineer is said to have stolen secrets from Apple and tried to board a flight to China, and iOS USB Restricted Mode and what it means for you. <p><b><a href="https://appleinsider.com/articles/18/07/13/podcast-discusses-the-new-macbook-pro-alleged-industrial-espionage-and-ios-usb-restricted-mode" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>706</wp:post_id>
		<wp:post_date><![CDATA[2018-07-13 13:10:17]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-13 13:10:17]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[podcast-discusses-the-new-macbook-pro-alleged-industrial-espionage-and-ios-usb-restricted-mode]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/07/13/podcast-discusses-the-new-macbook-pro-alleged-industrial-espionage-and-ios-usb-restricted-mode]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[844]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Former Apple employee charged with stealing autonomous car trade secrets</title>
		<link>https://fifthlevel.ai/archives/707</link>
		<pubDate>Tue, 10 Jul 2018 21:15:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/07/10/former-apple-employee-working-on-autonomous-car-project-charged-with-stealing-trade-secrets</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/26707-38494-25327-34215-applecar-testbed2-l-l.jpg" alt="Article Image" border="0" /> <br><br> Former hardware engineer, who worked on autonomous cars at Apple, arrested at airport, attempting to board a flight for China with company secrets. <p><b><a href="https://appleinsider.com/articles/18/07/10/former-apple-employee-working-on-autonomous-car-project-charged-with-stealing-trade-secrets" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>707</wp:post_id>
		<wp:post_date><![CDATA[2018-07-10 21:15:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-10 21:15:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[former-apple-employee-charged-with-stealing-autonomous-car-trade-secrets]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/07/10/former-apple-employee-working-on-autonomous-car-project-charged-with-stealing-trade-secrets]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[843]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>YouTuber Mark Rober has been developing VR tech for Apple&#039;s self-driving car project</title>
		<link>https://fifthlevel.ai/archives/708</link>
		<pubDate>Wed, 27 Jun 2018 00:13:44 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/06/26/youtuber-mark-rober-has-been-developing-vr-tech-for-apples-self-driving-car-project</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/26566-38012-180626-Rober-l.jpg" alt="Article Image" border="0" /> <br><br> According to a report on Tuesday, popular YouTube personality Mark Rober has been working with Apple's special projects group since 2015 on the development of virtual reality technology for self-driving cars. <p><b><a href="https://appleinsider.com/articles/18/06/26/youtuber-mark-rober-has-been-developing-vr-tech-for-apples-self-driving-car-project" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>708</wp:post_id>
		<wp:post_date><![CDATA[2018-06-27 00:13:44]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-27 00:13:44]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[youtuber-mark-rober-has-been-developing-vr-tech-for-apples-self-driving-car-project]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/06/26/youtuber-mark-rober-has-been-developing-vr-tech-for-apples-self-driving-car-project]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[841]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Active Learning and Labeling by Roland Meertens</title>
		<link>https://fifthlevel.ai/archives/745</link>
		<pubDate>Tue, 26 Jun 2018 06:50:26 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://aid-driving.eu/?p=3883</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Der Beitrag <a rel="nofollow" href="http://aid-driving.eu/active-learning-and-labeling/">Active Learning and Labeling by Roland Meertens</a> erschien zuerst auf <a rel="nofollow" href="http://aid-driving.eu">AID</a>.</p> <p><b><a href="http://aid-driving.eu/active-learning-and-labeling/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>745</wp:post_id>
		<wp:post_date><![CDATA[2018-06-26 06:50:26]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-26 06:50:26]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[active-learning-and-labeling-by-roland-meertens]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/active-learning-and-labeling/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Artificial intelligence will be net UK jobs creator, finds report</title>
		<link>https://fifthlevel.ai/archives/1302</link>
		<pubDate>Mon, 16 Jul 2018 23:01:50 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.theguardian.com/technology/2018/jul/17/artificial-intelligence-will-be-net-uk-jobs-creator-finds-report</guid>
		<description></description>
		<content:encoded><![CDATA[<p>AI and robotics forecast to generate 7.2m jobs, more than will be lost due to automation </p><p>Artificial intelligence is set to create more than 7m new UK jobs in healthcare, science and education by 2037, more than making up for the jobs lost in manufacturing and other sectors through automation, according to a report.</p><p>A report from PricewaterhouseCoopers argued that AI would create slightly more jobs (7.2m) than it displaced (7m) by boosting economic growth. The firm estimated about 20% of jobs would be automated over the next 20 years and no sector would be unaffected.</p><p> <span>Related: </span><a href="https://www.theguardian.com/technology/2018/jul/04/its-going-create-revolution-how-ai-transforming-nhs">'It's going to create a revolution': how AI is transforming the NHS</a> </p> <a href="https://www.theguardian.com/technology/2018/jul/17/artificial-intelligence-will-be-net-uk-jobs-creator-finds-report">Continue reading...</a> <p><b><a href="https://www.theguardian.com/technology/2018/jul/17/artificial-intelligence-will-be-net-uk-jobs-creator-finds-report" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1302</wp:post_id>
		<wp:post_date><![CDATA[2018-07-16 23:01:50]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-16 23:01:50]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[artificial-intelligence-will-be-net-uk-jobs-creator-finds-report]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/self-driving-cars/rss]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/2018/jul/17/artificial-intelligence-will-be-net-uk-jobs-creator-finds-report]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The crucial flaw of self-driving cars? They will always need human involvement &#124; John Naughton</title>
		<link>https://fifthlevel.ai/archives/2429</link>
		<pubDate>Sun, 15 Jul 2018 06:00:02 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.theguardian.com/commentisfree/2018/jul/15/crucial-flaw-of-self-driving-cars-always-need-human-involvement</guid>
		<description></description>
		<content:encoded><![CDATA[The introduction of new technology into everyday life will always take longer than you think<p>In 1979, Douglas Hofstadter, an American cognitive scientist, formulated a useful general rule that applies to all complex tasks. <a href="https://en.wikipedia.org/wiki/Hofstadter%27s_law" title="">Hofstadter’s law</a> says that “It always takes longer than you expect, even when you take into account Hofstadter’s law”. It may not have the epistemological status of Newton’s first law, but it is “good enough for government work”, as the celebrated computer scientist <a href="https://en.wikipedia.org/wiki/Roger_Needham" title="">Roger Needham</a> used to say.</p><p>Faced with this assertion, readers&nbsp;of <em>Wired</em> magazine, visitors to Gizmodo or followers of Rory Cellan-Jones, the BBC’s sainted technology correspondent, will retort that while Hofstadter’s law may apply to mundane activities such as building a third runway at Heathrow, it most definitely does not apply to digital technology, where miracles are routinely delivered at the speed of light. Think&nbsp;of the astonishing advances in machine learning, for example, or&nbsp;the sophistication of smartphones. Or think of the self-driving car, an idea that seemed preposterous only 15 years ago and yet is already a reality on the highways of&nbsp;a number of US states.&nbsp;Surely these and other achievements of digital technology took less time than we thought?</p><p> <span>Related: </span><a href="https://www.theguardian.com/technology/2018/may/20/the-five-autonomous-car-innovations">Autonomous car innovations: from jam busters to cures for queasiness</a> </p> <a href="https://www.theguardian.com/commentisfree/2018/jul/15/crucial-flaw-of-self-driving-cars-always-need-human-involvement">Continue reading...</a> <p><b><a href="https://www.theguardian.com/commentisfree/2018/jul/15/crucial-flaw-of-self-driving-cars-always-need-human-involvement" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2429</wp:post_id>
		<wp:post_date><![CDATA[2018-07-15 06:00:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-15 06:00:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[the-crucial-flaw-of-self-driving-cars-they-will-always-need-human-involvement-john-naughton]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/self-driving-cars/rss]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/commentisfree/2018/jul/15/crucial-flaw-of-self-driving-cars-always-need-human-involvement]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Uber scales back self-driving car tests in wake of fatal crash</title>
		<link>https://fifthlevel.ai/archives/2430</link>
		<pubDate>Thu, 12 Jul 2018 09:27:06 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.theguardian.com/technology/2018/jul/12/uber-scales-back-self-driving-car-tests-in-wake-of-fatal-crash</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Company lays off Pittsburgh drivers as it prepares to return to roads with a smaller fleet </p><p>Uber laid off 100 of its self-driving car backup drivers in Pittsburgh on Wednesday as it scales back its testing in the wake of its fatal crash in March.</p><p>The ride-hailing firm made 55 new mission specialist positions available to replace them, according to <a draggable="true" href="https://qz.com/1326155/uber-has-terminated-its-self-driving-car-operators-in-pittsburgh/">a report by Quartz</a>, with the intention of returning to on-the-road testing but with a reduced fleet of cars.</p> <a href="https://www.theguardian.com/technology/2018/jul/12/uber-scales-back-self-driving-car-tests-in-wake-of-fatal-crash">Continue reading...</a> <p><b><a href="https://www.theguardian.com/technology/2018/jul/12/uber-scales-back-self-driving-car-tests-in-wake-of-fatal-crash" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2430</wp:post_id>
		<wp:post_date><![CDATA[2018-07-12 09:27:06]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-12 09:27:06]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[uber-scales-back-self-driving-car-tests-in-wake-of-fatal-crash]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/self-driving-cars/rss]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/2018/jul/12/uber-scales-back-self-driving-car-tests-in-wake-of-fatal-crash]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Monitoring the self-driving car innovation process: California AV permits</title>
		<link>https://fifthlevel.ai/archives/2472</link>
		<pubDate>Sat, 30 Jun 2018 12:33:35 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.driverless-future.com/?p=1152</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Since 2014, many companies have applied in California for testing self-driving cars. The list of companies which have received a permit can be used as a measure for the innovation process associated with autonomous vehicle technology. The graph below shows how the number of companies active in California has only increased gradually from 2014 to the third quarter of 2016. A steep increase follows in 2017. The slope softens in the first half of 2018.</p>
<p>Of course it would be premature to conclude that we are already seeing the beginning of the end of the S-curve which is so typical for innovation processes. And the California AV permits can only be seen as a proxy for the larger distributed self-driving vehicle innovation process currently unfolding across the world.</p>
<div id="attachment_1156" style="width: 2599px" class="wp-caption alignright"><a href="http://www.driverless-future.com/?attachment_id=1156" rel="attachment wp-att-1156"><img class="size-full wp-image-1156" src="http://www.driverless-future.com/wp-content/uploads/2018/07/California-autonomous-vehicle-permits-innovation-curve.png" alt="Active AV permits by month" width="2589" height="2020" /></a><p class="wp-caption-text">Active California AV permits by month (Data from <a href="https://www.dmv.ca.gov/portal/dmv/detail/vr/autonomous/permit">California DMV</a>) Updated: 2018-09-08</p></div>
<p>But neither the number of California AV permits nor the number of companies providing self-driving vehicle solutions will grow indefinitely. The time will come where the industry moves into the next phase, where the exploratory modes of development will be replaced by more systematic, managerial approaches and where commercialization will be the primary focus. A shakeout is inevitable. Time may be running out for those who still want to jump onto the self-driving car train&#8230;</p>
<p>Notes:<br />
&#8211; 58 permits have been issued by the end of June 2018; One permit has not been renewed (Uber), making it 57 active permits.</p>
<p><strong>Updates:</strong></p>
<p>The graphic is updated from time to time. You may need to <strong>reload your this page</strong> in your browser to view the current version of the graphic.</p>
<p>&#8211; 2018-09-07: By the end of August 2018, Mando America Corporation received a permit, increasing the number of active permits to 56.</p>
<p>&#8211; 2018-08-07: By the end of July 2018, 2 companies did not renew their permits (Wheego Electric and Bauer&#8217;s Intelligent Transportation) reducing the number of active permits to 55.</p> <p><b><a href="http://www.driverless-future.com/?p=1152" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2472</wp:post_id>
		<wp:post_date><![CDATA[2018-06-30 12:33:35]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-30 12:33:35]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[monitoring-the-self-driving-car-innovation-process-california-av-permits]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?feed=rss2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?p=1152]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>AID Selects Cognata as Autonomous Vehicle Simulation Partner</title>
		<link>https://fifthlevel.ai/archives/3004</link>
		<pubDate>Tue, 26 Jun 2018 11:27:24 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.cognata.com/?p=515</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>Cognata’s End-to-End Autonomous Vehicle Simulation Platform Allows AID-Enabled Autonomous Vehicles to Safely Accelerate Time-To-Market</em></p>
<p><strong><em> </em></strong></p>
<p>(Rehovot, Israel – June 26, 2018) Cognata, Ltd. today announced that Autonomous Intelligent Driving GmbH (AID), a wholly-owned subsidiary of AUDI AG, has selected Cognata as its Autonomous Vehicle simulation partner.</p>
<p>AID chose Cognata’s solution, which leverages artificial intelligence, deep learning and computer vision in a highly realistic simulation environment, in order to safely and quickly simulate and validate autonomous vehicles prior to physical roadway tests. Cognata recreates cities from around the world, allowing an expanded range of testing scenarios, including AI-based traffic models simulating real-world traffic conditions. The simulation engine reproduces sensor input by emulating the specific sensors’ interactions with real-world materials.</p>
<p>Through this multi-year partnership with Cognata, AID has chosen to implement a large-scale, cloud-based simulation solution and facilitate thorough, efficient and rapid testing, thereby increasing safety while speeding up time to market for AID-enabled autonomous vehicles. Cognata will provide AID with its end-to-end simulation offering and support the autonomous vehicle’s entire product lifecycle.</p>
<p>“At AID, we are convinced that simulation is a key tool to increase our development speed and a necessary one for the validation of our product and for proving it is safe. After exploring various solutions, we decided that partnering with Cognata is the fastest way to reach these goals,” AID CTO Alex Haag says.</p>
<p>“We are extremely honored to have been selected by AID, a leading player in the autonomous vehicles space. Thorough simulation is a critical and integral part of safely preparing autonomous vehicles for the road, and we look forward to a long-term collaboration with AID.” Cognata CEO Danny Atsmon says. “The combination of the groundbreaking work done by AID and the end-to-end simulation offering from Cognata will safely accelerate commercial deployment of AID-enabled autonomous vehicles.”</p>
<p><strong>About AID</strong></p>
<p>Autonomous Intelligent Driving is building a test fleet of vehicles that are running the latest version of AID self-driving software every day. This enables the AID team to run very agile and hands-on. AID develops the full software stack from AI and Machine Learning for perception and prediction to localization, trajectory planning and interface to sensors and computers. Initially focused on urban environment and mobility services, the AID software will eventually be a standard platform for all vehicles.</p>
<p>Autonomous Intelligent Driving is a wholly owned subsidiary of AUDI AG and is the ‘center of excellence’ for urban autonomous driving in the Volkswagen Group. AID technology will be used in models and brands across the group including VW, Audi and Porsche. For more information visit <u><a href="http://aid-driving.eu/">http://aid-driving.eu/</a></u>.</p>
<p>&nbsp;</p>
<p><img data-attachment-id="520" data-permalink="http://www.cognata.com/aid-selects-cognata-autonomous-vehicle-simulation-partner/logo-aid-autonomous-driving-2-1/" data-orig-file="https://i2.wp.com/www.cognata.com/wp-content/uploads/2018/06/LOGO-AID-autonomous-driving-2-1.png?fit=2191%2C536" data-orig-size="2191,536" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="LOGO-AID-autonomous-driving-2 (1)" data-image-description="" data-medium-file="https://i2.wp.com/www.cognata.com/wp-content/uploads/2018/06/LOGO-AID-autonomous-driving-2-1.png?fit=300%2C73" data-large-file="https://i2.wp.com/www.cognata.com/wp-content/uploads/2018/06/LOGO-AID-autonomous-driving-2-1.png?fit=1024%2C251" class="size-medium wp-image-520 alignleft" src="https://i2.wp.com/www.cognata.com/wp-content/uploads/2018/06/LOGO-AID-autonomous-driving-2-1.png?resize=300%2C73" alt="" width="300" height="73" srcset="https://i2.wp.com/www.cognata.com/wp-content/uploads/2018/06/LOGO-AID-autonomous-driving-2-1.png?resize=300%2C73 300w, https://i2.wp.com/www.cognata.com/wp-content/uploads/2018/06/LOGO-AID-autonomous-driving-2-1.png?resize=768%2C188 768w, https://i2.wp.com/www.cognata.com/wp-content/uploads/2018/06/LOGO-AID-autonomous-driving-2-1.png?resize=1024%2C251 1024w, https://i2.wp.com/www.cognata.com/wp-content/uploads/2018/06/LOGO-AID-autonomous-driving-2-1.png?w=2000 2000w" sizes="(max-width: 300px) 100vw, 300px" data-recalc-dims="1" /></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><strong>About Cognata</strong></p>
<p>Cognata provides a fast lane to autonomous driving with its testing and evaluation solution for self-driving vehicles—a realistic automotive simulation platform where virtual cars travel virtual roads in virtual cities, all remarkably true to real-world conditions. Led by CEO Danny Atsmon, a widely respected expert in ADAS and deep learning, Cognata brings the disruptive potential of artificial intelligence, deep learning, and computer vision to the autonomous driving simulation world. Cognata’s simulated testing and evaluation environment shaves years off the validation time by generating fast, highly accurate results, and eliminates the safety concerns, high costs, and limited scalability of road-testing in the physical world. Cognata was founded in 2016 by a team of experts in deep learning, autonomous vehicles and computer vision. The company is headquartered in Rehovot, Israel, close to the Weizmann Institute of Science. For more information, visit <u><a href="http://www.cognata.com/">http://www.cognata.com</a></u>.</p>
<p>The post <a rel="nofollow" href="http://www.cognata.com/aid-selects-cognata-autonomous-vehicle-simulation-partner/">AID Selects Cognata as Autonomous Vehicle Simulation Partner</a> appeared first on <a rel="nofollow" href="http://www.cognata.com">Cognata Autonomous Simulation</a>.</p> <p><b><a href="http://www.cognata.com/aid-selects-cognata-autonomous-vehicle-simulation-partner/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3004</wp:post_id>
		<wp:post_date><![CDATA[2018-06-26 11:27:24]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-26 11:27:24]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[aid-selects-cognata-as-autonomous-vehicle-simulation-partner]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.cognata.com/blog/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.cognata.com/aid-selects-cognata-autonomous-vehicle-simulation-partner/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The Half Way Point</title>
		<link>https://fifthlevel.ai/archives/3254</link>
		<pubDate>Fri, 13 Jul 2018 13:36:42 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/55662cef04f2</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The truth about the overly hyped “self driving cars” is this: they don’t really exist, and when they do exist the service will be a slower Uber. They will drive the speed limit and yield to everyone. Oh and if you’re thinking at least it’ll be cheaper ride sharing, not likely until there’s competition. Self driving cars are a scam. One look at Waymo’s cringeworthy advertising makes that obvious.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/577/1*tkRJDpszqD-1XpAuehJWmw.png" /><figcaption>No real person has ever been this happy in the back of a minivan</figcaption></figure><p>Back in reality, the average American commute is 25.4 minutes. It’s mostly on the freeway. And it’s mostly unpleasant. This is what we can fix, and it is the path to practical self driving cars.</p><p>Our plan is <a href="https://medium.com/@comma_ai/our-road-to-self-driving-victory-603a9ed20204">what it was a year ago</a>. Everything takes a long time. But we are doing it. And things are coming along.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bUPSTsCJ0xwo1o-HHW_lEQ.png" /><figcaption>Our hardware is sort of beautiful now.</figcaption></figure><p><a href="https://github.com/commaai/openpilot">openpilot</a> is making less mistakes than ever. It’s gotten so good on highways, we’ve felt it necessary to <a href="https://medium.com/@comma_ai/safety-and-driver-attention-2a33d3d23109">add driver monitoring</a> to make sure people pay attention! It’s good, but alone, no software is as good as you.</p><p>Unlike competing products Autopilot and Super Cruise, openpilot works on cars people actually own. We support <a href="https://github.com/commaai/openpilot#supported-cars">almost all new Honda and Toyota</a>. 5 out of the top 10 cars sold. And since <a href="https://github.com/commaai/openpilot">openpilot is open source</a>, we’ve created <a href="https://medium.com/@comma_ai/openpilot-port-guide-for-toyota-models-e5467f4b5fe6">a guide to help you port your car</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LzOPhwCPfkbZaVjewxxvOA.jpeg" /><figcaption>Still the best car interface in the world. Now on Amazon!</figcaption></figure><p>To help you add your car to openpilot, we’ve brought together a <a href="http://slack.comma.ai">vibrant community</a> of car hackers and made <a href="https://medium.com/@comma_ai/a-panda-and-a-cabana-how-to-get-started-car-hacking-with-comma-ai-b5e46fae8646">the best car hacking tools</a> to ever exist. <a href="https://www.amazon.com/chffr-panda-OBD-II-Interface/dp/B07D6Y3GN2/">panda even made its way to Amazon</a>. As a community, we’ve reverse engineered the protocols for almost every car maker as part of the <a href="https://github.com/commaai/opendbc">opendbc project</a>.</p><p>But that’s all the past.</p><h3>The future — openpilot 0.5</h3><p>This week marks the release of a <a href="https://github.com/commaai/openpilot">new major version of openpilot</a>. It includes optional driver monitoring!</p><p>If you use it, you’ll no longer have the 6 minute timer. Instead, the system will track if your head is facing the road.</p><p>Waze and Spotify have been removed to free up resources for larger models and precise localization. And from here on out, we’ll be carefully tracking mistakes the system makes and crowdsourcing corrections using…</p><h3>comma.ai explorer</h3><p>In order to get to the point where you can nap and <a href="https://twitter.com/comma_ai">use Twitter</a> during your commute, we need your help. To make the system good enough, we need to drive disengagements to zero.</p><p>A “disengagement” is when you need to take over because the car made a mistake. When you drive with openpilot, you are contributing every time you correct it. And by crowdsourcing annotations for these corrections, we can figure out the mistakes our cars are making and fix them in the next version.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4mHE-8vG7l4hHIVAZkjbLw.png" /><figcaption>comma.ai explorer</figcaption></figure><p>If you have an EON, a grey panda, and an officially supported car by openpilot, you’ll see this interface. The top “timeline” shows green when you were engaged.</p><p>On the left, you’ll see all the disengagements from your trip. You’ll be asked to select a reason.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/886/1*XnWdtrRRa1ByXSPj_8s54A.png" /></figure><p>Our team gets this data and feeds it into the machine learning, and openpilot gets better.</p><p>Existing comma users, you’ll find explorer at <a href="https://my.comma.ai/">https://my.comma.ai/</a></p><h3>Call to Action</h3><ol><li><a href="https://twitter.com/comma_ai">Follow us on Twitter</a> (you should already be doing this)</li><li>Buy the required hardware from <a href="https://comma.ai/shop/">our lovely little shop</a>. (reminder: that’s an EON, a grey panda, and the appropriate giraffe for your car)</li><li>When it arrives, download and install <a href="https://github.com/commaai/openpilot">the latest openpilot software</a>. (free and open source, no warranty!)</li><li><a href="http://slack.comma.ai/">Join our slack</a> when you inevitability run into problems. The people there are very nice. Celebrate when it all works.</li><li>Drive. Commute. Go to taco bell drive thru!</li><li>Annotate your drives on the <a href="https://my.comma.ai">comma.ai explorer</a>. Watch as openpilot gets better at your commute with every update.</li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=55662cef04f2" width="1" height="1"> <p><b><a href="https://medium.com/@comma_ai/the-half-way-point-55662cef04f2?source=rss-330bac69b283------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3254</wp:post_id>
		<wp:post_date><![CDATA[2018-07-13 13:36:42]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-13 13:36:42]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[the-half-way-point]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@comma_ai/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@comma_ai/the-half-way-point-55662cef04f2?source=rss-330bac69b283------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>I think GIL should mostly impact CPU-intensive operations. IO blocking may release GIL.</title>
		<link>https://fifthlevel.ai/archives/3586</link>
		<pubDate>Tue, 17 Jul 2018 16:43:24 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/2acc8c01e8f6</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I think GIL should mostly impact CPU-intensive operations. IO blocking may release GIL.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2acc8c01e8f6" width="1" height="1"> <p><b><a href="https://medium.com/@surmenok/i-think-gil-should-mostly-impact-cpu-intensive-operations-io-blocking-may-release-gil-2acc8c01e8f6?source=rss-d002f056f8c------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3586</wp:post_id>
		<wp:post_date><![CDATA[2018-07-17 16:43:24]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-17 16:43:24]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[i-think-gil-should-mostly-impact-cpu-intensive-operations-io-blocking-may-release-gil]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@surmenok]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@surmenok/i-think-gil-should-mostly-impact-cpu-intensive-operations-io-blocking-may-release-gil-2acc8c01e8f6?source=rss-d002f056f8c------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Probably you are missing some import statement.</title>
		<link>https://fifthlevel.ai/archives/3587</link>
		<pubDate>Mon, 16 Jul 2018 06:12:03 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/a8bd6809fe0</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Probably you are missing some import statement. tfms_from_model function is declared in transforms.py: <a href="https://github.com/fastai/fastai/blob/master/fastai/transforms.py">https://github.com/fastai/fastai/blob/master/fastai/transforms.py</a></p><p>Dataset loader assumes that there is a “test” folder inside the dataset path. That folder should contain images for the test set.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a8bd6809fe0" width="1" height="1"> <p><b><a href="https://medium.com/@surmenok/probably-you-are-missing-some-import-statement-a8bd6809fe0?source=rss-d002f056f8c------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3587</wp:post_id>
		<wp:post_date><![CDATA[2018-07-16 06:12:03]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-16 06:12:03]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[probably-you-are-missing-some-import-statement]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@surmenok]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@surmenok/probably-you-are-missing-some-import-statement-a8bd6809fe0?source=rss-d002f056f8c------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Becoming the Industry Standard</title>
		<link>https://fifthlevel.ai/archives/141</link>
		<pubDate>Mon, 30 Jul 2018 15:01:43 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/af16e0996279</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ctNMYIcw4m_ScZujMTUI5Q.png" /></figure><p>When we started the <a href="https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013?utm_source=medium&amp;utm_medium=content&amp;utm_campaign=david_silver_blog">Udacity Self-Driving Car Engineer Nanodegree Program</a>, our goal was to become the industry standard for training self-driving car engineers.</p><p>We thought that if we could give people the best education available anywhere, that those students would form the next generation of autonomous vehicle engineers. And they would go out into the industry, and look back to Udacity as a source of talent to hire from themselves.</p><p>At that point, the industry would be open to anybody, anywhere, with the desire and passion to learn how self-driving cars work.</p><p>Our partners at <a href="http://www2.mazda.com/en/publicity/release/2017/201708/170808a.html">Mazda</a> just sent over a <a href="https://js01.jposting.net/mazda/u/job.phtml?job_code=299">job description</a> that includes the “Self-Driving Car Nanodegree” as a qualification, and it makes me feel like we’re getting closer to that goal.</p><p>The <a href="https://js01.jposting.net/mazda/u/job.phtml?job_code=299">original job description</a> is in Japanese. Here is the <a href="https://translate.google.com/translate?sl=auto&amp;tl=en&amp;js=y&amp;prev=_t&amp;hl=en&amp;ie=UTF-8&amp;u=https%3A%2F%2Fjs01.jposting.net%2Fmazda%2Fu%2Fjob.phtml%3Fjob_code%3D299&amp;edit-text=">English translation</a>.</p><p>If you have joined the Nanodegree Program, then you should <a href="https://js01.jposting.net/mazda/u/job.phtml?job_code=299">apply to join the Mazda Co-Pilot team!</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=af16e0996279" width="1" height="1"><hr><p><a href="https://medium.com/self-driving-cars/becoming-the-industry-standard-af16e0996279">Becoming the Industry Standard</a> was originally published in <a href="https://medium.com/self-driving-cars">Self-Driving Cars</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>141</wp:post_id>
		<wp:post_date><![CDATA[2018-07-30 15:01:43]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-30 15:01:43]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[becoming-the-industry-standard]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driving-cars]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driving-cars/becoming-the-industry-standard-af16e0996279?source=rss----bdc6e635b3c0---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[854]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Self-Driving Cars Webinar Tomorrow</title>
		<link>https://fifthlevel.ai/archives/142</link>
		<pubDate>Mon, 23 Jul 2018 22:23:03 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/609f741ded78</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aX7l2-SqNffG6lAU2NsGUg.jpeg" /></figure><p>Tomorrow I will be hosting a webinar about how self-driving cars work, how they will change the world, and how you can get a job as a self-driving car engineer!</p><p>Come for my charm and good looks, stay for the employment opportunities.</p><p>It’s free!</p><p>Tuesday, July 24th, 9am Pacific Daylight Time. Register below.</p><p><a href="https://www.eventbrite.com/e/intro-to-self-driving-cars-tickets-47934161367">https://www.eventbrite.com/e/intro-to-self-driving-cars-tickets-47934161367</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=609f741ded78" width="1" height="1"><hr><p><a href="https://medium.com/self-driving-cars/self-driving-cars-webinar-tomorrow-609f741ded78">Self-Driving Cars Webinar Tomorrow</a> was originally published in <a href="https://medium.com/self-driving-cars">Self-Driving Cars</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>142</wp:post_id>
		<wp:post_date><![CDATA[2018-07-23 22:23:03]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-23 22:23:03]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[self-driving-cars-webinar-tomorrow]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driving-cars]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driving-cars/self-driving-cars-webinar-tomorrow-609f741ded78?source=rss----bdc6e635b3c0---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[848]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Partnering with Valley Metro to explore public transportation solutions</title>
		<link>https://fifthlevel.ai/archives/251</link>
		<pubDate>Tue, 31 Jul 2018 16:30:14 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/ff01ae36484d</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZHLyIt2BI8N0Ang7bKewSA.jpeg" /></figure><p>Public transportation is an integral part of our cities, providing vital social, economic, and environmental benefits. That’s why cities around the world invest significant resources to build and maintain public infrastructure — light rail, trains, and buses — that help people commute and move around. However, as cities grow and evolve, the “last mile” — how people connect to public transportation efficiently, affordably, and safely — is one of the main challenges communities struggle to solve.</p><p>Waymo is partnering with Valley Metro, the Phoenix area’s regional public transportation authority, to explore mobility solutions that use self-driving technology to better connect travelers with the city’s existing buses and light rail.</p><p>Working together, we want to explore how self-driving vehicles could fill transportation and mobility gaps for riders across the Greater Phoenix area.</p><p><strong>Testing a solution</strong></p><p>The first phase of this partnership, which we plan to launch in August, will offer first- and last-mile transit connections for Valley Metro employees, helping to connect them with public transportation. These riders will be able to use the Waymo app to hail a ride to take them to their nearest public transportation option.</p><p>We’ll then expand the partnership by providing first- and last-mile travel to Valley Metro RideChoice travelers, which cover groups traditionally underserved by public transit. This will form the basis of joint research to evaluate the adoption of Waymo technology, its impact, and its long-term potential to enable greater access to public transit.</p><p>Waymo hopes to open this service to the public to transport more people to Valley Metro transportation hubs in the Phoenix area in the future.</p><p><strong>Another step forward</strong></p><p>For nearly a decade, Waymo has been working to build the world’s most experienced driver ― one that can make it safe and easy for people and things to move around. Our goal is to focus on four areas: creating a ride-hailing service, developing self-driving trucks for logistics, licensing with OEMs for personally-owned vehicles, and connecting people to public transportation.</p><p>So far in 2018, we’ve made progress on the other three pillars. We <a href="https://medium.com/waymo/same-driver-different-vehicle-bringing-waymo-self-driving-technology-to-trucks-e55824b55b8f">announced</a> a logistics pilot that put self-driving trucks on roads in Atlanta; cemented partnership deals with <a href="https://medium.com/waymo/meet-our-newest-self-driving-vehicle-the-all-electric-jaguar-i-pace-375cecc70eb8">Jaguar Land Rover</a> and Fiat Chrysler Automobiles for vehicles to support the launch and expansion of our ride-hailing service; and began discussions with FCA around licensing for personally-owned vehicle</p><p>Today’s announcement is an important step toward the fourth area: connecting people to public transportation. As always, Waymo’s goal is to be an enabler rather than a disruptor: to help everyone better utilize the investments and infrastructure that already exist today.</p><p>Waymo is honored to be involved in the exploration of new transportation solutions in Greater Phoenix and to see the future of transportation take shape here. In the meantime, we’ll keep working to make it safe and easy for everyone to get around.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ff01ae36484d" width="1" height="1"><hr><p><a href="https://medium.com/waymo/partnering-with-valley-metro-to-explore-public-transportation-solutions-ff01ae36484d">Partnering with Valley Metro to explore public transportation solutions</a> was originally published in <a href="https://medium.com/waymo">Waymo</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>251</wp:post_id>
		<wp:post_date><![CDATA[2018-07-31 16:30:14]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-31 16:30:14]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[partnering-with-valley-metro-to-explore-public-transportation-solutions]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/waymo]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/waymo/partnering-with-valley-metro-to-explore-public-transportation-solutions-ff01ae36484d?source=rss----7075a35566d9---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[855]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Growing Waymo’s Partnerships in Metro Phoenix</title>
		<link>https://fifthlevel.ai/archives/252</link>
		<pubDate>Wed, 25 Jul 2018 12:00:44 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/965941451902</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*CRUCy7c8CEkWXDyxXxbWQA.jpeg" /></figure><p>Today, our <a href="https://medium.com/waymo/waymos-early-rider-program-one-year-in-3a788f995a9c">early rider program</a> is helping us build the world’s most experienced driver and a service that is letting people take a self-driving car to the places they go daily. We’ve been busy teaming up with partners this summer to create new experiences, and provide unique value for our riders. We know from our early riders that most of their rides are to run errands, shop for groceries, commute to work, head to dinner or fix their personal vehicles. We’ve tailored our partnerships to meet the top rider needs; in fact, the partnerships below represent eight of the <a href="https://medium.com/waymo/waymos-early-rider-program-one-year-in-3a788f995a9c">top ten</a> activities our riders do when they get in a Waymo.</p><p>Riders spend a significant portion of time each week running errands and shopping. That’s why we’re launching two pilots with Walmart and DDR Corp., to make shopping more convenient.</p><p>Later this week, Walmart and Waymo will launch a test pilot that gives early riders savings on groceries each week when they are ordered on Walmart.com. While orders are being prepared at the store, Waymo vehicles will transport the rider to and from Walmart to collect their groceries.</p><p>For those who want to visit Ahwatukee Foothills Towne Center in Chandler, DDR will offer shoppers and diners rides in our self-driving vehicles, letting them avoid the stress of parking lots.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*b_cNtz-a71z-e4b0eosJGw.jpeg" /></figure><p>Our self-driving cars will also become an integral part of the VIP experience for visitors in Phoenix. Waymo and the Element Hotel in Chandler have teamed up to give select guests access to Waymo vehicles, for example, business travelers who need to commute to and from the office during their frequent stays, giving them a chance to experience fully self-driving technology.</p><p>Finally, we are expanding our existing partnerships with AutoNation and Avis Budget Group. AutoNation, who already helps Waymo service and maintain our vehicles in Phoenix, is now making it possible for their customers to ride with Waymo technology; while customers are having their personal vehicles serviced, AutoNation will now offer them a Waymo, rather than a loaner car, to get around. Avis Budget Group, who makes sure our vehicles are charged, refueled, and presentable for riders, will also soon provide Waymos as a last mile solution for Avis customers in Phoenix to help them pick up or drop off their rental cars, beginning with their two Chandler locations. Avis and AutoNation are ideal partners, providing different and complementary support and maintenance services to our growing fleet of self-driving cars in Phoenix over the past year and ensuring they are always ready to pick up a rider.</p><p>While these are Metro Phoenix-specific partnerships today, these businesses are national and what we learn from these programs will give us a network of partners when we launch in new cities down the road. We’re proud to be a part of Metro Phoenix and are excited to grow and add partnerships that support the cities we operate in, bring unique value to our riders, and give more people access to a safe, self-driving future.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=965941451902" width="1" height="1"><hr><p><a href="https://medium.com/waymo/growing-waymos-partnerships-in-metro-phoenix-965941451902">Growing Waymo’s Partnerships in Metro Phoenix</a> was originally published in <a href="https://medium.com/waymo">Waymo</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>252</wp:post_id>
		<wp:post_date><![CDATA[2018-07-25 12:00:44]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-25 12:00:44]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[growing-waymos-partnerships-in-metro-phoenix]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/waymo]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/waymo/growing-waymos-partnerships-in-metro-phoenix-965941451902?source=rss----7075a35566d9---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[849]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Here We Go, Frisco:</title>
		<link>https://fifthlevel.ai/archives/369</link>
		<pubDate>Mon, 30 Jul 2018 12:01:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/a2eafb6d1d65</guid>
		<description></description>
		<content:encoded><![CDATA[<p><strong>Our Self-Driving Service is Live, With One Million Simulated Miles Under Our Belt</strong></p><p><strong>Time to Ride</strong></p><p>We <a href="https://medium.com/@drive.ai/self-driving-cars-are-here-drive-ai-is-launching-in-texas-e095668854ae">announced the news</a> of our self-driving service back in May, and now the time has arrived: today the pilot is launching in Frisco, Texas. At Drive.ai, our mission is to create self-driving technology that solves people’s transportation problems — today. That’s why we’ve been collaborating with Frisco TMA to design a program that serves the community and brings them to and from the places they want to go. Rather than creating tech for tech’s sake, we’re working to help people get where they need to go in a way that’s safe, efficient, and enjoyable to experience.</p><p>In April 2018, we touched down in Texas and began driving along the route in Frisco, collecting data to inform our deep learning AI systems, documenting the scenarios we encounter, and creating custom simulations to improve the way our vehicles perform on the roads. In the last four months, we’ve been driving the streets of our geo-fenced route in Frisco, which crosses six lanes of traffic, takes us through parking lots, and involves pulling over to pick up and drop off passengers on-demand. We’ve been focused on creating a great user experience for those who will be hailing and experiencing these rides, and have been working with the city to educate the community and hear what people have to say.</p><p>Our focus is on solving a mobility challenge, and doing so safely. To that end, in addition to our “real world” logged miles, we’ve racked up over one million simulated miles on our Frisco route. For us, the one million marker is more than a milestone; it’s a number that reinforces our ability to deliver a safe, thoroughly-tested service to the public.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZujH9rHI7fqunsZq" /></figure><p><strong>Seeing It All With Simulation</strong></p><p>Simulations are a major component of our approach to self-driving, as running comprehensive simulations improves our vehicles’ ability to foresee and handle a wider variety of driving scenarios — those that are common, and those that are less so. This is critical to ensuring the depth of our vehicles’ understanding of the world around them, and in turn, the safety of our passengers, other drivers, cyclists, and pedestrians who share the roads with us.</p><p>When we create simulations, we do so in two ways: by replicating scenarios we’ve encountered in the “real world,” and by creating our own.</p><p>As we examine our driving logs from autonomous rides on our routes, we can choose to convert the “real world data” into simulations that we want to examine more closely. It’s like a high tech version of SimCity, where we design the world, and can then replay events and modify their components to explore how our technology responds in unique scenarios. This is a good place to start for the more common things that people do on the roads: navigating tricky intersections, right-of-way decisions, and observing the behaviors of cyclists and pedestrians.</p><p>On the other hand, we can also create original simulations that introduce some of the more unusual situations that we want our vehicles to be prepared to handle. In this way, we can specifically craft unusual situations for our vehicles, like piloting around cars that are double parked, or carefully steering through tight turns. By deliberately exposing our technology to “stress tests” like these, we are able to ensure a level of competency in complicated circumstances we may not encounter organically.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/0*ADEjpk-aZsXu01G-" /></figure><p><strong>The Human Factor</strong></p><p>With our self-driving program officially launching in Frisco, safety and trust are our top concerns. Our vehicles drive like the most respectful, by-the-book drivers out there; and remarkably uneventful rides are exactly what we’re going for, every time. While our vehicles are designed to carefully heed all traffic rules, the fact of the matter is that we can’t control how other drivers behave.</p><p>So, how do we ensure that our vehicles still operate as safely as possible even when the world around them may not be as predictable? That’s where simulation steps up, helping us create scenarios that include ‘unlikely’ human behaviors — like people darting across the road or objects rolling into the road, driving through crowded or sparsely populated streets, navigating around poorly parked cars, or riding alongside a cyclist for an extended period of time. By introducing “dynamic agents” such as these, we have expanded the scope of scenarios our vehicles might experience and can confidently address.</p><p>Within these scenarios, we can then get even more specific: adjusting various parameters to new sizes and shapes, to see how our cars will react if things were slightly different. For example, we might change the angle of a double-parked car, or the size of a left-hand turning lane. We test our simulations to exhaustive detail, so that our vehicles know how to approach these variable driving conditions and situations, in every combination that could be possible.</p><p><strong>One Million Down, Many More to Come</strong></p><p>We’re launching our on-demand Frisco service to provide a convenient option for people looking to easily get from point A to point B, and we want our riders — and all others on the roads — to have the utmost confidence and comfort in our vehicles. That’s why we’ve prioritized our vehicles’ ability to operate safely in simulated settings, which improves their performance on the roads each day.</p><p>While we have logged one million simulated miles on the streets of Frisco, we’re not stopping there. With each mile driven — both on the streets and in simulations — we will continue to improve our technology. Today is the exciting start to our on-demand ride service, which we hope will benefit a community and increase understanding of and confidence in self-driving technology.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a2eafb6d1d65" width="1" height="1"> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>369</wp:post_id>
		<wp:post_date><![CDATA[2018-07-30 12:01:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-30 12:01:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[here-we-go-frisco]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@drive.ai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@drive.ai/here-we-go-frisco-a2eafb6d1d65?source=rss-37851f177ff8------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[886]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Elon Musk Says Tesla Has A Blazingly Fast Onboard Computer To Aid Autopilot</title>
		<link>https://fifthlevel.ai/archives/461</link>
		<pubDate>Fri, 03 Aug 2018 05:31:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b63d99c64aaf94bc0d832e6</guid>
		<description></description>
		<content:encoded><![CDATA[Tesla has built its own computer that is ten times faster than anybody else's. And they might be betting their future on it. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>461</wp:post_id>
		<wp:post_date><![CDATA[2018-08-03 05:31:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-03 05:31:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[elon-musk-says-tesla-has-a-blazingly-fast-onboard-computer-to-aid-autopilot]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/08/03/elon-musk-says-teslas-onboard-computer-is-blazingly-fast/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/08/03/elon-musk-says-teslas-onboard-computer-is-blazingly-fast/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_status]]></wp:meta_key>
			<wp:meta_value><![CDATA[draft]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_time]]></wp:meta_key>
			<wp:meta_value><![CDATA[1538399801]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_desired_post_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[elon-musk-says-tesla-has-a-blazingly-fast-onboard-computer-to-aid-autopilot]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/08/03/elon-musk-says-teslas-onboard-computer-is-blazingly-fast/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_status]]></wp:meta_key>
			<wp:meta_value><![CDATA[draft]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_time]]></wp:meta_key>
			<wp:meta_value><![CDATA[1538582980]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_desired_post_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[elon-musk-says-tesla-has-a-blazingly-fast-onboard-computer-to-aid-autopilot]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Ford Launches Subsidiary To Lead Autonomous Vehicles</title>
		<link>https://fifthlevel.ai/archives/462</link>
		<pubDate>Mon, 30 Jul 2018 22:59:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b5f2afb64aaf976f615eee2</guid>
		<description></description>
		<content:encoded><![CDATA[Ford Motor Company is forming a new entity, Ford Autonomous Vehicles LLC, to run its autonomous vehicle business and technology. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>462</wp:post_id>
		<wp:post_date><![CDATA[2018-07-30 22:59:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-30 22:59:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[ford-launches-subsidiary-to-lead-autonomous-vehicles]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/07/30/ford-launches-subsidiary-to-lead-autonomous-vehicles/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/07/30/ford-launches-subsidiary-to-lead-autonomous-vehicles/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_status]]></wp:meta_key>
			<wp:meta_value><![CDATA[draft]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_time]]></wp:meta_key>
			<wp:meta_value><![CDATA[1538399802]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_desired_post_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[ford-launches-subsidiary-to-lead-autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/07/30/ford-launches-subsidiary-to-lead-autonomous-vehicles/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_status]]></wp:meta_key>
			<wp:meta_value><![CDATA[draft]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_time]]></wp:meta_key>
			<wp:meta_value><![CDATA[1538582980]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_desired_post_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[ford-launches-subsidiary-to-lead-autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The Top 10 Places Self-Driving Car Passengers Go</title>
		<link>https://fifthlevel.ai/archives/463</link>
		<pubDate>Sun, 29 Jul 2018 02:50:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b5d1cea64aaf976f615eae3</guid>
		<description></description>
		<content:encoded><![CDATA[The top ten trips for early riders in the Waymo self-driving car program are both expected and surprising. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>463</wp:post_id>
		<wp:post_date><![CDATA[2018-07-29 02:50:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-29 02:50:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[the-top-10-places-self-driving-car-passengers-go]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/07/28/the-top-10-places-self-driving-car-passengers-go/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/07/28/the-top-10-places-self-driving-car-passengers-go/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_status]]></wp:meta_key>
			<wp:meta_value><![CDATA[draft]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_time]]></wp:meta_key>
			<wp:meta_value><![CDATA[1538399802]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_desired_post_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[the-top-10-places-self-driving-car-passengers-go]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/07/28/the-top-10-places-self-driving-car-passengers-go/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_status]]></wp:meta_key>
			<wp:meta_value><![CDATA[draft]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_time]]></wp:meta_key>
			<wp:meta_value><![CDATA[1538582980]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_desired_post_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[the-top-10-places-self-driving-car-passengers-go]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Waymo Has The Most Autonomous Miles, By A Lot</title>
		<link>https://fifthlevel.ai/archives/464</link>
		<pubDate>Thu, 26 Jul 2018 17:43:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b58e9f664aaf976f615e0bf</guid>
		<description></description>
		<content:encoded><![CDATA[Waymo has driven 8 million miles autonomously and is pulling further ahead of competitors with each passing day. But there is a long way to go before any company wins the self-driving race. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>464</wp:post_id>
		<wp:post_date><![CDATA[2018-07-26 17:43:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-26 17:43:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[waymo-has-the-most-autonomous-miles-by-a-lot]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/07/26/waymo-has-the-most-autonomous-miles-by-a-lot/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/07/26/waymo-has-the-most-autonomous-miles-by-a-lot/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_status]]></wp:meta_key>
			<wp:meta_value><![CDATA[draft]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_time]]></wp:meta_key>
			<wp:meta_value><![CDATA[1538399802]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_desired_post_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[waymo-has-the-most-autonomous-miles-by-a-lot]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/07/26/waymo-has-the-most-autonomous-miles-by-a-lot/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_status]]></wp:meta_key>
			<wp:meta_value><![CDATA[draft]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_time]]></wp:meta_key>
			<wp:meta_value><![CDATA[1538582980]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_desired_post_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[waymo-has-the-most-autonomous-miles-by-a-lot]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Self-Driving Vehicles Will Look Different And Exciting</title>
		<link>https://fifthlevel.ai/archives/465</link>
		<pubDate>Fri, 20 Jul 2018 17:07:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b511f1a64aaf9287cf3cc43</guid>
		<description></description>
		<content:encoded><![CDATA[Self-driving vehicles will look different than previous generations of vehicles. The new functions of autonomous vehicles will bring whole new form factors into play. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>465</wp:post_id>
		<wp:post_date><![CDATA[2018-07-20 17:07:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-20 17:07:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[self-driving-vehicles-will-look-different-and-exciting]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/07/20/self-driving-vehicles-will-look-different-and-exciting/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/07/20/self-driving-vehicles-will-look-different-and-exciting/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_status]]></wp:meta_key>
			<wp:meta_value><![CDATA[draft]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_time]]></wp:meta_key>
			<wp:meta_value><![CDATA[1538399802]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_desired_post_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[self-driving-vehicles-will-look-different-and-exciting]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/07/20/self-driving-vehicles-will-look-different-and-exciting/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_status]]></wp:meta_key>
			<wp:meta_value><![CDATA[draft]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_trash_meta_time]]></wp:meta_key>
			<wp:meta_value><![CDATA[1538582980]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_desired_post_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[self-driving-vehicles-will-look-different-and-exciting]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The Way We See It</title>
		<link>https://fifthlevel.ai/archives/486</link>
		<pubDate>Thu, 26 Jul 2018 17:00:20 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/bd4458fc6809</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Using visualization for analysis, simulation, and training in self-driving AI systems</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fGNsme5RI4STznbHeLRgpQ.png" /></figure><p>At Drive.ai, we’re using deep learning to build the safest, smartest self-driving vehicles possible. We want to solve the transportation challenges facing communities today. To do this, we plan to deploy our technology with a growing fleet of self-driving vehicles in a variety of locations and conditions. Previously, we shared key <a href="https://medium.com/@drive.ai/what-we-learned-driving-an-autonomous-vehicle-for-24-hours-straight-587defe151bd">operational learnings</a> from this fleet and announced <a href="https://medium.com/@drive.ai/self-driving-cars-are-here-drive-ai-is-launching-in-texas-e095668854ae">our first deployment</a> in Frisco, Texas.</p><p>Our self-driving system generates incredible amounts of compelling data, which our engineering team uses to iterate and improve our system’s safety, performance, and capabilities. Visualization is one of the most important tools to understand data, and we invest in it heavily.</p><p>Here’s an inside look at some of our primary visualizations and how we use them in on-board displays, off-board analysis, annotation tools, and simulation.</p><p><strong>On-board displays</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*Khlm_JRfEZOM1m1aFR1XtA.gif" /><figcaption><em>An early iteration of the visualization used for passenger interfaces (3x playback)</em></figcaption></figure><p>On-board visualization is important for passengers inside the car. These displays allow passengers to understand exactly what the car is seeing, thinking, and planning to do as it drives. Displaying a simple visualization helps passengers feel more comfortable.</p><p>On-board displays show sensor data in a way that is intuitive and easy to understand. We create a full-surround 3D picture of the environment using “point clouds” derived from lidar sensors and combined with rich video data about the scene, which is collected from full-surround cameras. By combining both sensors, along with others elements such as radar, GPS, and a device called an inertial measurement unit (IMU), the displays show how the car understands what is happening in the environment around it.</p><p><strong>Off-board analysis</strong></p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FtuEyKT2H-6Y%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DtuEyKT2H-6Y&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FtuEyKT2H-6Y%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/404fb1b1a0edefa61609e2a4b44b5c35/href">https://medium.com/media/404fb1b1a0edefa61609e2a4b44b5c35/href</a></iframe><p>While the on-board system comprises the AI doing the driving, there are even more components in our off-board system. Our AI system is composed of a complete robotics stack, including: sensor calibration, mapping, localization and state estimation, perception, motion planning, and controls.</p><p>Each component depends on many other components and the real-time data produced by them. These components need to be developed, trained, and thoroughly tested. We use 3D visualization tools to replay data collected from human-driven and AI-driven rides. Similar to using pause, rewind, and fast forward to skip ahead in a video, we play and review our driving data logs meticulously. Time synchronization is key when combining multiple lidar and camera streams, as well as localization reports, object detections, and motion plans. We draw all of the data in a coherent 3D world, and align it with our HD maps (like street maps but in 3D) and road networks. These visualizations provide us the tools we need to ask and answer questions about the data.</p><p><strong>Annotating massive datasets</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*_l7Mt0dYAbGirMXImFVpUw.gif" /><figcaption><em>Ground truth object annotations visualized with point cloud data</em></figcaption></figure><p>Annotation is the process of labeling all of the data collected on the road in order for our systems to understand what they are seeing, and ensure they learn how to discern and differentiate stimuli. Annotation is a critical part of the self-driving cycle. For every one hour driven, it takes approximately <a href="https://arxiv.org/pdf/1709.03553.pdf">800 human hours</a> to label it. But we want to annotate much faster than this — and at 100% accuracy.</p><p>To accomplish this, we use deep-learning-enhanced automation, enabling us to annotate data in a faster, more reliable way. We begin with a team of human annotators who do the first iteration — identifying objects like trees, cars, pedestrians, and bicyclists seen on the roads by our vehicles and sensor kits — and couple this with various engineering optimizations. We prioritize UI and workflow efficiency for our annotation team, building in features such as easily “scrubbing” backwards or forwards through time by dragging the cursor, and automatic interpolation of data between frames. Efficiencies such as these improve the rate and accuracy of the data annotation process.</p><p>As we continue to expand and train our vehicles on new routes, scaling the annotation process with absolute precision is of utmost importance. By using human input to corroborate our AI system’s decisions, we are iteratively improving the deep-learning system, which in turn, starts to learn and operate at higher speeds and with larger data sets.</p><p><strong>The spectrum of simulation</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/453/1*3Dk8rb3Y2YoQ2knzeYJf0A.gif" /><figcaption><em>Simulating a complex pedestrian scene using real-world data</em></figcaption></figure><p>Simulation is a critical part of validating the safety and performance of our self-driving system. We use our simulator as an interactive tool to continuously test and evaluate our systems. The simulator provides a virtual environment to stand in for the real world. Engineers interact with the simulator through a user interface with a 3D visualization of the world. They can modify the simulated “world” however they want — like switching the state of traffic lights, positioning other vehicles on the roads, or creating algorithmically-controlled dynamic agents like pedestrians or cars — and watch how our self-driving AI responds.</p><p>We’ve built massive libraries of scenarios automatically from the driving data we’ve collected, as well as from modifying different parameters carefully in code to cover a broad distribution of events. Through these methods, we can evaluate our self-driving system on both common and edge case scenarios. Visualization tools enable our engineers to glean the most information from the huge volume of simulation results. We’ll be sharing more details about our approach to simulation in a future post.</p><p><strong>From the shoulders of giants</strong></p><p>Open-source software has greatly impacted the software industry, especially in robotics and self-driving vehicles. An example of open-source success is the Robot Operating System (ROS). We’d like to acknowledge those we’ve built on top of; like many self-driving companies, our company’s roots were in ROS-based software development. We have since moved to using a new middleware system of our own design — which we have named Drive.ai pub-sub, or DPS — but still leverage open-source elsewhere. While our middleware is not yet ready to be open-sourced, we’ve designed it with the possibility of this in mind.</p><p>In the case of visualization, we build on <a href="https://github.com/RobotLocomotion/director">Director</a>, a robotics visualization and interface framework, which is based on the leading open-source libraries, VTK (The Visualization Toolkit) and Qt (user interface software). Director is a robotics visualization and interface framework developed during the 2012–2015 DARPA Robotics Challenge (DRC) by the MIT team. Director was used to develop and operate the highly-autonomous humanoid robot Atlas (built by Boston Dynamics).</p><p>Director’s 3D graphics and user interface system are designed to meet the demands of robotics R&amp;D. Director works together with robotics middleware systems to visualize high-frequency sensor streams and combine them with large-scale data like pointcloud maps and panoramic imagery.</p><p><strong>The way we see it</strong></p><p>At Drive.ai, we use state-of-the art visualization techniques to show accurate, high-resolution data to both passengers in our vehicles and our team designing the systems on the back-end. Whether it’s a simplified passenger visual displayed on-board or a data-rich annotation set, the visualizations we use are a crucial part of our engineering R&amp;D cycle.</p><p>In sharing more about how we use visualization, we hope to have provided a deeper understanding of the variety of use cases and contexts in which we use these tools. We hope to have demonstrated the power and impact of these approaches as we develop our self-driving systems to be the best they can be.</p><p>To keep up with future behind-the-scenes posts about Drive.ai, you can subscribe to our Medium page, <a href="https://www.youtube.com/channel/UCjr3womnFCi0ukTWlQXNTFg">YouTube channel</a>, or follow us on Twitter at <a href="https://twitter.com/driveai_">@driveai_</a>.</p><p>—</p><p><em>Pat Marion is Drive.ai’s Head of Simulation and the author of Director, a robotics interface and visualization framework. Kah Seng Tay is Drive.ai’s VP of Engineering overseeing the R&amp;D and software team.</em></p><p><em>Interested in data visualization, AI/ML and simulation challenges at Drive.ai? Apply to join us now at </em><a href="https://www.drive.ai/careers"><em>https://www.drive.ai/careers</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bd4458fc6809" width="1" height="1"> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>486</wp:post_id>
		<wp:post_date><![CDATA[2018-07-26 17:00:20]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-26 17:00:20]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[the-way-we-see-it]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@drive.ai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@drive.ai/the-way-we-see-it-bd4458fc6809?source=rss-37851f177ff8------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[852]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple&#039;s self-driving car system could change how it drives by detecting passenger stress levels</title>
		<link>https://fifthlevel.ai/archives/703</link>
		<pubDate>Thu, 26 Jul 2018 14:28:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/07/26/apples-self-driving-car-system-could-change-how-it-drives-by-detecting-passenger-stress-levels</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/26913-39085-26821-38809-26035-36466-applecar-testbed2-l-l-l.jpg" alt="Article Image" border="0" /> <br><br> Apple's self-driving car technology could potentially change the way it drives based on the reactions of its passengers, using monitoring sensors to analyze the vehicle's occupants and determine an appropriate driving style to suit the situation, potentially reducing the chance of a passenger panicking. <p><b><a href="https://appleinsider.com/articles/18/07/26/apples-self-driving-car-system-could-change-how-it-drives-by-detecting-passenger-stress-levels" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>703</wp:post_id>
		<wp:post_date><![CDATA[2018-07-26 14:28:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-26 14:28:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apples-self-driving-car-system-could-change-how-it-drives-by-detecting-passenger-stress-levels]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/07/26/apples-self-driving-car-system-could-change-how-it-drives-by-detecting-passenger-stress-levels]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[851]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Impressions from a successfull inhouse event: AI Rendezvous #3</title>
		<link>https://fifthlevel.ai/archives/744</link>
		<pubDate>Thu, 02 Aug 2018 18:28:31 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://aid-driving.eu/?p=3897</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Der Beitrag <a rel="nofollow" href="http://aid-driving.eu/impressions-ai-rendezvous-3/">Impressions from a successfull inhouse event: AI Rendezvous #3</a> erschien zuerst auf <a rel="nofollow" href="http://aid-driving.eu">AID</a>.</p> <p><b><a href="http://aid-driving.eu/impressions-ai-rendezvous-3/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>744</wp:post_id>
		<wp:post_date><![CDATA[2018-08-02 18:28:31]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-02 18:28:31]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[impressions-from-a-successfull-inhouse-event-ai-rendezvous-3]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/impressions-ai-rendezvous-3/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>General Motors Doubles Down On Bug Bounty Cybersecurity Effort</title>
		<link>https://fifthlevel.ai/archives/2470</link>
		<pubDate>Sun, 05 Aug 2018 22:10:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b67697864aaf94bc0d838eb</guid>
		<description></description>
		<content:encoded><![CDATA[Dan Ammann, President of General Motors, announced that the company will invite cybersecurity researchers to identify vulnerabilities in its products, and pay them a bug bounty in exchange. <p><b><a href="https://www.forbes.com/sites/davidsilver/2018/08/05/general-motors-doubles-down-on-bug-bounty-cybersecurity-effort/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2470</wp:post_id>
		<wp:post_date><![CDATA[2018-08-05 22:10:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-05 22:10:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[general-motors-doubles-down-on-bug-bounty-cybersecurity-effort]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/08/05/general-motors-doubles-down-on-bug-bounty-cybersecurity-effort/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Three Companies Vying For Traction In Self-Driving Software Platform Race</title>
		<link>https://fifthlevel.ai/archives/2471</link>
		<pubDate>Fri, 03 Aug 2018 17:01:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b63e9dc64aaf94bc0d832f2</guid>
		<description></description>
		<content:encoded><![CDATA[Apollo, Autoware, and DRIVE are three self-driving car platforms available as foundations for building a self-driving car. Keep an eye on how much traction they get. <p><b><a href="https://www.forbes.com/sites/davidsilver/2018/08/03/three-companies-that-offer-self-driving-software-platforms/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2471</wp:post_id>
		<wp:post_date><![CDATA[2018-08-03 17:01:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-03 17:01:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[three-companies-vying-for-traction-in-self-driving-software-platform-race]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/08/03/three-companies-that-offer-self-driving-software-platforms/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Improving Self-Driving Car Safety And Reliability With V2X Protocols</title>
		<link>https://fifthlevel.ai/archives/2506</link>
		<pubDate>Wed, 25 Jul 2018 23:41:18 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/1408082bae54</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fFwV0zFTAHAlasJ1evB7iw.png" /><figcaption>How V2X technology is used in communications</figcaption></figure><p>A protocol that would help with self-driving cars is V2X or <em>Vehicle-to-Everything</em>. At the moment AI using machine learning techniques (deep learning) have been the focus of development for autonomous vehicle software. A communications protocol layer for self-driving cars can add further improvements for SAE Level 5 compliance. It is a protocol that supports <em>V2V (Vehicle-to-Vehicle), V2I (Vehicle-to-Infrastructure), V2P (Vehicle-to-Pedestrian), V2H (Vehicle-to-Home), V2N (Vehicle-to-Network) </em>and <em>V2C (Vehicle-to-Cloud)</em> communications. This will allow self-driving cars to communicate and share information among themselves. This will make self-driving cars aware of each other much like how people interact. Another thing this will allow is direct communication with infrastructure like intelligent buildings, smart roads, traffic lights, bridges, railroads, airports and other intelligent transportation systems.</p><h4>Understanding The “Everything” in V2X</h4><p><strong>V2V Vehicle-to-Vehicle </strong>— Allows V2X enabled self-driving cars to communicate with one another.</p><p><strong>V2I Vehicle-to-Infrastructure </strong>— Allows self-driving cars to get information from buildings, bridges, roads, traffic lights etc.</p><p><strong>V2P Vehicle-to-Pedestrian </strong>— Makes use of pedestrian detection systems that can work with a car’s ADAS.</p><p><strong>V2H Vehicle-to-Home</strong> — Smart homes can send and receive information directly from the car.</p><p><strong>V2N Vehicle-to-Network</strong> — This is a mobile connection from the car to a carrier’s cellular network.</p><p><strong>V2C Vehicle-to-Cloud</strong> — Provides direct access to cloud networks using secure TCP/IP connections.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*W3ehS7fMKA5iSQIdqlf3Bg.png" /><figcaption>A driverless shuttle that uses self-driving technology from NAVYA (Las Vegas, NV)</figcaption></figure><p>The technology uses short-range wireless signals to communicate using a network that is compliant with their standards. This can address many issues that self-driving car developers face to ensure the safety of operating driverless autonomous vehicles for commercial use, which regulators like the NHTSA require. It has always been a big concern for regulators. Having standards like this will add more to safety and reliability as well. V2X is also meant to be implemented and deployed in a decentralized way without any single authority controlling the cars. Each self-driving car will be its own independent V2X sensor system, so it doesn’t require a central system operator. The sensors built to support V2X are high-bandwidth, low-latency and support high-reliability links. These sensors are also meant to work in inclement weather, providing higher reliability when needed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rM7CDRWvIxv_O_NxAgUbIA.png" /><figcaption>Example of how V2X can work in the real world (Source: Qualcomm)</figcaption></figure><h4>V2X has 3 main objectives</h4><ol><li>Road Safety</li><li>Traffic Efficiency</li><li>Energy Savings</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WPvvf0PGkZFacbbrj-fRQA.png" /><figcaption>Improves NLOS performance of self-driving cars (Source: Qualcomm)</figcaption></figure><p>Let’s discuss the 3 objectives in more detail.</p><p><strong>Road Safety </strong>— In order to assure regulators of safety, V2X developers address main issues that improve overall safety. This includes (among others) AEB Automatic Emergency Braking, collision detection, road hazard warnings, blind spot warning and platooning or creating safe distances for autonomous vehicles on roads and highways. The deployment of AHS (Automated Highway Systems) also called “Smart Roads” provide an intelligent transportation system that aims to prevent and avoid accidents through a coordinated communications system. For example Car A will avoid bumping into Car B at all times based on a measured safety distance while on the road. Perhaps even more important is preventing self-driving cars from hitting pedestrians crossing a street. Information from the road or intersection can give a V2X enabled self-driving car a heads up on common pedestrian crossing areas and even alert them when there is an actual pedestrian crossing the street. This can work together with a self-driving car’s LiDAR or vision system.</p><p><strong>Traffic Efficiency</strong> — Using congestion recognition and avoidance will allow self-driving cars to understand road conditions. While this is supposed to make self-driving cars more aware of traffic, it will depend a lot on the city and how well mapped the area is. The idea here is that self-driving cars can share information among themselves about the traffic conditions in their area. The use of navigation systems can interface with the self-driving car through an API. An algorithm can then be used by navigation systems to calculate routes to avoid congested roads. Other information that can be shared include the presence of roadblocks, intersection closures, accidents and other road related news. Another application of this would be intelligent route navigation in which self-driving cars can explore the best route to a destination in the shortest possible time.</p><p><strong>Energy Savings</strong> — Most cars after 2019 are expected to be electric. Self-driving cars, for the most part, are also electric so V2X has taken energy savings into consideration. EV run on batteries which provide the power to give the car its range. Self-driving cars being developed at the moment are electric, so a means to save more energy at longer ranges is definitely efficient. Energy is also saved when self-driving cars follow more optimum routes when navigating to their destination. This will become more apparent, or not, when electric cars with self-driving capabilities are deployed in greater numbers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wqjnuiI_HJ_jlQKUMrxhPQ.png" /><figcaption>V2X can communicate with other V2X enabled vehicles, carrier networks and even with the highway or road to determine traffic conditions and adjust routes for the most efficient way to get to a destination.</figcaption></figure><p>Currently, ADAS (Advanced Driver Assistance System) in some cars like the Tesla Model S Enhanced Autopilot provide semi-autonomous capabilities. What V2X can do is work with ADAS software features to provide more capabilities. For example V2X can feed information to the car’s AEB system to allow more accurate information when to apply the brakes in emergency situations. Another example is guiding the automatic lane change feature to more heightened safety by verifying the lane is fully clear and safe.</p><h4>V2X can be implemented in two ways</h4><ol><li>WLAN based architecture using IEEE 802.11p DSRC wireless standard</li><li>Cellular based LTE aka “Cellular V2X” or C-V2X which makes use of 4G and 5G networks</li></ol><p>In a recent 2017 study, C-V2X using LTE has been found superior to 802.11p in terms of performance, communication range and reliability. The two standards could very well merge to give the best features from both systems.</p><p>There are two ways V2X an be implemented. The FCC allocated 75 MHz of spectrum in the 5.9 GHz band to be used by Intelligent Transportation Systems (ITS). This is the spectrum that V2X developers will use. One way that V2X implements this is using Dedicated Short Range Communications (DSRC) in the IEEE 802.11p specification. The other way is to deliver C-V2X or Cellular V2X. C-V2X makes use of existing LTE 4G networks to communicate, which extends to 5G when available.</p><p>Working together with the car’s software, they can in theory coordinate the best way to navigate the streets and roads without a human driving behind the wheel. These are all standards that aim to deliver ways to improve the safety and reliability of self-driving cars.</p><p><strong><em>References:</em></strong></p><p>Congestion Recognition and Avoidance<br><a href="https://www.researchgate.net/publication/221450143_V2X-Based_Traffic_Congestion_Recognition_and_Avoidance">https://www.researchgate.net/publication/221450143_V2X-Based_Traffic_Congestion_Recognition_and_Avoidance</a></p><p>V2X (Wikipedia)<br><a href="https://en.wikipedia.org/wiki/Vehicle-to-everything">https://en.wikipedia.org/wiki/Vehicle-to-everything</a></p><p>Energy savings and V2X<br><a href="https://ieeexplore.ieee.org/document/6425172/">https://ieeexplore.ieee.org/document/6425172/</a></p><p>V2X communications<br><a href="https://www.zdnet.com/article/what-is-v2x-communication-creating-connectivity-for-the-autonomous-car-era/">https://www.zdnet.com/article/what-is-v2x-communication-creating-connectivity-for-the-autonomous-car-era/</a></p><p>Qualcomm C-V2X<br><a href="https://www.qualcomm.com/invention/5g/cellular-v2x">https://www.qualcomm.com/invention/5g/cellular-v2x</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1408082bae54" width="1" height="1"><hr><p><a href="https://medium.com/self-driving-cars/improving-self-driving-car-safety-and-reliability-with-v2x-protocols-1408082bae54">Improving Self-Driving Car Safety And Reliability With V2X Protocols</a> was originally published in <a href="https://medium.com/self-driving-cars">Self-Driving Cars</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><b><a href="https://medium.com/self-driving-cars/improving-self-driving-car-safety-and-reliability-with-v2x-protocols-1408082bae54?source=rss----bdc6e635b3c0---4" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2506</wp:post_id>
		<wp:post_date><![CDATA[2018-07-25 23:41:18]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-25 23:41:18]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[improving-self-driving-car-safety-and-reliability-with-v2x-protocols]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driving-cars]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driving-cars/improving-self-driving-car-safety-and-reliability-with-v2x-protocols-1408082bae54?source=rss----bdc6e635b3c0---4]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>You will control driverless cars… just not in the way you expect</title>
		<link>https://fifthlevel.ai/archives/2510</link>
		<pubDate>Mon, 06 Aug 2018 09:36:19 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/3154b57e868c</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*oGs8oE4bPtWJU651Gp1a8w.jpeg" /><figcaption>X marks the spot. Autonomous cars have the power to change economies but you will still be in ultimate control.</figcaption></figure><p>Let’s jump forward 10–20 years so we can accept that fully autonomous cars are starting to appear in most towns and cities.</p><p>Lots of other infrastructures have had to change to enable this revolution, many of which came at tremendous cost and largely borne by taxpayers, not car makers. Oh, you think a Billion dollar investment is big? Honestly, that’s nothing when you have to build a motorway.</p><p>Built infrastructure is expensive, and initially planned in a rarified ideal world of unlimited budgets and perfect traffic flows. Around London, as is now a fairly well-known alternative to hell on Earth, the M25 orbital motorway is one of the busiest pieces of tarmac in the world. Conceived decades before it was completed, it went over its original planned capacity within a year of completion.</p><p>Since then, additional lanes have been added (up to six lanes of crawling traffic at peak times), junctions upgraded, computer-controlled speed restrictions installed to ease the surge of traffic and most recently it has become a ‘smart motorway’ allowing the safety lane to be requisitioned for live traffic in times of peak demand.</p><p>The ‘smart motorway’ upgrade for a section of this road near my home, running for roughly 25 miles, has cost near enough to £100 million.</p><p>How is the cost justified, you might ask, even if you do have your journey shortened by 2 minutes per day…</p><p>Easy: political will. Governments have to be seen to do something to get elected, and when a road which is critical to national infrastructure (the large ones usually are) is not upgraded, the economy suffers.</p><p>Critical is a strong word, I hear you say, when we can find an alternate route…</p><p>You might know a back-road route but when the main highway is blocked your local area will descend into chaos, and journey times will double, treble, even. Small roads with lots of junctions don’t have the capacity to carry large trucks, large volumes of cars or maintain high speeds, so everything must slow down dramatically.</p><p>Deliveries from mail order outlets, people going to work, emergency services, supply chain shipments for manufacturers — all of it.</p><p>As individuals, we rarely see these impacts, because the delay only affects us to create a story of woe as our holiday is disrupted or we’re delayed for a meeting, but traffic delays are carefully studied on a mass scale, a privilege afforded to the best data scientists and economists who in turn inform transport planners, who are employed by governments, that are elected by you.</p><p>Why are some national governments so keen to embrace, fund and experiment with autonomous vehicles? At first glance, you might think it’s about updating their business eco-system, perhaps creating new jobs or preparing society for change? Sure, those things affect the economy, just as road traffic accidents also negatively impact most economies by about 4%. But it’s also about getting data on how the transport infrastructure, nationalised assets worth several percentage points of every country’s budget, will need to change.</p><p>If you’re cynical, you might simply see it as a wasteful investment of taxpayer money because autonomous cars ‘will never happen’ because you ‘enjoy driving too much’. You don’t enjoy it in traffic jams though, and the technology already exists in high end cars to alleviate that..</p><p>These delays have a financial impact that reduce national productivity. This is a figure that tells a government how efficient a country is, with thousands of data inputs, it allows for governments to make sweeping guesses about where to spend money that is being invested into large scale, long life projects which may well outlive the government and even the lives of the people working on it.</p><p>Take the Hoover Dam for example, a project which helped kick-start the American economy out of the Depression and is still generating billions in profit every year, 80 years after its creation.</p><p>Likewise, public transport doesn’t just ‘happen’. In growing and vibrant economies, timetables change to optimally utilise the available trains, the limited track infrastructure and react with some flexibility to the changing pattern of use. Some of those timetables align with other services, such a peak air or sea-port activity.</p><p>While the individuals working in government departments are often technical specialists with a life-long training, passion and experience for civil engineering, economic engagement and business development, they are directed by and responsible to elected officials, who are human beings, and are elected by you.</p><p>Irrespective of the arrival date of autonomous vehicles, whether they are public transport, shared vehicles or individually owned, their path, the legislation which enables them, the economy which empowers them, and the infrastructure they must use (from roads, to power, connectivity and even where they are allowed) is 100% in your control.</p><p>Use your voice, get involved and take a step of getting your local politicians informed about autonomous vehicles.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3154b57e868c" width="1" height="1"><hr><p><a href="https://medium.com/self-driving-cars/you-will-control-driverless-cars-just-not-in-the-way-you-expect-3154b57e868c">You will control driverless cars… just not in the way you expect</a> was originally published in <a href="https://medium.com/self-driving-cars">Self-Driving Cars</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><b><a href="https://medium.com/self-driving-cars/you-will-control-driverless-cars-just-not-in-the-way-you-expect-3154b57e868c?source=rss----bdc6e635b3c0---4" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2510</wp:post_id>
		<wp:post_date><![CDATA[2018-08-06 09:36:19]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-06 09:36:19]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[you-will-control-driverless-cars-just-not-in-the-way-you-expect]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driving-cars]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driving-cars/you-will-control-driverless-cars-just-not-in-the-way-you-expect-3154b57e868c?source=rss----bdc6e635b3c0---4]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Contemplating the eyes in the sky</title>
		<link>https://fifthlevel.ai/archives/2534</link>
		<pubDate>Fri, 20 Jul 2018 04:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/contemplating-eyes-sky-lisa-parks-satellites-0720</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Satellites have changed the way we experience the world, by beaming back images from around the globe and letting us explore the planet through online maps and other visuals. Such tools are so familiar today we often take them for granted.</p> <p>Lisa Parks does not. A professor in MIT’s Comparative Media Studies/Writing program, Parks is an expert on satellites and their cultural effects, among other forms of aerial technology. Her work analyzes how technology informs the content of our culture, from images of war zones to our idea of a “global village.”</p> <p>“I really wanted people to think of the satellite not only as this technology that’s floating around out there in orbit, but as a machine that plays a structuring role in our everyday lives,” Parks says.</p> <p>As such, Parks thinks we often need to think more crisply about both the power and limitations of the technology. Satellite images helped reveal the presence of mass graves following the Srebrenica massacre in the 1990s Balkans war, for instance. But they became a form of “proof” only after careful follow-up reporting by journalists and other investigators who reconstructed what had happened. Satellites often offer hints about life on the ground, but not omniscience.</p> <p>“Since satellite images are so abstract and remote, they necessitate closer scrutiny, re-viewing, careful description, and interpretation in ways that other images of war do not,” Parks writes in her 2005 book “Cultures in Orbit.”</p> <p>Alternately, satellite images can open up our world — or be exclusionary. The landmark 1967 BBC show “Our World,” one of the first broadcasts to feature live global satellite video links, was touted as a global celebration. But as Parks writes, it reinforced distinctions between regions, by emphasizing “the modernity, permanence, and civilizational processes of industrial nations,” and thus “undermining the utopian assumption that satellites inevitably turned the world into a harmonic ‘global village.’”&nbsp;</p> <p>For her distinctive scholarship, Parks was hired by MIT in 2016. She studies a range of media technologies — from the content of television to drone imagery — and has co-edited five books of essays on such topics, including the 2017 volume “Life in the Age of Drone Warfare.” Parks is also the principal investigator for MIT’s Global Media Technologies and Cultures Lab, which conducts on-site research about media usage in a range of circumstances.</p> <p>“Technology and culture is what I’m interested in,” Parks says.</p> <p><strong>Big sky, then and now</strong></p> <p>Parks grew up in Southern California and Montana. Her father was a civil engineer and her mother was a social worker — a combination, Parks suggests, that may have helped shape her interests in the social effects of technology.</p> <p>As an undergraduate at the University of Montana, Parks received her BA in political science and history. She initially expected to become a lawyer but then reconsidered her career path.</p> <p>“I didn’t want to be in an office all of the time,” Parks says. So she went back to the classroom, at the University of Wisconsin at Madison, where she received her PhD in media studies. It was there that Parks’ attention really turned to the skies and the technologies orbiting in them. She wrote a research paper on satellites that turned into both her dissertation and first book. Parks then took a job at the University of California at Santa Barbara, where she taught for over a decade before joining MIT.</p> <p>“I loved my job there, I loved working in the U.C. system, and I had excellent colleagues,” says Parks. Still, she adds, she was fascinated by the opportunities MIT offers, including its abundant interdisciplinary projects that pull together researchers from multiple fields.</p> <p>“MIT seems to really value those kinds of relationships,” Parks says.</p> <p>In the classroom, Parks teaches an undergraduate course on current debates in media, which grapples with topics ranging from surveillance to net neutrality and media conglomerations. For graduate students, she has been teaching a foundational media theory course.&nbsp;</p> <p>“If you’re an MIT student and you want to come out of this place having thought about some of the policy implications relating to the media in this current environment, our classes equip you to think historically and critically about media issues,” Parks says.</p> <p><strong>Technology … and justice for all</strong></p> <p>One other issue strongly motivates Parks’ scholarship: the idea that technology is unevenly distributed around the world, with important implications for inequality.</p> <p>“Most people in the world live in relatively disenfranchised or underprivileged conditions,” Parks says. “If we shift the question about designing technologies so they serve a broader array of people’s interests, and designs are interwoven with concerns about equity, justice, and other democratic principles, don’t those technologies start to look different?”</p> <p>To this end, MIT’s Global Media Technologies and Cultures Lab, under Parks’ direction, studies topics such as media infrastructure, to see how video is distributed in places such as rural Zambia. Parks’ research has also examined topics such as the video content accessible to Aboriginal Australians, who, starting in the 1980s, attempted to gain greater control of, and autonomy over, the satellite television programming in rural Australia.</p> <p>Parks’ research takes place in a variety of social and economic orbits: In March, you could have found her and a research assistant, Matt Graydon, at the Satellite 2018 convention in Washington, interviewing CEOs and industry leaders for a new study of satellite-based internet services.</p> <p>In some places around the globe, the effects of aerial technology are more immediate. In the volume on drones, Parks writes that these tools create a “vertical mediation” between ground and sky — that when “drones are operating in an area over time, above a certain region, they change the status of sites and motions on the ground.” She elaborates on this in her new book, out this year, “Rethinking Media Coverage: Vertical Mediation and the War on Terror.”</p> <p>As diverse as these topics may seem at first, Parks’ scholarly output is intended to expore more deeply the connection between aerial and orbital technologies and life on the ground, even if it is not on the mental radar for most of us.&nbsp;</p> <p>“We need to be studying these objects in orbit above, and think about orbital real estate as something that’s relevant to life on Earth,” Parks says.</p>
<p><b><a href="http://news.mit.edu/2018/contemplating-eyes-sky-lisa-parks-satellites-0720" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2534</wp:post_id>
		<wp:post_date><![CDATA[2018-07-20 04:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-20 04:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[contemplating-the-eyes-in-the-sky]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/contemplating-eyes-sky-lisa-parks-satellites-0720]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>How We’re Working to Reduce Congestion by Going Directly to Businesses, Universities and Cities</title>
		<link>https://fifthlevel.ai/archives/2542</link>
		<pubDate>Wed, 25 Jul 2018 14:00:34 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/5497f20fe04c</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Dan Grossman, CEO, Chariot</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*10ztizW_CJNBZSaBk6lkdg.jpeg" /></figure><p>When it comes to making traffic less of a nightmare, the goal has become increasingly straightforward: Change the one person to one car culture. Especially during rush hour, reducing the number of single-occupancy cars can go a long way to improving commutes for everyone.</p><p>As you can imagine, that’s easier said than done.</p><p>At Chariot, we’re committed to reducing congestion with a comfortable shuttle service that provides another way to improve access to jobs and bring groups of people to the public transit stations that serve their communities. Using mostly our 14-passenger Ford Transit vans, we’re already operating commuter services in various cities and serving businesses looking for reliable ways to transport employees. We’ve also established an experienced team focused on developing contracts with city and public transit agencies.</p><p>But this challenge will not be solved by simply targeting cities alone. To truly make an impact on congestion, we need participation from all sectors — and the good news is that participation is building. Increasingly, we’re hearing from companies, universities and others that are interested in dependable, convenient transportation services for their employees and residents. As a result of this demand, we are sharpening our focus on our enterprise division at Chariot, which develops custom microtransit routes that serve public and private entities such as corporate campuses, hospitals, universities, business parks and residential developments.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ip543YIo1s-lMJTg5uGmZg.jpeg" /></figure><p>Working directly with these types of organizations will help us keep delivering on our goal of providing a shared, high-occupancy transportation option that complements public transit while decreasing congestion. It’s not always easy convincing someone to start sharing their commute, so we’re committed to making that idea appealing with great vehicles and great service.</p><p>More and more, businesses are looking to improve commuting times — not to mention the commuting experience as a whole — for their employees. At Chariot, we’re perfecting the ability to provide that higher level of service. Our Chariot vans are operated by full-time, fully trained employee drivers who reliably get their riders where they need to be — in safe, comfortable, Wi-Fi-enabled vehicles that regularly undergo the maintenance necessary to keep them in top shape.</p><p>We’re already doing this across the United States. In California, we recently launched routes serving employees of the University of California at its San Francisco Mission Bay Campus. As part of a grant from the Metropolitan Transportation Commission, our service is designed for university employees, while supporting the commission’s mission to reduce the number of single-occupancy vehicles traveling across the Bay Bridge and decrease congestion during peak commute times. We’re also working with Mastercard in the Bay Area to provide its employees an easy way to travel between their office and multiple local rail stations.</p><p>Of course, our enterprise services don’t have to be limited to serving a single organization’s employees. In most cities, transit is essentially fixed in terms of light rail and core bus routes, but residential and commercial development is increasingly occurring outside the area served by those systems. That means there’s a need to connect those developments — whether people own their own vehicles or not — to existing transit options. That’s where Chariot can play a role, moving groups of people in a single vehicle and getting them around campuses or to public transit stations.</p><p>In Chicago’s western suburbs, we recently launched a new route that serves the Oak Brook business community by connecting employees and the general public between the district and the nearby transit station. This is a two-year pilot in partnership with the Regional Transportation Authority of Northeastern Illinois, the Village of Oak Brook and the Greater Oak Brook Chamber of Commerce. In Denver, meanwhile, our new Chariot service connects students, faculty and staff at the University of Denver to a nearby light rail station, all with the goals of decreasing congestion and continuing the university’s commitment to sustainability.</p><p>Our commitment to this service extends beyond North America. In London, Chariot’s first international market, we are looking at ways to work with businesses and other organizations to improve the commutes of employees and visitors, while continuing to serve the public via our four commuter routes.</p><p>To support our increased focus on enterprise and our continued growth, we’ve welcomed Kari Novatney as our new chief operating officer. Novatney brings valuable experience in two important areas — driving growth for companies, including Sony and JustAnswer, and helping to deliver smart transportation initiatives with Caltrans and University of California, Berkeley. Greg Jorgensen also joined Chariot as chief financial officer after 18 years in various finance roles at Ford and after working as chief financial officer for Quick Lane.</p><p>Additionally, we’ve brought onboard our first vice president of markets in Kate Roberts, who led strategic partnerships for Zipcar and will now oversee existing and emerging markets for Chariot. And to oversee our human resources and recruiting teams, Michelle Streicher joins us from Ford as our new vice president of human resources.</p><p>These additions to Chariot are already making an impact, aiding the development of our latest launches and helping to move us into the future. We know every city is different, but encouraging people to share their commute and ease the burden on our roadways is crucial to ensuring our communities have the capacity to improve their residents’ quality of life and set them up for a prosperous future. We’re committed to understanding what city leaders, businesses and residents need to make traveling more efficient. By providing the right services to those who live and work there every day, we can start reducing the number of vehicles on our roadways and work to improve everyone’s daily commute and overall quality of life.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5497f20fe04c" width="1" height="1"> <p><a href="https://medium.com/@ford/how-were-working-to-reduce-congestion-by-going-directly-to-businesses-universities-and-cities-5497f20fe04c?source=rss-db92c082f24a------2" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2542</wp:post_id>
		<wp:post_date><![CDATA[2018-07-25 14:00:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-07-25 14:00:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[how-were-working-to-reduce-congestion-by-going-directly-to-businesses-universities-and-cities]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@ford]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@ford/how-were-working-to-reduce-congestion-by-going-directly-to-businesses-universities-and-cities-5497f20fe04c?source=rss-db92c082f24a------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>How to write a car port for openpilot</title>
		<link>https://fifthlevel.ai/archives/3253</link>
		<pubDate>Thu, 02 Aug 2018 01:15:13 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/7ce0785eda84</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="https://github.com/commaai/openpilot">openpilot</a> is an open source self-driving agent. Currently it performs the functions of Adaptive Cruise Control (ACC) and Lane Keeping Assist System (LKAS). There are hundreds of daily openpilot drivers, a community of thousands of enthusiasts who joined the <a href="http://slack.comma.ai">comma’s Slack channel</a>. Numbers are growing quickly and so does openpilot driving quality.</p><p>A significant number of developers contributed to the <a href="https://github.com/commaai/openpilot">openpilot repository on GitHub</a> in many ways: bug fixes, improvements, and car ports. A car port consists in adding support for additional car brands and models to openpilot’s code. As of July 2018, <a href="https://github.com/commaai/openpilot/blob/devel/README.md">openpilot officially supports</a> almost every Honda/Acura and Toyota/Lexus currently sold, plus a Chevy. Many of these car models have been added to the list thanks to the comma community.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pcDSLTEsvdPRysBDNutFbA.jpeg" /></figure><p>We can distinguish between 2 main types of car ports:</p><ol><li><strong><em>Model Port</em></strong>: the car’s brand is already supported, but the specific model, model year or trim isn’t. Despite some exceptions where apparently similar cars are very different “under the hood” (see Honda Civic 2018 sedan VS hatchback), usually such ports require minimal code changes. <a href="https://github.com/commaai/openpilot/pull/198/files">Here</a> is an example of pull request for a <em>Model Port</em>.</li><li><strong><em>Brand Port</em></strong>: Either the car’s brand isn’t supported or the specific car model is substantially different from any currently supported cars. Those type of ports usually require a substantial amount of code to be written in openpilot and panda repositories. In particular, original safety logic needs to be added to panda.</li></ol><p>comma already published a <a href="https://medium.com/@comma_ai/openpilot-port-guide-for-toyota-models-e5467f4b5fe6">detailed guide on how to make a <em>Model Port</em>, specifically for Toyota</a>. The goal for this guide is to provide a more high level procedure that covers all car ports, including <em>Brand Ports.</em> Particular attention is given to the safety aspect of the port.</p><h3>Background — safety architecture</h3><p>No safety relevant code runs on the EON. To use functional safety terminology, the EON functionality is considered Quality Management (QM). This means that any failure in delivering the desired output at the right time is perceived as bad quality and has no safety implications. You might be perplexed: the code running on the EON is responsible, for example, for determining the desired car trajectory and you might think that a bug in this algorithm might be a safety threat. But it’s not, openpilot is a <a href="https://web.archive.org/web/20170903105244/https://www.sae.org/misc/pdfs/automated_driving.pdf">Level 2 ADAS system</a> and the driver must pay attention at all time! No ADAS system currently on the market has safety guarantees on perception or planning algorithms.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/947/1*0YZsWlcltvV7u9ASZL6u4A.jpeg" /></figure><p>So, what must be guaranteed is the ability of the driver to easily regain full control of the vehicle at any time. In openpilot, this is done through the satisfaction of the <a href="https://github.com/commaai/openpilot/blob/devel/SAFETY.md">2 main safety principles</a> that a Level 2 driver assistance system must have:</p><ol><li>the driver must always be capable to immediately re-take manual control of the vehicle, by stepping on either pedal or by pressing the cancel button;</li><li>the vehicle must not alter its trajectory too quickly for the driver to safely react. This means that while the system is engaged, the actuators are constrained to operate within reasonable limits.</li></ol><p>All the safety relevant code runs real-time on a STM32 micro (inside the <a href="https://comma.ai/shop/products/panda-obd-ii-dongle/">Panda</a> OBD II dongle), it’s written in C and it’s placed at the interface between the car and the EON. The safety code is included in openpilot as a submodule but it’s maintained in its own <a href="https://github.com/commaai/panda">panda repository</a>.</p><p>These safety requirements, written as is, are still a little abstract and a further step is required to translate them into implementable requirements. We can develop them as follow.</p><ul><li>Upon stepping on either pedal or pressing the cancel button, the panda safety code shall not allow any non-zero gas, brake or steer command until the user presses the engage button again.</li><li>openpilot shall immediately cancel when the driver’s seat-belt is unlatched or the driver’s door is open. This is to prevent the driver leaving the vehicle while openpilot is still engaged.</li><li>Max accel and decel injections: the maximum acceleration and deceleration of the vehicle shall be limited between 2m/s² and -3.5m/s², respectively¹. These limits are a more conservative than what’s recommended in <a href="https://www.iso.org/obp/ui/#iso:std:iso:22179:ed-1:v1:en">ISO 22179</a>.</li><li>Max steer injection: in the case of a completely erroneous steering command, the driver shall have at least 1 second of reaction time before the car significantly deviates from its original path.</li><li>Steer loss: while in a turn, in the case of an immediate loss of steering torque, the driver shall have at least 1 second of reaction time before the car significantly deviates from the desired path (e.g. lane lines crossing).</li><li>Steer injection against driver’s desire: the driver shall be capable of overriding openpilot’s steering strenght with minimal effort: less than 3 Nm of extra torque applied at the steering wheel shall be necessary to correct openpilot’s steering control.</li></ul><p>The specific implementation of those requirements depends on the specific car control APIs, as explained <a href="https://github.com/commaai/openpilot/blob/devel/SAFETY.md">here</a>.</p><h3>Submit a successful car port pull request to openpilot</h3><p><strong>STEP 1a — Develop a car port<br></strong>openpilot is written to be as generic as possible, by separating car specific code (e.g. decoding CAN messages into generic car states, such as speed and acceleration) from common openpilot functionalities (e.g. perception, planning and high level controls).<br>With the exception of the panda safety code and the dbc file, the car specific code if fully contained in /data/openpilot/selfdrive/car/. When developing a car port you should focus on making changes to this folder only, although it’s possible that some generalization to the rest of the code are required. If you are making a <em>Brand Port</em>, you should create a new folder with the name of the brand you intend supporting. Otherwise, in the case of a <em>Model Brand</em><strong>, </strong>you should only modify files within the existing car brand folder (e.g. /data/openpilot/selfdrive/car/toyota/).</p><p><strong>STEP 1b — Develop a safety model for your car port<br></strong>In the case of a <em>Brand Port</em>, you should also implement a new safety model for the panda repo. Each car brand has at least one safety model associated to it (see header files in/data/openpilot/panda/board/safety/). Also, each safety model must have its own regression test file (see /data/openpilot/panda/tests/safety/), which you should submit as well as part of the pull request for the panda repo.</p><p>Writing a new safety model might seem hard at first. Luckily, there is a high chance that you car has longitudinal and lateral control APIs similar to some that already supported cars have.</p><p>For example, if the longitudinal control API allows to command directly the gas pedal and the brake pressure (see Chevy Volt), you might simply enforce that the gas and brake commands are constrained within values that satisfy the acceleration limits mentioned above.</p><p>In the case of a lateral control API based on steering torque (again, see Chevy Volt), you might want to enforce the max torque commanded and the rate at which such torque is commanded. Additionally, you might want to limit the steering torque when applied in the opposite direction of the torque applied by the driver.</p><p><strong>STEP 2 — Open a pull request to openpilot, panda and opendbc repos<br></strong>Opening a pull request to openpilot is probably the best way to get support from the large comma.ai community. However, while you are welcome to open a car port pull request for openpilot at anytime, be conscious about the possibility that other users might be installing your WIP fork on their EON dev kit.</p><p>The panda repo is included within openpilot, but it’s maintained in its own repo. In the case of a <em>Brand Port</em><strong>,</strong> you should submit a pull request to cover the safety in panda. See the above guidelines on how to write a safety model.</p><p>If needed, you should also open a pull request for the opendbc repo if the car you are writing the port for needs a new dbc file. We already have many dbc files, so <a href="https://github.com/commaai/opendbc">your car might already be here</a>.</p><p><strong>STEP 3 — Get opendbc and panda pull requests merged<br></strong><a href="https://github.com/commaai/opendbc">opendbc</a> and <a href="https://github.com/commaai/panda">panda</a> are independent repositories (shipped as subtrees in openpilot), so comma will merge those pull request before and regardless the status of the openpilot pull request. New dbc files and panda safety models are always welcome.</p><p><strong>STEP 4 — Have the port officially listed in “Community Maintained Cars”<br></strong>comma will review that proper safety code has been implemented. In the case of a<strong> </strong><em>Model Port</em>, this is usually unnecessary as the safety code is shared with other already supported models.<br>If the safety model review is successful and comma has proof that the car port meets the quality standards (send us a <a href="https://www.youtube.com/watch?v=-sErJrZZrUo">video</a> of your car being controlled by openpilot!), then the port can be listed in the “Community Maintained Cars” section of the openpilot README. A car listed in “Community Maintained Cars” does not guarantee that the comma safety model is met, just that enough diligence has been done for this step; the port can then be listed in the “Community Maintained Cars” section of the openpilot README.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FIWrJvG2jZEU%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DIWrJvG2jZEU&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FIWrJvG2jZEU%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/a59aba6adc7e4dd0b01c612ad5068d2f/href">https://medium.com/media/a59aba6adc7e4dd0b01c612ad5068d2f/href</a></iframe><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FiRkz7FuJsA8%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DiRkz7FuJsA8&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FiRkz7FuJsA8%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/9592c4fcc78c83363a8068e26edcd747/href">https://medium.com/media/9592c4fcc78c83363a8068e26edcd747/href</a></iframe><p><strong>STEP 5 — Get the openpilot pull request merged<br></strong>At comma’s discretion, some car ports can be upstreamed into the openpilot repo and become officially supported, while other might maintain the status of “Community Maintained Cars” indefinitely. It mainly depends on the code quality/complexity of the pull request, and on the popularity of the car/the presence of such a brand in comma’s fleet. Feel free to bring your car to comma’s HQ for a test drive :)</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7ce0785eda84" width="1" height="1"> <p><b><a href="https://medium.com/@comma_ai/how-to-write-a-car-port-for-openpilot-7ce0785eda84?source=rss-330bac69b283------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3253</wp:post_id>
		<wp:post_date><![CDATA[2018-08-02 01:15:13]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-02 01:15:13]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[how-to-write-a-car-port-for-openpilot]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@comma_ai/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@comma_ai/how-to-write-a-car-port-for-openpilot-7ce0785eda84?source=rss-330bac69b283------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Why I Ride with Waymo: Lilla</title>
		<link>https://fifthlevel.ai/archives/249</link>
		<pubDate>Wed, 22 Aug 2018 15:01:19 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/6136f1bb279c</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>Editor’s Note: In April 2017, we launched our early rider program to learn how our fully self driving technology fits into people’s daily lives. Now, almost </em><a href="https://medium.com/waymo/waymos-early-rider-program-one-year-in-3a788f995a9c"><em>a year</em></a><em> and a half later, well over 400 people ride with us every day </em>―<em> to get to school, work, dinner with friends, and around town for errands, helping us shape the future of our service. Meet Lilla, one of our riders in Metro Phoenix, who shares her perspective on what it’s like to ride with Waymo.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*n1HszOo2-Hf36zP0GtngIg.png" /></figure><p><strong>Tell us a bit about yourself!</strong></p><p>I’m Lilla and I live in Mesa, Arizona. I’ve always enjoyed learning. My first degree was in English and Literature, and now I work for a small software company where I get to learn new types of (software) languages! I love to read, play video games and be active.</p><p><strong>Can you share what it’s like to be one of our early riders?</strong></p><p>Riding in Waymo’s cars is always fun. I like to watch the screens to see what the car’s sensors are picking up. I’m always pleased to see small improvements in the car’s driving patterns, like how it nudges up to the car in front of it, how it changes lanes when other cars are nearby, or how much buffer it gives before making an unprotected left turn. It has also started to remind me to shut the door when I exit the vehicle (I’ve been guilty of leaving it open!).</p><p>I try to always give feedback during the ride (that’s if I’m not too busy reading!). I always say I’m so proud of the car when it completes a merge/lane change or a turn that was once difficult or uncomfortable. I kid, but I really do feel proud when I notice the car has improved based on feedback I’ve submitted ― like I’m part of the process and that the team at Waymo is genuinely listening to the early riders.</p><p><strong>What do you use Waymo for today?</strong></p><p>I use Waymo to visit my friends, go to the gym, and run simple errands. I carpool with a friend a few days a week since the gym we go to is next to her house. At the start of the day, Waymo takes me to her house. At the end of the day, Waymo brings me home. It’s become a routine, and I LOVE routines.</p><p>In my household, when we go out, there’s no more rock, paper, scissors to see who is the designated driver. No more ridesharing in a car that may feel cramped. We know exactly what to expect with a Waymo, and we all get to be comfortable.</p><p>Waymo has also allowed me to help my friends in need. I let them borrow my personal car because I have less of a dependency on it, and I can still do the things I want to do.</p><p><strong>What do you think the potential is for self-driving cars?</strong></p><p>Self-driving cars are the future that I want. I personally don’t like driving; it gives me anxiety though I’m a very cautious and courteous driver. Waymo eliminates the stress of getting behind the wheel, and I feel safer in a car that’s designed to never get distracted or tired.</p><p>By riding with Waymo, I’ve gotten cumulative hours of my life back, hours that I would have spent driving, being anxious and filled with road rage. And I’ve had the chance to experience a bit of the future. Humanity has this wonderful opportunity to move forward in terms of personal transportation, and I feel honored to be a part of it.</p><p>—</p><p>Our early rider program <a href="https://waymo.com/apply/">continues to take applications</a>; if you live in the area, drop us a line.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6136f1bb279c" width="1" height="1"><hr><p><a href="https://medium.com/waymo/why-i-ride-with-waymo-lilla-6136f1bb279c">Why I Ride with Waymo: Lilla</a> was originally published in <a href="https://medium.com/waymo">Waymo</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>249</wp:post_id>
		<wp:post_date><![CDATA[2018-08-22 15:01:19]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-22 15:01:19]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[why-i-ride-with-waymo-lilla]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/waymo]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/waymo/why-i-ride-with-waymo-lilla-6136f1bb279c?source=rss----7075a35566d9---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[865]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Getting ready for more early riders in Phoenix</title>
		<link>https://fifthlevel.ai/archives/250</link>
		<pubDate>Tue, 21 Aug 2018 11:58:31 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/1699285cbb84</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>By: Ellice Perez, Head of Operations</em></p><p>We launched our <a href="https://medium.com/waymo/waymos-early-rider-program-one-year-in-3a788f995a9c">early rider program </a>in Phoenix in April 2017, and today, hundreds of riders are using our fully self-driving vehicles to get around every day, with even more joining throughout the summer. To run this self-driving service, we rely on our local operations teams to maintain our vehicles, manage fleet logistics, and provide around-the-clock support to our riders.</p><p>We’ve been busy growing these teams and building out our operations in Phoenix. Recently, we doubled the size of our operations center in Chandler, which will help us increase our fleet capacity and hire even more local talent for our test driving, fleet operations, and rider support teams there.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*T_i-4iKYHv56H9Usw0ogUA.jpeg" /><figcaption><em>Waymo vehicles in our Chandler operations center, ready for the day</em></figcaption></figure><p>What do all those teams do? Let’s take a look at what happens inside our operations center to ensure that our self-driving cars are safe, reliable, and easy to use for our riders.</p><p><strong>Behind the scenes at our operations center</strong></p><p>At any given moment, you’ll find our operations center buzzing with teams doing everything from fine-tuning our vehicles to answering real-time rider questions about their trip. Four teams are based in our Chandler center: fleet technicians, fleet dispatch, fleet response, and rider support. Each plays a key role in creating a safe and enjoyable rider experience.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/720/1*BHTsG_3hLfKvcpyC1f81VQ.gif" /><figcaption><em>Our vehicles coming and going throughout the day</em></figcaption></figure><p><strong><em>Fleet Technicians:</em> </strong>Our vehicles are equipped with self-driving sensors, designed in-house by Waymo engineers, that help our vehicles “see” and safely navigate the world around them. Keeping this hardware in tip-top shape is the job of our fleet technicians, who conduct regular maintenance checks on our self-driving systems. Think of these like the pre-flight checks performed on planes before they’re cleared for takeoff.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sFgnWRNa-1j35XYxSj_dKg.jpeg" /><figcaption><em>One of our fleet technicians checking up on a vehicle</em></figcaption></figure><p><strong><em>Fleet Dispatch:</em></strong> To efficiently accommodate our early riders’ demand, this team makes sure we always have the right number of vehicles out on the road. Fleet dispatchers also play an important role as we continue to expand our service area and driving capabilities: they manage our fleet of mapping and test vehicles and send our trained drivers out on driving “missions” to test new features and gain real world experience.</p><p><strong><em>Fleet Response:</em></strong><em> </em>Our self-driving system is designed to identify changing road conditions that might affect the ordinary flow of traffic. In these highly contextual situations, our vehicles can automatically call on our fleet response team to weigh in with an extra set of eyes. While the vehicle remains entirely responsible for its safe operation, the confirmation provided by this team can help make a journey more convenient for riders. Take a road closure for example: if one of our vehicles detects that a road is blocked up ahead, it may come to a stop and request confirmation from our fleet response team before plotting an alternate route. Once the blockage is verified, our vehicle decides the best way to proceed, and our specialists can then share this intel with the rest of the fleet so that our vehicles can route trips to avoid this area.</p><p><strong><em>Rider Support: </em></strong>For many people, their first ride in a Waymo vehicle is their first experience with fully self-driving technology. Our rider support team is on hand to make this experience as smooth as possible. With the tap of a button on our in-car console or through the Waymo app, riders can instantly connect to a rider support agent with any questions they have — from “How can I connect my music?” to “What if I want to change my destination during the trip?” Rider support is also available in case of an emergency or if riders leave something behind in the vehicle.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*p4i4kiFEIj1RF9UqURY8xA.jpeg" /><figcaption><em>An agent on our rider support team speaking with one of our early riders</em></figcaption></figure><p>With our fleet technicians, dispatchers, responders, and rider support agents always hard at work, our self-driving cars can move our riders safely from point A to point B everyday.</p><p>—</p><p>To learn more about our open roles in Phoenix, please visit <a href="https://waymo.com/joinus/">https://waymo.com/joinus/</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1699285cbb84" width="1" height="1"><hr><p><a href="https://medium.com/waymo/getting-ready-for-more-early-riders-in-phoenix-1699285cbb84">Getting ready for more early riders in Phoenix</a> was originally published in <a href="https://medium.com/waymo">Waymo</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>250</wp:post_id>
		<wp:post_date><![CDATA[2018-08-21 11:58:31]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-21 11:58:31]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[getting-ready-for-more-early-riders-in-phoenix]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/waymo]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/waymo/getting-ready-for-more-early-riders-in-phoenix-1699285cbb84?source=rss----7075a35566d9---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[862]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Expanding Our Reach in the Lone Star State: Drive.ai is Coming to Arlington, TX</title>
		<link>https://fifthlevel.ai/archives/345</link>
		<pubDate>Wed, 22 Aug 2018 11:42:27 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/4560d99e02d7</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Earlier this summer, we announced our first ever self-driving pilot program available to the public in Frisco, Texas. We launched the service weeks ago, and we’ve been lucky to have received a warm welcome in Texas. Now we are excited to announce that we’re deepening our roots in the Dallas/Fort Worth area. The City of Arlington signed a contract initiating a one-year self-driving program for Arlington residents, visitors, and anyone who’s interested in taking a ride in one of Drive.ai’s self-driving vehicles.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/596/0*m3Iil84QkXLvg2N5" /></figure><p><strong>Come On In, We’re Open</strong></p><p>From the start, Drive.ai’s approach to succeeding in the self-driving market has been through highly collaborative public and private partnerships with cities, businesses, and OEMs. We’ve been encouraged by the progress and reception following our first deployment with Frisco, and are looking forward to building on that momentum in the coming months.</p><p>Our partnership with the City of Arlington represents another important milestone: this program marks our first revenue. We’re officially “open for business,” — now with two publicly-available self-driving services in U.S. cities — and we will continue to look for ways to solve the real-world transportation challenges facing communities today.</p><p><strong>New City, New Solutions</strong></p><p>At Drive.ai, one of our fundamental philosophies is that we aren’t making technology for technology’s sake. We’re here to solve communities’ mobility challenges, and every community is different; that means that we tailor each deployment of our technology to meet the unique needs of the community we are servicing. No two places are exactly the same, and our partnerships and our programs aren’t, either.</p><p>The Dallas/Fort Worth area is one of the fastest growing metropolitan areas in this country, with a diverse community and rapid development. Our first deployment, in Frisco, is a blueprint for what self-driving programs can look like in today’s world. This program with Arlington is yet another step forward for public self-driving programs, and it demonstrates how self-driving technology is reshaping the way we all think about and experience transportation. We believe that self-driving cars can and will make a difference by improving the lives of those who use them and those who share the roads with them.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/810/0*UQEeE8Ae7wmJKFa4" /></figure><p><strong>Deep Collaboration for Continued Success</strong></p><p>Thanks to close coordination with our partners, we’ve been able to design self-driving programs in Frisco and Arlington that can improve transportation through innovation. We look forward to working with the City of Arlington and serving the local community and visitors that will use the self-driving program.</p><p>This is an exciting moment for our team as we continue to grow and improve our technology. We continue to seek new locations and challenges where our self-driving programs can make a real difference, and are thrilled that Arlington is the next place where we can have that kind of impact.</p><p>For partnership and pilot program inquiries, please contact: partnerships@drive.ai.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4560d99e02d7" width="1" height="1"> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>345</wp:post_id>
		<wp:post_date><![CDATA[2018-08-22 11:42:27]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-22 11:42:27]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[expanding-our-reach-in-the-lone-star-state-drive-ai-is-coming-to-arlington-tx]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@drive.ai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@drive.ai/expanding-our-reach-in-the-lone-star-state-drive-ai-is-coming-to-arlington-tx-4560d99e02d7?source=rss-37851f177ff8------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[918]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>A Matter of Trust: Ford’s Approach to Developing Self-Driving Vehicles</title>
		<link>https://fifthlevel.ai/archives/385</link>
		<pubDate>Thu, 16 Aug 2018 04:01:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/12f602887822</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Sherif Marakby, CEO, Ford Autonomous Vehicles LLC</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xpOoJVHsargk_Ru-E2QsWg.jpeg" /></figure><p><em>Earlier this week, I shared a report outlining our approach to self-driving vehicle development with U.S. Department of Transportation Secretary Elaine Chao. Our full report, “</em><a href="https://media.ford.com/content/dam/fordmedia/pdf/Ford_AV_LLC_FINAL_HR_2.pdf"><em>A Matter of Trust</em></a><em>,” is available for you to read and you can view the letter I sent to Secretary Chao below.</em></p><p><em>As Ford pursues its goal of developing a self-driving vehicle for deployment at scale in 2021, we believe that ensuring people trust the safety, reliability and the experience that our vehicles will enable is paramount. We’ve spent more than a century earning consumer trust by designing and building safe, quality vehicles, and we’re committed to preserving that trust in a future of self-driving vehicles.</em></p><p><em>In our report, you will learn how we always prioritize safety at Ford; how we are working closely with industry and government partners; and how we are applying self-driving technology to solve the challenges our cities face. Our goal is to give you a sense of the new services that will be available in the near future and how they could impact our lives.</em></p><p>The Honorable Elaine Chao<br>Secretary, U.S. Department of Transportation<br>1200 New Jersey Avenue SE<br>Washington, D.C. 20590</p><p>Dear Madam Secretary:</p><p>We truly appreciate your work with the Department of Transportation to set a framework for self-driving technology development with “<a href="https://www.nhtsa.gov/sites/nhtsa.dot.gov/files/documents/13069a-ads2.0_090617_v9a_tag.pdf">Automated Driving Systems 2.0: A Vision for Safety</a>.” We share your goal of being good stewards of the future, which includes both ensuring safety and facilitating innovation and competitiveness. As such, we’re pleased to share our voluntary safety self-assessment, “<a href="https://media.ford.com/content/dam/fordmedia/pdf/Ford_AV_LLC_FINAL_HR_2.pdf">A Matter of Trust: Ford’s Approach to Developing Self-Driving Vehicles</a>.”</p><p>As you know well, technology is changing the world we live in, including how we get around. Transportation is no longer just about owning a car or catching a train. Much of the change we are experiencing is driven by innovations focused on solving issues of traffic congestion and air pollution, as well as pedestrian, cyclist and vehicle occupant safety.</p><p>At Ford, we celebrate this innovation with the understanding that many of these technologies are still in the early stages of development. We appreciate the fact that you and the department do too and agree that the benefit of these technologies comes from our working together to ensure they are deployed in safe, responsible and effective ways.</p><p>Self-driving vehicles are an innovation that shows tremendous promise and, yet, for each and every one of us, the technology is all new. With all of the excitement and anticipation, and yes, some trepidation, it is important to share our perspective and listen to others.</p><p>At Ford, we’re working hard to understand the transformative potential of self-driving technology in order to best apply it to positively change the lives of our customers, the operation of our cities, and the economics of businesses large and small.</p><p>We also believe that developing self-driving vehicles is not simply about the technology — it is about earning the trust of our customers and of those cities and businesses that will ultimately use it. Safety, reliability and the experience the technology will enable are the key pillars to developing trust.</p><p>We hope you view this report as another sign of our commitment to being a collaborative partner with the U.S. Department of Transportation as we enter the next era in transportation. We look forward to working together to build consumer awareness and confidence in the technology on our journey to deploy safe, reliable and valued self-driving vehicles.</p><p>Very sincerely yours,</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/197/1*c6IqwshksIMyfOZlQ-6eww.jpeg" /></figure><p>Sherif Marakby<br>CEO, Ford Autonomous Vehicles LLC</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=12f602887822" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/a-matter-of-trust-fords-approach-to-developing-self-driving-vehicles-12f602887822">A Matter of Trust: Ford’s Approach to Developing Self-Driving Vehicles</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/a-matter-of-trust-fords-approach-to-developing-self-driving-vehicles-12f602887822?source=rss----7192a82ca09a---4" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>385</wp:post_id>
		<wp:post_date><![CDATA[2018-08-16 04:01:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-16 04:01:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[a-matter-of-trust-fords-approach-to-developing-self-driving-vehicles]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driven/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/a-matter-of-trust-fords-approach-to-developing-self-driving-vehicles-12f602887822?source=rss----7192a82ca09a---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[860]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>A transformative summer for student engineers</title>
		<link>https://fifthlevel.ai/archives/542</link>
		<pubDate>Mon, 13 Aug 2018 17:20:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/high-school-student-engineers-wrap-beaver-works-summer-stem-program-0813</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Visitors roaming the MIT Stratton Student Center chatted with high school students stationed at various booths, as 3-D printers hummed and a remote-controlled inflatable shark swam above their heads. Down the street at the Johnson Ice Rink, self-driving miniature racecars hurtled down a racetrack while onlookers cheered them on.&nbsp;</p> <p>This was the scene on Sunday, Aug. 5 at the final event of the Beaver Works Summer Institute (BWSI), a four-week summer science, technology, engineering, and math (STEM) program for rising high school seniors. BWSI is an initiative of Beaver Works, a research and education center jointly operated by MIT Lincoln Laboratory and the MIT School of Engineering. BWSI started&nbsp;in 2016 with 46 students. On&nbsp;Sunday, the program concluded its third year with 198 students from 105 schools around the country.&nbsp;</p> <p>“The Beaver Works Summer Institute is a transformational high school program that finds and attracts talented and motivated students throughout the world to science and engineering,” said&nbsp;Professor Sertac Karaman, an academic director of BWSI and an associate professor in MIT’s Department of Aeronautics and Astronautics. “At their core, all our classes offer hands-on, project-based experiences that strengthen the students’ understanding of fundamental concepts in emerging technologies of tomorrow.”&nbsp;</p> <p>This year’s BWSI featured eight courses: Autonomous RACECAR (Rapid Autonomous Complex-Environment Competing Ackermann-steering Robot) Grand Prix;&nbsp;Autonomous Air Vehicle Racing;&nbsp;Autonomous Cognitive Assistant;&nbsp;Medlytics: Data Science for Health and Medicine;&nbsp;Build a Cubesat;&nbsp;Unmanned Air System-Synthetic Aperture Radar (UAS-SAR);&nbsp;Embedded Security and Hardware Hacking;&nbsp;and Hack a 3-D Printer. All courses were supplemented by lectures, and by an online portion&nbsp;designed to teach&nbsp;the fundamentals of each topic, which students took prior to arriving at the program.</p> <p>Students from Mexico, Canada, and Nauset Regional High School in Massachusetts also participated in the courses remotely by building the technologies in their own classrooms and listening in on webcasted lectures. At the end of the program, they travelled to the MIT campus to participate in the final event.</p> <p>In the spirit of hands-on learning, students made their own 3-D printers and radars, security tested a home door lock system, and designed their own autonomous capabilities for unpiloted aerial vehicles (UAVs) and miniature racecars. Despite the challenging nature of the material, the students caught on quickly.</p> <p>“They constantly exceeded my already high expectations. Their ability to immediately engage with what is often graduate-level course materials is inspiring,” said Mark Mazumder, a lead instructor of the Autonomous Air Vehicle course and a Lincoln Laboratory staff member.</p> <p>Medlytics lead instructor and Lincoln Laboratory staff member&nbsp;Danelle Shah said the participants&nbsp;“amazed me every day.”</p> <p>“Not only are these students remarkably bright, but they are ambitious, curious, and passionate about making a difference,” Shah said.</p> <p>The students showed off the results of their hard work at the final event after four weeks of learning and building. During the first half of the day, the Autonomous RACECAR teams ran time trials at the Johnson Ice Rink in preparation for the final race later in the afternoon. At Building 31, students in the Autonomous Air Vehicle Racing course raced their drones around an LED track, while nearby, drones from the UAS-SAR course used their radar to image an object obscured by a tarp.</p> <p>Students from the remaining courses set up booths at the Stratton Student Center and talked to visitors about their projects, which included a design for a miniature satellite to be launched by NASA, a 3-D printer that could print icing on top of cakes, a cognitive assistant similar to Amazon’s Alexa, and a technique for using machine learning to detect cyberbullying on Twitter.</p> <p>The event culminated in a final grand prix-style race in which the autonomous RACECARs competed to finish an intricate racetrack. Various obstacles such as a windmill and bowling pins were placed on the path, which the cars had to navigate around using&nbsp;lidar, cameras, and motion sensors that the students had integrated into the vehicles. The race was followed by an awards ceremony and closing remarks from the BWSI organizers.</p> <p>“BWSI 2018 was a huge success, thanks to the passion and dedication of our staff and instructors and the enthusiasm of the students,” said Robert Shin, the director of Beaver Works. “In all eight engineering courses, the students far exceeded our expectations in their achievements.&nbsp;That validated our view that there is no limit to what STEM-focused high school students can achieve under the right circumstances. Our vision is to make this opportunity available to all passionate and motivated high school students everywhere.”</p> <p>For many BWSI participants, the program does not end after just four weeks. Nine of this year’s associate instructors were former BWSI students, including Karen Nguyen, who now attends MIT.</p> <p>“Before the program, MIT seemed like an unattainable goal. But working with the same autonomous racecars that were used by MIT students and professors allowed me to see that technology could be both advanced and accessible,” Nguyen said. Now on the teaching side of the program, Nguyen is still benefiting from it. “By helping instruct high school students on autonomous robotics, I furthered my own knowledge within the field and also learned how to make certain topics in STEM more approachable to a wider range of people,” she said.</p> <p>The organizers hope to expand the program in the future by helping schools develop their own local STEM programs based on the BWSI curricula. They are also brainstorming possible new course topics for next year, such as autonomous marine vehicles, disaster relief technologies, and an assistive technology hackathon. Although the third year of BWSI is now over, the organizers hope the program will have a lasting impact on the students.</p> <p>“At the end of the program, you just look at the things you accomplished and it utterly changes what you think high schoolers, given the right tools and guidance, are capable of,” said William Shi, a student who participated in the Embedded Security and Hardware Hacking course. “You feel empowered to go out and try your hand at even more ambitious things, to see how far you truly can go.”</p>
<p><b><a href="http://news.mit.edu/2018/high-school-student-engineers-wrap-beaver-works-summer-stem-program-0813" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>542</wp:post_id>
		<wp:post_date><![CDATA[2018-08-13 17:20:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-13 17:20:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[a-transformative-summer-for-student-engineers]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/high-school-student-engineers-wrap-beaver-works-summer-stem-program-0813]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple poached &#039;scores&#039; of Tesla employees in recent months, but not all go to &#039;Project Titan&#039;</title>
		<link>https://fifthlevel.ai/archives/601</link>
		<pubDate>Thu, 23 Aug 2018 22:53:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/08/23/apple-poached-scores-of-tesla-employees-in-recent-months-but-not-all-go-to-project-titan</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/27285-40385-14533-10152-12816-7087-140219-Musk-l-l-l.jpg" alt="Article Image" border="0" /> <br><br> Apple appears to have stepped up its poaching activities involving Tesla employees over the past few months, luring away manufacturing, security and software engineers, and supply chain experts to work on the &quot;Project Titan&quot; self-driving car initiative and other products, according to a new report. <p><b><a href="https://appleinsider.com/articles/18/08/23/apple-poached-scores-of-tesla-employees-in-recent-months-but-not-all-go-to-project-titan" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>601</wp:post_id>
		<wp:post_date><![CDATA[2018-08-23 22:53:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-23 22:53:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-poached-scores-of-tesla-employees-in-recent-months-but-not-all-go-to-project-titan]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/08/23/apple-poached-scores-of-tesla-employees-in-recent-months-but-not-all-go-to-project-titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[866]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple&#039;s &#039;Project Titan&#039; car could warn you what it is about to do, well before it does it</title>
		<link>https://fifthlevel.ai/archives/602</link>
		<pubDate>Tue, 21 Aug 2018 13:50:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/08/21/apples-project-titan-car-could-warn-you-what-it-is-about-to-do-well-before-it-does-it</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/27230-40216-26913-39085-26821-38809-26035-36466-applecar-testbed2-l-l-l-l.jpg" alt="Article Image" border="0" /> <br><br> Apple has come up with a way to allow a self-driving car to communicate much more than lane changing information to other drivers of maneuvers it intends to perform, with the advance warning likely to reduce the possibility of accidents involving both driven and autonomous vehicles. <p><b><a href="https://appleinsider.com/articles/18/08/21/apples-project-titan-car-could-warn-you-what-it-is-about-to-do-well-before-it-does-it" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>602</wp:post_id>
		<wp:post_date><![CDATA[2018-08-21 13:50:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-21 13:50:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apples-project-titan-car-could-warn-you-what-it-is-about-to-do-well-before-it-does-it]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/08/21/apples-project-titan-car-could-warn-you-what-it-is-about-to-do-well-before-it-does-it]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[863]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Podcast discusses Project Titan, ARM MacBook, 90GB of data stolen from Apple, and more</title>
		<link>https://fifthlevel.ai/archives/603</link>
		<pubDate>Fri, 17 Aug 2018 12:35:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/08/17/podcast-discusses-apples-money-where-it-comes-from-where-it-goes-and-more</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/27166-40035-macbook-air-2018-redesign-possible-00002-l.jpg" alt="Article Image" border="0" /> <br><br> This week on the AppleInsider Podcast, Victor and William talk through the latest news about Project Titan rumors, ARM Macbook rumors, and the strange case of the teenager who took 90GB of data from Apple. <p><b><a href="https://appleinsider.com/articles/18/08/17/podcast-discusses-apples-money-where-it-comes-from-where-it-goes-and-more" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>603</wp:post_id>
		<wp:post_date><![CDATA[2018-08-17 12:35:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-17 12:35:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[podcast-discusses-project-titan-arm-macbook-90gb-of-data-stolen-from-apple-and-more]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/08/17/podcast-discusses-apples-money-where-it-comes-from-where-it-goes-and-more]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[861]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Kuo: &#039;Apple Car&#039; likely to launch in 2023 to 2025, fuel $2 trillion company valuation</title>
		<link>https://fifthlevel.ai/archives/604</link>
		<pubDate>Wed, 15 Aug 2018 01:32:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/08/14/kuo-apple-car-likely-to-launch-in-2023-2025-fuel-2-trillion-company-valuation</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/27156-39988-16526-13438-160414-Apple_Car-l-l.jpg" alt="Article Image" border="0" /> <br><br> Noted Apple analyst Ming-Chi Kuo believes the Cupertino tech giant's autonomous vehicle aspirations -- &quot;Project Titan&quot; -- will be realized in a shipping consumer product as early as 2023, helping push the company toward its next trillion dollar valuation. <p><b><a href="https://appleinsider.com/articles/18/08/14/kuo-apple-car-likely-to-launch-in-2023-2025-fuel-2-trillion-company-valuation" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>604</wp:post_id>
		<wp:post_date><![CDATA[2018-08-15 01:32:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-15 01:32:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[kuo-apple-car-likely-to-launch-in-2023-to-2025-fuel-2-trillion-company-valuation]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/08/14/kuo-apple-car-likely-to-launch-in-2023-2025-fuel-2-trillion-company-valuation]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[859]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Doug Field returns to Apple&#039;s &#039;Project Titan&#039; after stint at Tesla</title>
		<link>https://fifthlevel.ai/archives/605</link>
		<pubDate>Fri, 10 Aug 2018 04:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/08/10/doug-field-returns-to-apples-project-titan-after-stint-at-tesla</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/27100-39793-tesla-131024-l.jpg" alt="Article Image" border="0" /> <br><br> Doug Field, who previously worked as a vice president of Mac hardware engineering at Apple, has returned to the Cupertino tech giant after a nearly five-year stint at Tesla to work on the company's secretive automotive initiative. <p><b><a href="https://appleinsider.com/articles/18/08/10/doug-field-returns-to-apples-project-titan-after-stint-at-tesla" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>605</wp:post_id>
		<wp:post_date><![CDATA[2018-08-10 04:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-10 04:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[doug-field-returns-to-apples-project-titan-after-stint-at-tesla]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/08/10/doug-field-returns-to-apples-project-titan-after-stint-at-tesla]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[858]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Volvo Drops New Teaser Of What Looks To Be An Autonomous Concept</title>
		<link>https://fifthlevel.ai/archives/630</link>
		<pubDate>Tue, 28 Aug 2018 10:16:11 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/263856/volvo-autonomous-concept-teaser/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[We could be wrong, but this is most likely a self-driving EV study. <p><b><a href="https://www.motor1.com/news/263856/volvo-autonomous-concept-teaser/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>630</wp:post_id>
		<wp:post_date><![CDATA[2018-08-28 10:16:11]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-28 10:16:11]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[volvo-drops-new-teaser-of-what-looks-to-be-an-autonomous-concept]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/263856/volvo-autonomous-concept-teaser/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Formula Student Germany 2018</title>
		<link>https://fifthlevel.ai/archives/743</link>
		<pubDate>Mon, 20 Aug 2018 07:29:15 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://aid-driving.eu/?p=3908</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Der Beitrag <a rel="nofollow" href="http://aid-driving.eu/formula-student-germany-2018/">Formula Student Germany 2018</a> erschien zuerst auf <a rel="nofollow" href="http://aid-driving.eu">AID</a>.</p> <p><b><a href="http://aid-driving.eu/formula-student-germany-2018/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>743</wp:post_id>
		<wp:post_date><![CDATA[2018-08-20 07:29:15]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-20 07:29:15]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[formula-student-germany-2018]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://aid-driving.eu/formula-student-germany-2018/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Toyota to invest $500m in Uber for self-driving car programme</title>
		<link>https://fifthlevel.ai/archives/1297</link>
		<pubDate>Tue, 28 Aug 2018 07:37:47 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.theguardian.com/business/2018/aug/28/toyota-to-invest-500m-in-uber-for-self-driving-car-programme</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Firms expand relationship in bid to catch up with rivals over autonomous vehicles</p><p>The Japanese carmaker Toyota is to invest $500m (£388m) in the ride-hailing company Uber as the two companies expand their partnership on the development of self-driving cars.</p><p>It deepens an existing relationship in a bid to catch up with rivals in the race to design and produce autonomous vehicles for the mass market. </p><p> <span>Related: </span><a href="https://www.theguardian.com/technology/2018/aug/27/uber-to-diversify-into-electric-bikes-and-scooters-to-drive-growth">Uber to diversify into electric bikes and scooters to drive growth</a> </p> <a href="https://www.theguardian.com/business/2018/aug/28/toyota-to-invest-500m-in-uber-for-self-driving-car-programme">Continue reading...</a> <p><b><a href="https://www.theguardian.com/business/2018/aug/28/toyota-to-invest-500m-in-uber-for-self-driving-car-programme" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1297</wp:post_id>
		<wp:post_date><![CDATA[2018-08-28 07:37:47]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-28 07:37:47]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[toyota-to-invest-500m-in-uber-for-self-driving-car-programme]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/self-driving-cars/rss]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/business/2018/aug/28/toyota-to-invest-500m-in-uber-for-self-driving-car-programme]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Driverless taxi debuts in Tokyo in &#039;world first&#039; trial ahead of Olympics</title>
		<link>https://fifthlevel.ai/archives/1298</link>
		<pubDate>Tue, 28 Aug 2018 05:23:30 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.theguardian.com/technology/2018/aug/28/driverless-taxi-debuts-in-tokyo-in-world-first-trial-ahead-of-olympics</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Minivan equipped with sensors makes four round-trips a day on a busy stretch of road, picking up paying passengers</p><p>A self-driving taxi has successfully taken paying passengers through the busy streets of Tokyo, raising the prospect that the service will be ready in time to ferry athletes and tourists between sports venues and the city centre during the 2020 Summer Olympics.</p><p>ZMP, a developer of <a href="https://www.theguardian.com/technology/self-driving-cars">autonomous driving</a> technology, and the taxi company Hinomaru Kotsu, claim that the road tests, which began this week, are the first in the world to involve driverless taxis and fare-paying passengers.</p><p> <span>Related: </span><a href="https://www.theguardian.com/environment/2018/aug/23/kalashnikov-takes-on-tesla-with-retro-look-electric-supercar">Kalashnikov takes on Tesla with retro-look electric 'supercar'</a> </p> <a href="https://www.theguardian.com/technology/2018/aug/28/driverless-taxi-debuts-in-tokyo-in-world-first-trial-ahead-of-olympics">Continue reading...</a> <p><b><a href="https://www.theguardian.com/technology/2018/aug/28/driverless-taxi-debuts-in-tokyo-in-world-first-trial-ahead-of-olympics" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1298</wp:post_id>
		<wp:post_date><![CDATA[2018-08-28 05:23:30]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-28 05:23:30]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[driverless-taxi-debuts-in-tokyo-in-world-first-trial-ahead-of-olympics]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/self-driving-cars/rss]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/2018/aug/28/driverless-taxi-debuts-in-tokyo-in-world-first-trial-ahead-of-olympics]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Four Companies Selling Self-Driving Car Platforms Today, And One Coming</title>
		<link>https://fifthlevel.ai/archives/2467</link>
		<pubDate>Wed, 22 Aug 2018 19:20:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b7da54464aaf93ef039f658</guid>
		<description></description>
		<content:encoded><![CDATA[Getting the right hardware is one of the challenges of building self-driving cars. Here are four companies selling self-driving platforms today, and one coming soon. <p><b><a href="https://www.forbes.com/sites/davidsilver/2018/08/22/four-companies-selling-self-driving-car-platforms-today-and-one-coming/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2467</wp:post_id>
		<wp:post_date><![CDATA[2018-08-22 19:20:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-22 19:20:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[four-companies-selling-self-driving-car-platforms-today-and-one-coming]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/08/22/four-companies-selling-self-driving-car-platforms-today-and-one-coming/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>An Apple Car Would Require An Automotive Foxconn</title>
		<link>https://fifthlevel.ai/archives/2468</link>
		<pubDate>Thu, 16 Aug 2018 04:42:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b73baf364aaf942a5e2f99c</guid>
		<description></description>
		<content:encoded><![CDATA[Apple products are typically designed in California and assembled in China. But it's not clear which companies would even be capable of assembling an Apple self-driving car. <p><b><a href="https://www.forbes.com/sites/davidsilver/2018/08/16/building-an-apple-car-would-require-an-automotive-foxconn/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2468</wp:post_id>
		<wp:post_date><![CDATA[2018-08-16 04:42:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-16 04:42:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[an-apple-car-would-require-an-automotive-foxconn]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/08/16/building-an-apple-car-would-require-an-automotive-foxconn/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Tesla Model 3 Autopilot Outperforms Competitors In Lane-Keeping Test</title>
		<link>https://fifthlevel.ai/archives/2469</link>
		<pubDate>Wed, 15 Aug 2018 05:28:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b73b25c64aaf942a5e2f994</guid>
		<description></description>
		<content:encoded><![CDATA[In a test of lane-keeping ability, the Tesla Model 3 outperformed competitors from other manufacturers and even the Tesla Model S. <p><b><a href="https://www.forbes.com/sites/davidsilver/2018/08/15/tesla-model-3-autopilot-outperforms-model-s-competitors-in-lane-keeping-test/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2469</wp:post_id>
		<wp:post_date><![CDATA[2018-08-15 05:28:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-15 05:28:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[tesla-model-3-autopilot-outperforms-competitors-in-lane-keeping-test]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/08/15/tesla-model-3-autopilot-outperforms-model-s-competitors-in-lane-keeping-test/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Infusing Ford’s Future with Mustang’s Pride and Passion</title>
		<link>https://fifthlevel.ai/archives/2541</link>
		<pubDate>Thu, 09 Aug 2018 18:37:47 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/7fc97ca2dc2b</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Jim Farley, Executive Vice President and President, Global Markets, Ford Motor Company</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bM-A_ZGrslxUu8DHd3dU4w.jpeg" /></figure><p>In 1964, a Canadian pilot named Stanley Tucker took the very first Ford Mustang ever made out for a test drive. He never brought it back.</p><p>The vehicle — in perfect “Wimbledon White” — was never meant to be sold. It was a show car for test drives only. But Stanley loved it so much that he simply said, “no thanks” for two years until Ford promised him Mustang No. 1,000,001 in exchange for the original. (Stanley’s car now resides safe and sound at the Henry Ford Museum.)</p><p>Stanley wasn’t the only one who fell in love with the Mustang at first sight: On the day it debuted at the 1964 New York World’s Fair, over 22,000 people purchased a Mustang without even getting behind the wheel.</p><p>It was an instant classic. And the sexiest car on the market.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PibS6R8cSy6J4Hh7czGtgQ.jpeg" /></figure><p>In fact, the Ford Mustang created a whole new class of vehicle, becoming the world’s first pony car — despite not actually being named after the animal. Designer John Najjar suggested the name Mustang after the World War II P-51 Mustang fighter plane (though the plane was named after the horse so it’s probably fine). Since its release, it has made appearances in over 500 movies to date, from Sean-Connery-as-James-Bond’s car in Goldfinger, to the star of the most memorable car chase scene in cinematic history in Steve McQueen’s Bullitt.</p><p>It’s been the subject of dozens of popular songs, including multiple love songs where the object of desire was the car itself. And while it’s impossible to put a price on everything the Mustang has come to represent, a 1967 Shelby GT500 Super Snake model once sold at auction for $1.3 million.</p><p>I’m familiar with the kind of devotion Mustang inspires. My first Mustang was a ’66 coupe I bought for $500 with money from my first job in California. When it came time to visit my family in Michigan, there was absolutely no way I was leaving that car — no <em>way</em>. So I drove it there — underage, without a driver’s license. While I certainly can’t condone that, I couldn’t look you in the eye and say I regret it either. Why?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*VEMookqnB1ZNxwGT75973w.gif" /></figure><p>Because Mustang. To this day, the Ford Mustang remains the international symbol of American cool, the patron saint of steamy windows everywhere. It’s the official car of heartthrobs, the one that you want to drive but also want to be driven in.</p><p>It evokes emotion, pride, swagger. The name, the look, the sound — all instantly recognizable. Now we’re building on that as we shape our future at Ford. We’re taking that success — that swagger — and infusing it in our upcoming silhouettes and a Mustang-inspired exciting electric utility coming in 2020. These new vehicles are ones that people will feel connected to and passionate about — just like they do with Mustang today.</p><p><a href="https://medium.com/@ford/returning-to-detroit-to-power-up-our-electrified-self-driving-future-fd7fb7ab7dd8">Returning to Detroit to Power Up Our Electrified, Self-Driving Future</a></p><p>Yesterday, Ford officially produced its 10 millionth Mustang. A parade of Mustangs representing all 54 model years roared from Dearborn to our Flat Rock Assembly Plant where our amazing team celebrated this achievement. A trio of P-51 fighter planes streaked overhead, and “Mustang Sally” pumped from the speakers. Mr. Tucker’s Wimbledon White Mustang was on display — right next to the 10 millionth Mustang — also white! Where else would you rather be?</p><p>Ten million — can you believe it? Ten million cars that have turned heads on every road they’ve touched. Ten million memories. I could not have been more proud. And there’s so much more to come.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FdNqtw7Ef1nc%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DdNqtw7Ef1nc&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FdNqtw7Ef1nc%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/f643714091c1791e9a43b92343c4f559/href">https://medium.com/media/f643714091c1791e9a43b92343c4f559/href</a></iframe><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7fc97ca2dc2b" width="1" height="1"> <p><a href="https://medium.com/@ford/infusing-fords-future-with-mustang-s-pride-and-passion-7fc97ca2dc2b?source=rss-db92c082f24a------2" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2541</wp:post_id>
		<wp:post_date><![CDATA[2018-08-09 18:37:47]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-09 18:37:47]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[infusing-fords-future-with-mustangs-pride-and-passion]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@ford]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@ford/infusing-fords-future-with-mustang-s-pride-and-passion-7fc97ca2dc2b?source=rss-db92c082f24a------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>HD Maps for the Masses</title>
		<link>https://fifthlevel.ai/archives/3252</link>
		<pubDate>Fri, 17 Aug 2018 18:58:51 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/9a0d582dd274</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The future of navigation will be centralized around HD maps. While plenty of companies exist to provide this type of data, all are limited by paywalls and strict Terms-of-Service. It is time for this paradigm to change. <strong>Comma.ai is releasing US interstate HD maps from our </strong><a href="https://community.comma.ai/leaderboard.php"><strong>5,000,000+</strong></a><strong> miles of data.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nkX-yrAcy-KACRnTwwz0Ew.png" /><figcaption>A slice of highway data around the United States</figcaption></figure><h4>History</h4><p>In the early 2000s, several navigation companies started creating maps of the US roadways; changing the way people thought about driving. However, the groups capable of creating these maps were restricted to those who had the finances. As time went on, and the internet matured, services such as Google Maps provided free access to their data. This was fine for your average consumer but licensing restrictions limited commercial use of map data. To quote OpenStreetMaps (OSM):</p><blockquote>Most hackers around the world are familiar with the difference between “free” as in “free beer” and as in “free speech”. Google Maps are free as in beer, not as in speech.</blockquote><p>This ultimately led to the creation of projects such as OpenStreetMaps and corporate backers like Mapbox who help to maintain map integrity. As of 2018, the number of companies using open-source maps has grown as quality increases and <a href="http://geoawesomeness.com/developers-up-in-arms-over-google-maps-api-insane-price-hike/">prices for access to commercial data rise</a>.</p><h4>HD Maps</h4><p>The next frontier of mapping is high-precision maps that are updated frequently. Yet the same problems seen in the infancy of the industry have re-materialized in HD mapping: either have the capital to build your own maps or concede to license someone else’s data. Properly building maps requires a significant amount of resources including a fleet of vehicles, processing power, and infrastructure. Meanwhile, purchasing a license to map data can be incredibly expensive.</p><p>But what even are HD Maps? Are they fundamentally different or is it just another buzzword thrown around in marketing departments? The data in OSM or Google Maps was primarily optimized for assisting humans, resulting in an over simplified and abstract representation of the world. Self-driving cars have generated an entirely new set of requirements. HD maps used for self-driving need to be a complete centimeter-accurate depiction of the real world, containing all things relevant to road navigation such as lanes, curvature and road boundaries.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tliKcoYdTqMvs1RfjVuFXg.png" /><figcaption>High Precision Lanes from I-280</figcaption></figure><p>Comma.ai uses fleet data from <a href="https://comma.ai/shop/products/eon-dashcam-devkit/">EON</a> and <a href="https://comma.ai/shop/products/panda-obd-ii-dongle/">Grey Panda</a> to build an accurate representation of highways throughout the United States. Therefore, as our fleet grows, the likelihood of our maps representing the real world increases. The end result is a map that is not only open and decimeter accurate, but also up-to-date. As of today, 75,000+ miles of roadways are updated per-week.</p><h3>Comma Maps</h3><p>In the future, we plan on making data available to the public. Since comma.ai’s initial <a href="https://www.youtube.com/watch?v=N9WxlR1ZTZc">announcement at the Mapbox conference</a>, we have continued developing tools and infrastructure to facilitate building our maps.</p><p>Our new map viewer, called “comma maps,” visualizes recorded paths driven, lane markings, and world features.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*E2o2oLRRg0maVEUt9ZztNQ.gif" /><figcaption>World Features from the Bay Bridge</figcaption></figure><p>In addition to a viewer, we also created infrastructure to tile our data. These tools are also being open-sourced, feel free to check out our project — <a href="https://github.com/commaai/entium">entium</a> — if you are interested in our tiling process. (<em>Shout-out to </em><a href="http://cesiumjs.org"><em>Cesium</em></a><em> and </em><a href="http://entwine.io"><em>entwine</em></a><em> for providing awesome open-source tools that helped make “comma maps” possible</em>)</p><h4>When can I get access?</h4><p>Follow us on Twitter @ <a href="https://medium.com/u/330bac69b283">comma ai</a> for updates and announcements regarding data availability. Also, if you are interested in helping grow the mapping network, grab an EON devkit with Grey Panda from <a href="https://comma.ai/shop/products/eon-dashcam-devkit/">comma.ai/shop</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9a0d582dd274" width="1" height="1"> <p><b><a href="https://medium.com/@comma_ai/hd-maps-for-the-masses-9a0d582dd274?source=rss-330bac69b283------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3252</wp:post_id>
		<wp:post_date><![CDATA[2018-08-17 18:58:51]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-17 18:58:51]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[hd-maps-for-the-masses]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@comma_ai/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@comma_ai/hd-maps-for-the-masses-9a0d582dd274?source=rss-330bac69b283------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>I was keeping it open, but I don’t think it’s safe for use in production.</title>
		<link>https://fifthlevel.ai/archives/3585</link>
		<pubDate>Thu, 23 Aug 2018 16:14:34 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/7846faeeee34</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I was keeping it open, but I don’t think it’s safe for use in production. There are different cases as well as different carrier boards you can choose from. Take a look at the list of companies here: <a href="https://developer.nvidia.com/embedded/community/ecosystem">https://developer.nvidia.com/embedded/community/ecosystem</a></p><p>See the list of ports of the SDK here: <a href="https://www.nvidia.com/object/jetson-tk1-embedded-dev-kit.html">https://www.nvidia.com/object/jetson-tk1-embedded-dev-kit.html</a></p><p>It has 1 full size HDMI.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7846faeeee34" width="1" height="1"> <p><b><a href="https://medium.com/@surmenok/i-was-keeping-it-open-but-i-dont-think-it-s-safe-for-use-in-production-7846faeeee34?source=rss-d002f056f8c------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3585</wp:post_id>
		<wp:post_date><![CDATA[2018-08-23 16:14:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-23 16:14:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[i-was-keeping-it-open-but-i-dont-think-its-safe-for-use-in-production]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@surmenok]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@surmenok/i-was-keeping-it-open-but-i-dont-think-it-s-safe-for-use-in-production-7846faeeee34?source=rss-d002f056f8c------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Navimotive Conference</title>
		<link>https://fifthlevel.ai/archives/135</link>
		<pubDate>Sat, 08 Sep 2018 01:12:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/f703260ea64</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qhVx2g520zlnK_9WeKJuBw.jpeg" /></figure><p>Next Saturday, September 15, I am excited to speak at <a href="https://www.navimotive.com/">Navimotive</a>, organized by <a href="https://www.intellias.com/">Intellias</a>, in Kiev, Ukraine!</p><p>Self-driving cars really are becoming a world-wide phenomenon, with contributions across the entire world. Intellias is a Ukranian automotive supplier that works with companies like Volkswagen, HERE, and Yandex. There will also be speakers from <a href="https://www.globallogic.com/">GlobalLogic</a>, <a href="https://cloudmade.com/">CloudMade</a>, <a href="https://maps.me/">MAPS.ME</a>, and more.</p><p>I’ll talk about <a href="https://medium.com/udacity/how-the-udacity-self-driving-car-works-575365270a40">Carla, Udacity’s self-driving car</a>, and share some information about upcoming Nanodegree programs we’re building.</p><p><a href="https://www.navimotive.com/#pricing">You should come, too!</a></p><p>If you are a Udacity student in the area, please send me an email (david.silver@udacity.com) and I’d love to arrange a meetup.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f703260ea64" width="1" height="1"><hr><p><a href="https://medium.com/self-driving-cars/navimotive-conference-f703260ea64">Navimotive Conference</a> was originally published in <a href="https://medium.com/self-driving-cars">Self-Driving Cars</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>135</wp:post_id>
		<wp:post_date><![CDATA[2018-09-08 01:12:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-08 01:12:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[navimotive-conference]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driving-cars]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driving-cars/navimotive-conference-f703260ea64?source=rss----bdc6e635b3c0---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[878]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Securing Our Roads for Self-Driving Vehicles</title>
		<link>https://fifthlevel.ai/archives/383</link>
		<pubDate>Thu, 06 Sep 2018 10:31:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/9b8e68a4632b</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Sherif Marakby, CEO, Ford Autonomous Vehicles LLC</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/693/1*qb0AGgyocrJt0qlvshUoKw.jpeg" /><figcaption>A federal framework for self-driving vehicles is necessary.</figcaption></figure><p>This week, I sent a letter urging the Senate to take up legislation that establishes national guidelines for the safe deployment of autonomous vehicles. At Ford, we sincerely appreciate Congress’ commitment to making sure self-driving vehicles are safe for everyone, and believe now is the time to act.</p><p>Passing the AV START Act legislation would mark a pivotal moment, allowing us to safely deploy self-driving vehicles and ensure the United States maintains its leadership role with regards to autonomous technology.</p><p>To date, the U.S. House of Representatives has passed a bill that would allow self-driving vehicles to be tested on public roads while pushing the National Highway Traffic Safety Administration to develop federal standards for the entire industry. The Senate has a similar proposal on the table, and I strongly believe lawmakers should bring it up for a vote so that a bill can be sent to the president’s desk for his signature.</p><p>At Ford we are working hard to complete our goal of launching self-driving vehicles at scale in 2021, and we believe ensuring they are safe, reliable and trustworthy is absolutely crucial. We recently published “<a href="https://medium.com/self-driven/a-matter-of-trust-fords-approach-to-developing-self-driving-vehicles-12f602887822">A Matter of Trust</a>,” a report that outlines our approach to self-driving vehicle development. It details how we always prioritize safety at Ford as we work closely with both industry and government partners to deploy these vehicles.</p><p>You can view the letter I sent to Senators John Thune (R-SD), Bill Nelson (D-FL) and Gary Peters (D-MI) below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9rrmgRG8gH6WiqIAAA4Peg.jpeg" /></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9b8e68a4632b" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/securing-our-roads-for-self-driving-vehicles-9b8e68a4632b">Securing Our Roads for Self-Driving Vehicles</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/securing-our-roads-for-self-driving-vehicles-9b8e68a4632b?source=rss----7192a82ca09a---4" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>383</wp:post_id>
		<wp:post_date><![CDATA[2018-09-06 10:31:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-06 10:31:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[securing-our-roads-for-self-driving-vehicles]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driven/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/securing-our-roads-for-self-driving-vehicles-9b8e68a4632b?source=rss----7192a82ca09a---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[872]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Being a Better Place to Work Helps Us Build a Better Self-Driving Car</title>
		<link>https://fifthlevel.ai/archives/384</link>
		<pubDate>Thu, 06 Sep 2018 10:31:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/9288e8572b35</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Bryan Salesky, CEO, Argo AI</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pa3FKU19eS2PMJGdjWHC5Q.jpeg" /></figure><p>Every day Argo AI employees come to work, we all strive toward a higher purpose — transforming how the world moves. We don’t take this lightly, and that’s why it has always been important to me to make Argo AI the best place to work — a space where people want to be each day because they love what they do.</p><p>But how do we do this? Pete and I have created a workplace of transparency and collaboration, while promoting work/life balance. However, I think creating a place of openness is what truly makes Argo AI stand apart and it’s something our employees continually say they love. This unique environment is a positive one that allows us to grow and make continued progress against our goals.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uW5DicpfWL2DUPwguWmx_w.jpeg" /></figure><p>Not only did we recently move our headquarters into a brand-new building in Pittsburgh’s Strip District, I am also excited to share that Argo AI has been named to the <a href="https://www.linkedin.com/pulse/linkedin-top-startups-2018-50-most-sought-after-us-daniel-roth/"><strong>2018 LinkedIn Top Startups List</strong></a>, which honors the 50 most sought-after startups nationwide people want to work for.</p><p>I want to take this opportunity to congratulate and thank our team. I appreciate all the energy, passion and commitment they bring — they truly are the best! The Top Startups recognition is a testament to each employee at Argo AI and the value they bring daily to make our team and product the best it can be.</p><p>With teams across the U.S., including in Pittsburgh; Miami; Dearborn, Michigan; Mountain View, California; and Princeton, New Jersey, it’s important we pull away from traditional workplace cultural norms to leverage the talent we have from coast to coast and truly collaborate. To do this, we must break the geographic barriers. We strongly encourage interoffice travel among our team members so they get to know their colleagues — you don’t need a reason, just do it!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2T3zVNu83tHyHDWMXZvaaQ.jpeg" /></figure><p>Our leadership team also holds regular all-hands video meetings to bring everyone in the company together. This is one of my favorite things because we get to celebrate what we have accomplished and spotlight different employees who get to share what they are working on. Plus, our leadership can share where we are headed — the good and the bad — in an open and honest way. It is a time of community and collaboration that keeps us focused on our end goal and enables us to learn from each other.</p><p>Our safety drivers also have daily briefs at the beginning and end of their shift to make sure our work is cohesive across cities — reinforcing safety measures and sharing what they are working on inside and outside of work. While we take it very seriously, it is also a fun environment. As a fly on the wall, you feel like you are at a pep rally preparing for the big game and celebrating your teammates.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FozIraj4PcTg%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DozIraj4PcTg&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FozIraj4PcTg%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/56939192757c6cb51200626dbf2f9b85/href">https://medium.com/media/56939192757c6cb51200626dbf2f9b85/href</a></iframe><p>Most importantly, the leadership team and I sit in the open-floor space with the engineers because we want to be accessible and deeply involved in development of our product. We encourage the team to email us any questions or concerns they have anytime. This open environment creates a positive space, makes employees feel empowered, and allows us to move faster to work together. We are one team and when we make an achievement in one area, it is the entire company that is celebrated.</p><p>It’s very important as I think about the future of Argo AI that we create a university mentality in which everyone continues to learn from one another. Our team members come from diverse backgrounds, like TV production, teaching and even sports. We encourage everyone to share their ideas and to take a ride in our self-driving car — even if they aren’t an engineer. Our leadership team takes rides regularly. As we are dealing with complex engineering problems, having people involved from different backgrounds encourages everyone to think about things in new ways.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6l6964OZH0BRZuD8oh2oLw.jpeg" /></figure><p>We already have much to celebrate, in that self-driving vehicles will be one of the most transformative advancements in this century. Our employees are what make Argo AI the great place that it is, and we would be nothing without each person who contributes their brain power daily to make self-driving cars a reality and safe for everyone.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9288e8572b35" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/being-a-better-place-to-work-helps-us-build-a-better-self-driving-car-9288e8572b35">Being a Better Place to Work Helps Us Build a Better Self-Driving Car</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/being-a-better-place-to-work-helps-us-build-a-better-self-driving-car-9288e8572b35?source=rss----7192a82ca09a---4" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>384</wp:post_id>
		<wp:post_date><![CDATA[2018-09-06 10:31:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-06 10:31:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[being-a-better-place-to-work-helps-us-build-a-better-self-driving-car]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driven/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/being-a-better-place-to-work-helps-us-build-a-better-self-driving-car-9288e8572b35?source=rss----7192a82ca09a---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[873]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The Stakes Are High: Inside the Team Developing Ford’s New Generation of Electric Vehicles</title>
		<link>https://fifthlevel.ai/archives/393</link>
		<pubDate>Thu, 06 Sep 2018 13:31:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/2f105026012f</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By <em>Darren Palmer, Ford Team Edison Global Product Development Director</em></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*J8yWtPmidRcweIRLJvqKoQ.jpeg" /><figcaption>Ford’s all-new Mustang-inspired fully-electric performance utility arrives in 2020 with targeted range of 300 miles.</figcaption></figure><p>You’d think after 28 years of working for the same company that nothing would surprise me. I’ve been fortunate enough to work for Ford across many amazing teams on many exciting assignments, most recently leading development of the next-generation Mustang.</p><p>But earlier this year I received a call that would challenge everything I thought I knew about Ford and our future. I was asked to lead product development for a brand-new team, Ford Team Edison, focusing exclusively on electrified vehicles for both Ford and Lincoln. And to be successful, this new team had to be willing to challenge every truth and every process we had developed over the course of our careers.</p><p>Having just taken delivery of a Shelby GT350R, those in the dark about electric vehicles might think that they chose the wrong guy in that I’m a Mustang enthusiast. But as my team and I have quickly found out, the new generation of electric vehicles is just as exciting — only different. Different can be good. Very good. And it’s opened our eyes to a whole new Ford. The stakes are high. The challenge higher. We are being tasked to set the future trajectory of the company, and Team Edison is up for that challenge.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jCCO1jB_aF3DcU64wShpEA.jpeg" /><figcaption>Darren Palmer, global product development director of Ford’s Team Edison.</figcaption></figure><p>With Ford, I’ve had the opportunity to work around the globe — in Germany, India, China, South America and my home in the U.K. — and this new role with Ford Team Edison has allowed me to fully leverage my global experience.</p><p>The electric vehicle market overall is accelerating at an exponential rate, with each individual market presenting its own unique challenges. It’s an exciting time to be bringing forward a winning portfolio of electrified vehicles. My team and I are both proud of and energized by the company’s $11 billion investment to bring 16 fully electric vehicles within a global portfolio of 40 electrified vehicles through 2022. All of us here have unknowingly prepared for this our entire careers.</p><p>We’re a dedicated team who has been lucky enough to be chosen to pilot the future of Ford from an old factory in the heart of Corktown, Detroit’s oldest neighborhood. It’s open, airy and encourages collaboration. But don’t be fooled that new desks and an open office floor plan alone creates change. It’s the shift in mindset that is truly creating change at Ford, and giving license to the team to operate in a completely different way.</p><p><a href="https://medium.com/@ford/returning-to-detroit-to-power-up-our-electrified-self-driving-future-fd7fb7ab7dd8">Returning to Detroit to Power Up Our Electrified, Self-Driving Future</a></p><p>Change doesn’t happen overnight, but for our team, things are moving quickly. The team is cross-functional, and on any given day you can find yourself sitting next to someone working to market our electric vehicles, someone looking at the profit potential of our electric vehicles, or be on a coffee break with someone involved in our charging strategy.</p><p>We’ve been tasked to move fast and come together quickly to solve common problems — what we like to call “cross-functional sprints,” and we’ve embraced a “no stripes mentality” to encourage that ideas come from different viewpoints.</p><p>The other big change is the human-centric approach in everything we do. There isn’t a ton of historical data to look at for electric vehicles — meaning you can’t just look back at what happened before and use that as inspiration for what to do next. The world is fundamentally shifting, and we have to listen more than ever to really understand our customers and how they are evolving.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xGlFSBUnusaWhdNIR7zDTA.jpeg" /><figcaption>Led by Ted Cannis, Ford Team Edison is the company’s dedicated global battery electric vehicle team. They have been challenged by company leadership to think big and move fast to deliver vehicles and experiences that are uniquely Ford and human centric in design.</figcaption></figure><p>For example, on a recent trip to Shanghai, we looked at how extended families use their vehicles. We all wondered what on earth we were doing watching families get in and out of a three-row vehicle. I, of course, assumed that kids would go right back in the third row — but I was wrong. In China, children usually go straight to the second row, as it’s deemed the best place since the child represents the future. The grandmother generally rides up front and the grandfather climbs into the back. Goes to show you can — and should — always be learning something new that can be applied to improve the customer experience. We can take these insights and ensure we’re really designing vehicles in a way that is human-centered and right for each unique market.</p><p>Prototypes also play a big part in what we’re doing, allowing us to pivot along the way to deliver the best products and services possible. Having the flexibility to learn and iterate is a huge enabler. What I’ve come to learn is that design has to be intuitive. I only have to watch my 6-year-old twins play with my iPhone to know what intuitive design looks like. Gone are the days of shiny, expensive prototypes. Customers don’t care about that — a low-fidelity cardboard prototype is enough to get feedback.</p><p>This way of thinking is all part of our new, fast-moving team mantra. In fact, as a reminder of this, I’ve kept one of our first prototypes of the infotainment system for one of our new electric vehicles. It’s literally cardboard, with a piece of a plastic cup stuck to it with tape. What I’m trying to demonstrate is that innovation does not have to be expensive. It has to be smart. And I’m surrounded by some of the smartest people I’ve ever worked with.</p><p>Electric vehicle customers are buying into the future and our team is 100 percent focused on not only delivering vehicles they will love, but providing an entire electric vehicle ecosystem that works flawlessly. It’s exciting to know that my stint leading development of the next-generation Mustang actually comes full circle as we get ready to launch our electric Mustang-inspired utility. I, for one, can’t wait to have one — side-by-side — with my Shelby GT350R.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2f105026012f" width="1" height="1"><hr><p><a href="https://medium.com/live-electric/the-stakes-are-high-inside-the-team-developing-fords-new-generation-of-electric-vehicles-2f105026012f">The Stakes Are High: Inside the Team Developing Ford’s New Generation of Electric Vehicles</a> was originally published in <a href="https://medium.com/live-electric">Live Electric</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/live-electric/the-stakes-are-high-inside-the-team-developing-fords-new-generation-of-electric-vehicles-2f105026012f?source=rss-db92c082f24a------2" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>393</wp:post_id>
		<wp:post_date><![CDATA[2018-09-06 13:31:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-06 13:31:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[the-stakes-are-high-inside-the-team-developing-fords-new-generation-of-electric-vehicles]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@ford]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/live-electric/the-stakes-are-high-inside-the-team-developing-fords-new-generation-of-electric-vehicles-2f105026012f?source=rss-db92c082f24a------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[874]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Automotive Edge Computing Consortium to Present at Mobile World Congress Americas</title>
		<link>https://fifthlevel.ai/archives/548</link>
		<pubDate>Thu, 30 Aug 2018 12:00:42 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://aecc.org/?p=469</guid>
		<description></description>
		<content:encoded><![CDATA[<p>AECC Presentation and Panels, including “Driving Data to the Edge,” Examine Ways to Drive the Connected Vehicle Ecosystem Toward a Big Data Future WAKEFIELD, Mass., – USA – Aug. 30, 2018 –The Automotive Edge Computing Consortium (AECC) today announced that it will lead the presentation and panel titled “Driving Data to the Edge” at the &#91;...&#93;</p>
<p>The post <a rel="nofollow" href="https://aecc.org/automotive-edge-computing-consortium-to-present-at-mobile-world-congress-americas/">Automotive Edge Computing Consortium to Present at Mobile World Congress Americas</a> appeared first on <a rel="nofollow" href="https://aecc.org">Automotive Edge Computing Consortium</a>.</p> <p><b><a href="https://aecc.org/automotive-edge-computing-consortium-to-present-at-mobile-world-congress-americas/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>548</wp:post_id>
		<wp:post_date><![CDATA[2018-08-30 12:00:42]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-30 12:00:42]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[automotive-edge-computing-consortium-to-present-at-mobile-world-congress-americas]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://aecc.org/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://aecc.org/automotive-edge-computing-consortium-to-present-at-mobile-world-congress-americas/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple partner Didi staring down possible $585 million loss in first half of year</title>
		<link>https://fifthlevel.ai/archives/596</link>
		<pubDate>Fri, 07 Sep 2018 13:38:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/09/07/apple-partner-didi-reportedly-lost-585-million-in-first-half-of-year</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/27479-41131-21039-23579-20487-22332-170308-Didi-l-l-(1)-l.jpg" alt="Article Image" border="0" /> <br><br> Chinese ride-hailing company Didi, backed by Apple to the tune of $1 billion, has struggled to grow and is bleeding cash in China amid competition and regulatory scrutiny. <p><b><a href="https://appleinsider.com/articles/18/09/07/apple-partner-didi-reportedly-lost-585-million-in-first-half-of-year" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>596</wp:post_id>
		<wp:post_date><![CDATA[2018-09-07 13:38:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-07 13:38:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-partner-didi-staring-down-possible-585-million-loss-in-first-half-of-year]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/09/07/apple-partner-didi-reportedly-lost-585-million-in-first-half-of-year]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[877]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple working on headlight system that could highlight road hazards for drivers</title>
		<link>https://fifthlevel.ai/archives/597</link>
		<pubDate>Thu, 06 Sep 2018 13:46:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/09/06/apple-working-on-headlight-system-that-could-highlight-road-hazards-for-drivers</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/27463-41074-27230-40216-26913-39085-26821-38809-26035-36466-applecar-testbed2-l-l-l-l-l.jpg" alt="Article Image" border="0" /> <br><br> Apple is considering ways to make drivers more aware of potential road hazards, including headlights that can single out road elements that the driver needs to be aware of, and a heads-up display that can highlight pedestrians and other items of interest on a car's windscreen. <p><b><a href="https://appleinsider.com/articles/18/09/06/apple-working-on-headlight-system-that-could-highlight-road-hazards-for-drivers" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>597</wp:post_id>
		<wp:post_date><![CDATA[2018-09-06 13:46:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-06 13:46:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-working-on-headlight-system-that-could-highlight-road-hazards-for-drivers]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/09/06/apple-working-on-headlight-system-that-could-highlight-road-hazards-for-drivers]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[875]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple records first-ever accident in self-driving car program [u]</title>
		<link>https://fifthlevel.ai/archives/598</link>
		<pubDate>Fri, 31 Aug 2018 22:10:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/08/31/apple-records-first-ever-accident-in-self-driving-car-program</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/27409-40847-applecar-testbed2-l.jpg" alt="Article Image" border="0" /> <br><br> Apple's secretive self-driving car program has reported its first-ever accident, though no one was hurt. [Updated with more details] <p><b><a href="https://appleinsider.com/articles/18/08/31/apple-records-first-ever-accident-in-self-driving-car-program" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>598</wp:post_id>
		<wp:post_date><![CDATA[2018-08-31 22:10:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-31 22:10:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-records-first-ever-accident-in-self-driving-car-program-u]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/08/31/apple-records-first-ever-accident-in-self-driving-car-program]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[869]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Warren Buffett buys &#039;just a little&#039; more Apple stock, warns against entering auto business</title>
		<link>https://fifthlevel.ai/archives/599</link>
		<pubDate>Thu, 30 Aug 2018 22:51:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/08/30/warren-buffett-buys-just-a-little-more-apple-stock-warns-against-entering-auto-business</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/27388-40753-25916-36118-24819-32869-20204-21609-Warren-Buffett-l-l-l-l.jpg" alt="Article Image" border="0" /> <br><br> Thursday is Warren Buffett's 88th birthday, and to celebrate, the legendary investor sat down with media outlets to discuss investment strategy, including his position on Apple. <p><b><a href="https://appleinsider.com/articles/18/08/30/warren-buffett-buys-just-a-little-more-apple-stock-warns-against-entering-auto-business" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>599</wp:post_id>
		<wp:post_date><![CDATA[2018-08-30 22:51:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-30 22:51:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[warren-buffett-buys-just-a-little-more-apple-stock-warns-against-entering-auto-business]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/08/30/warren-buffett-buys-just-a-little-more-apple-stock-warns-against-entering-auto-business]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[868]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple researching refinements to sunroofs, seating for &#039;Project Titan&#039; car</title>
		<link>https://fifthlevel.ai/archives/600</link>
		<pubDate>Tue, 28 Aug 2018 13:11:34 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/08/28/apple-researching-refinements-to-sunroofs-seating-for-project-titan-car</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/27329-40558-27156-39988-16526-13438-160414-Apple_Car-l-l-l.jpg" alt="Article Image" border="0" /> <br><br> While Apple's self-driving vehicle fleet is the most prominent element of its &quot;Project Titan&quot; research, the company is still working on other areas of the car, including improving the safety of passengers in a vehicle and a sunroof of its own design. <p><b><a href="https://appleinsider.com/articles/18/08/28/apple-researching-refinements-to-sunroofs-seating-for-project-titan-car" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>600</wp:post_id>
		<wp:post_date><![CDATA[2018-08-28 13:11:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-28 13:11:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-researching-refinements-to-sunroofs-seating-for-project-titan-car]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/08/28/apple-researching-refinements-to-sunroofs-seating-for-project-titan-car]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[867]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Jaguar Land Rover&#039;s Big-Eyed Self-Driving Pod Seeks Our Trust</title>
		<link>https://fifthlevel.ai/archives/629</link>
		<pubDate>Tue, 28 Aug 2018 11:19:24 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/263884/jlr-autonomous-pod-trust-humans/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[Pod will mimic eye contact with pedestrians waiting to cross the road. <p><b><a href="https://www.motor1.com/news/263884/jlr-autonomous-pod-trust-humans/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>629</wp:post_id>
		<wp:post_date><![CDATA[2018-08-28 11:19:24]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-28 11:19:24]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[jaguar-land-rovers-big-eyed-self-driving-pod-seeks-our-trust]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/263884/jlr-autonomous-pod-trust-humans/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Franken-algorithms: the deadly consequences of unpredictable code</title>
		<link>https://fifthlevel.ai/archives/1295</link>
		<pubDate>Thu, 30 Aug 2018 05:00:48 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.theguardian.com/technology/2018/aug/29/coding-algorithms-frankenalgos-program-danger</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The death of a woman hit by a self-driving car highlights an unfolding technological crisis, as code piled on code creates ‘a universe no one fully understands’</p><p>The 18th of March 2018, was the day tech insiders had been dreading. That night, a new moon added almost no light to a poorly lit four-lane road in Tempe, Arizona, as a specially adapted Uber Volvo XC90 detected an object ahead. Part of the modern gold rush to develop self-driving vehicles, the SUV had been driving autonomously, with no input from its human backup driver, for 19 minutes. An array of radar and light-emitting lidar sensors allowed onboard algorithms to calculate that, given their host vehicle’s steady speed of 43mph, the object was six seconds away – assuming it remained stationary. But objects in roads seldom remain stationary, so more algorithms crawled a database of recognizable mechanical and biological entities, searching for a fit from which this one’s likely behavior could be inferred.</p><p>At first the computer drew a blank; seconds later, it decided it was dealing with another car, expecting it to drive away and require no special action. Only at the last second was a clear identification found – a woman with a bike, shopping bags hanging confusingly from handlebars, doubtless assuming the Volvo would route around her as any ordinary vehicle would. Barred from taking evasive action on its own, the computer abruptly handed control back to its human master, but the master wasn’t paying attention. <a href="https://www.theguardian.com/technology/2018/may/08/ubers-self-driving-car-saw-the-pedestrian-but-didnt-swerve-report">Elaine Herzberg</a>, aged 49, <a href="https://www.theguardian.com/technology/2018/mar/22/self-driving-car-uber-death-woman-failure-fatal-crash-arizona">was struck and killed</a>, leaving more reflective members of the tech community with two uncomfortable questions: was this algorithmic tragedy inevitable? And how used to such incidents would we, <em>should</em> we, be prepared to get?</p><p> <span>Related: </span><a href="https://www.theguardian.com/books/2017/jul/05/weapons-math-destruction-big-data-cathy-o-neil">Weapons of Math Destruction by Cathy O’Neil review – trouble with algorithms</a> </p><p> <span>Related: </span><a href="https://www.theguardian.com/business/2014/jun/07/inside-murky-world-high-frequency-trading">Fast money: the battle against the high frequency traders</a> </p><p>Facebook would claim they know what’s going on at the micro level … But what happens at the level of the population​?</p><p> <span>Related: </span><a href="https://www.theguardian.com/commentisfree/2018/aug/05/magical-thinking-about-machine-learning-will-not-bring-artificial-intelligence-any-closer">Magical thinking about machine learning won’t bring the reality of AI any closer | John Naughton</a> </p><p>You have all these pieces of code running on people’s iPhones, and collectively it acts like one multicellular organism</p><p> <span>Related: </span><a href="https://www.theguardian.com/technology/2017/nov/18/facebook-youtube-revenge-porn-science-and-tech-feature-sara-wachter-boettcher">How algorithms are pushing the tech giants into the danger zone</a> </p><p> <span>Related: </span><a href="https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth">'Fiction is outperforming reality': how YouTube's algorithm distorts truth</a> </p><p> <span>Related: </span><a href="https://www.theguardian.com/books/2018/jun/30/new-dark-age-by-james-bridle-review-technology-and-the-end-of-the-future">New Dark Age by James Bridle review – technology and the end of the future</a> </p> <a href="https://www.theguardian.com/technology/2018/aug/29/coding-algorithms-frankenalgos-program-danger">Continue reading...</a> <p><b><a href="https://www.theguardian.com/technology/2018/aug/29/coding-algorithms-frankenalgos-program-danger" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1295</wp:post_id>
		<wp:post_date><![CDATA[2018-08-30 05:00:48]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-30 05:00:48]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[franken-algorithms-the-deadly-consequences-of-unpredictable-code]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/self-driving-cars/rss]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/2018/aug/29/coding-algorithms-frankenalgos-program-danger]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Why Ridesharing Companies May Be More Profitable Than Airlines, Or Maybe Not</title>
		<link>https://fifthlevel.ai/archives/2464</link>
		<pubDate>Thu, 06 Sep 2018 16:57:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b9147fc64aaf93e2ca383e0</guid>
		<description></description>
		<content:encoded><![CDATA[Ridesharing companies may come to resemble airlines, but their lower cost structures are poised to make them more successful. <p><b><a href="https://www.forbes.com/sites/davidsilver/2018/09/06/why-ridesharing-companies-may-be-more-profitable-than-airlines-and-maybe-not/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2464</wp:post_id>
		<wp:post_date><![CDATA[2018-09-06 16:57:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-06 16:57:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[why-ridesharing-companies-may-be-more-profitable-than-airlines-or-maybe-not]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/09/06/why-ridesharing-companies-may-be-more-profitable-than-airlines-and-maybe-not/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Self-Driving Cars Will Keep Getting Better Forever</title>
		<link>https://fifthlevel.ai/archives/2465</link>
		<pubDate>Tue, 04 Sep 2018 15:47:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b8dc21864aaf95d2721b143</guid>
		<description></description>
		<content:encoded><![CDATA[Self-driving cars have the potential to improve both speed and road capacity well beyond what human drivers can accomplish. <p><b><a href="https://www.forbes.com/sites/davidsilver/2018/09/04/self-driving-cars-will-keep-getting-better-forever/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2465</wp:post_id>
		<wp:post_date><![CDATA[2018-09-04 15:47:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-04 15:47:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[self-driving-cars-will-keep-getting-better-forever]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/09/04/self-driving-cars-will-keep-getting-better-forever/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Ford’s Self-Driving Vehicle Approach</title>
		<link>https://fifthlevel.ai/archives/2507</link>
		<pubDate>Thu, 06 Sep 2018 15:17:04 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/1a6aba9ea6f4</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/proxy/1*xpOoJVHsargk_Ru-E2QsWg.jpeg" /></figure><p>A few weeks ago, Ford Motor Company published <a href="https://media.ford.com/content/dam/fordmedia/pdf/Ford_AV_LLC_FINAL_HR_2.pdf">“A Matter of Trust”</a>, a report outlining their autonomous vehicle development process. The document seems pretty clearly intended for government regulators, and indeed they <a href="https://medium.com/self-driven/a-matter-of-trust-fords-approach-to-developing-self-driving-vehicles-12f602887822">sent the report to Transportation Secretary Elain Chao</a>.</p><p>Compared to the <a href="https://storage.googleapis.com/sdc-prod/v1/safety-report/waymo-safety-report-2017.pdf">Waymo Safety Report</a> released last year, the Ford document is a little more dense, but broadly similar. Both documents discuss how autonomous vehicles work, what sensors are involved, and what processes are in place to ensure safety.</p><p>Ford’s document includes more detail about training and procedures for safety operators, as you might expect in the wake of the <a href="https://www.usatoday.com/story/opinion/nation-now/2018/06/25/uber-self-driving-car-death-blame-human-driver-column/730754002/">Uber ATG collision in Arizona</a>.</p><p>Ford also share a bit more information about security. The report mentions efforts related to authentication, network segmentation, and physical partitioning.</p><p>All in all, it’s a good read to see how a larger autonomous vehicle organization operates.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1a6aba9ea6f4" width="1" height="1"><hr><p><a href="https://medium.com/self-driving-cars/fords-self-driving-vehicle-approach-1a6aba9ea6f4">Ford’s Self-Driving Vehicle Approach</a> was originally published in <a href="https://medium.com/self-driving-cars">Self-Driving Cars</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><b><a href="https://medium.com/self-driving-cars/fords-self-driving-vehicle-approach-1a6aba9ea6f4?source=rss----bdc6e635b3c0---4" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2507</wp:post_id>
		<wp:post_date><![CDATA[2018-09-06 15:17:04]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-06 15:17:04]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[fords-self-driving-vehicle-approach]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driving-cars]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driving-cars/fords-self-driving-vehicle-approach-1a6aba9ea6f4?source=rss----bdc6e635b3c0---4]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Throughput and Latency and Self-Driving Cars</title>
		<link>https://fifthlevel.ai/archives/2508</link>
		<pubDate>Tue, 04 Sep 2018 15:57:52 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/10ddffdc05a3</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ICvAO8mPCA_sXOzW9zeM7g.jpeg" /></figure><p>For the past few months <a href="https://www.forbes.com/sites/davidsilver/#28f7d9af208f">I have been writing about self-driving cars</a> for <a href="https://www.forbes.com/transportation/#fc91d1310b3c">Forbes.com</a>. <a href="https://www.forbes.com/sites/davidsilver/2018/09/04/self-driving-cars-will-keep-getting-better-forever/#6c5496e7217d">My latest post</a> is on the potential for self-driving cars to take throughput (road capacity) and latency (speed) well beyond the plateau at which human drivers stop improving.</p><blockquote>“Driving around Naples at 25mph for a year is a triumph, but imagine a self-driving car that can navigate Naples, safely, at 40mph? Or faster? Equipped with an array of sensors and super-computers, self-driving vehicles could react quicker and achieve safer speeds well beyond the ability of human drivers.</blockquote><blockquote>Similarly, US highway capacity tops out at about <a href="http://atrf.info/papers/2007/2007_Laufer.pdf">2,000 vehicles</a> <a href="http://www.mikeontraffic.com/numbers-every-traffic-engineer-should-know/">per hour</a>, assuming human drivers. Self-driving (and potentially connected) cars, however, may be able to dramatically decrease the distance between vehicles, and even move in unison with other cars. Think of a pack of bumper-to-bumper race cars traveling at 100mph. The improvement in road capacity would be tremendous.”</blockquote><p><a href="https://www.forbes.com/sites/davidsilver/2018/09/04/self-driving-cars-will-keep-getting-better-forever/#6c5496e7217d">Read the whole thing</a>, and also check out the Benedict Evans post on <a href="https://www.ben-evans.com/benedictevans/2018/8/29/tesla-software-and-disruption">“Tesla, software and disruption”</a> that sparked this line of thought.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=10ddffdc05a3" width="1" height="1"><hr><p><a href="https://medium.com/self-driving-cars/throughput-and-latency-and-self-driving-cars-10ddffdc05a3">Throughput and Latency and Self-Driving Cars</a> was originally published in <a href="https://medium.com/self-driving-cars">Self-Driving Cars</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><b><a href="https://medium.com/self-driving-cars/throughput-and-latency-and-self-driving-cars-10ddffdc05a3?source=rss----bdc6e635b3c0---4" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2508</wp:post_id>
		<wp:post_date><![CDATA[2018-09-04 15:57:52]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-04 15:57:52]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[throughput-and-latency-and-self-driving-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driving-cars]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driving-cars/throughput-and-latency-and-self-driving-cars-10ddffdc05a3?source=rss----bdc6e635b3c0---4]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>I Took A Tesla Model 3 For A Road Test</title>
		<link>https://fifthlevel.ai/archives/2509</link>
		<pubDate>Mon, 03 Sep 2018 10:55:34 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/6d378d511aae</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rUU3vLpTCg0Njpvww8k9yA.png" /><figcaption>Tesla Model 3 (LA Autoshow 2017)</figcaption></figure><p>There has been plenty of anticipation for the Tesla Model 3 since it was first announced as an EV (Electric Vehicle). The goal was to bring EV to the mainstream auto market with an eco-friendly and affordable sedan. Due to its sudden surge in demand, Tesla would face a “production hell” in the words of Elon Musk. Mass producing a car that does not have a tried, tested and true framework can be a difficult task as Tesla struggled to produce their promise per month quota, leading to production delays. Nonetheless, the Model 3 is something worth looking into. It is fully electric, zero emission and non-fossil fuel dependent for starters. That means savings from gas and at the same time decreasing the carbon footprint. The Model 3 is more on the budget side of things, below the pricier Model S luxury performance sedan and the Model X SUV.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fuFJoseIaZgUIj0cQfy6ZQ.png" /><figcaption>Tesla Model 3 Sedan (Source: TESLA)</figcaption></figure><p>It is a warm and unusually humid Los Angeles summer in late August when I was invited to take a Model 3 road test. Prior to that, I had test driven the Model S and done research on it in Las Vegas. I really cannot expect the experience to be the same on both cars since the Model S is more high performance luxury. I didn’t expect Tesla had a gallery at The Grove, where they occupy a corner in the Nordstrom store. It is where you go to road test the Model 3, which had not been available before. This was going to be different from Vegas, since the streets of LA are more tighter and the traffic is more bumper to bumper. We were not going to drive on any of the freeways, though I wished we were allowed to. For this road test I will be driving the streets between La Cienega Boulevard and La Brea Avenue, where there will be pedestrian crosswalks, plenty of traffic lights and narrow side streets. The Tesla rep walks me into the parking structure of The Grove where there are a bunch of Model 3 parked. After a brief orientation, the road test begins. I am going to drive a Model 3 Dual Motor AWD Performance version.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Lb8t_2N1NpJxMugxTYOozA.png" /><figcaption>Physical Dimensions (Source: Car And Driver Magazine)</figcaption></figure><p>Looking at the Model 3, it is just a normal car, the same way you would see a Toyota Corolla or Ford Focus. If I weren’t familiar with Tesla, I would not have thought of this as an EV. What makes it standout is its styling. It is not as lavish as the Model S or sporty like the Model X, but it doesn’t look like your ordinary sedan. It becomes apparent once you get closer and notice a few things. It is a keyless and wireless system, requiring no key fob. Tesla will provide a card that unlocks the car and starts it. This is the one thing that I usually forget, a Tesla automatically starts when it detects the wireless key card or fob. This thing is “NO keys” and “NO push to start”. Even better, you can download an app to your smartphone mobile device. This then installs an encrypted key on your device that must be running to be detected by the car to automatically start. As long as you have your app running on your smartphone, all you need to do is get close enough to the car and it starts up or powers itself on ready to be driven. The Model 3, like all EV, has no engine so ignition is not necessary which means the car starts up very quietly. In fact it doesn’t make any noise, which is unusual for long time motorists.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AQ25ZMpBkpexP1r2J90fmQ.png" /><figcaption>Model 3 Size Specifications (Source: Car And Driver Magazine)</figcaption></figure><p>I would compare the Model 3 size and class to be like the BMW 3 series and the Audi A4. It is compact, but it is actually quite spacious and roomy on the inside. From the outside the glass roof is noticeable, a defining feature on the Model 3. The body is made from rigid steel and aluminum, a process that required precision welding that encountered problems during the early part of the Model 3’s production. That is because most of Tesla’s car’s like the Model S and X were constructed from aluminum, a process that Tesla has managed to optimize for mass production. However producing the Model 3 at a larger scale and using steel required a different process. The process of welding steel requires the precision because poor welding can lead to car body problems as the vehicle ages.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1KEkk2n4l8khimiZZYOtWQ.png" /><figcaption>Model 3 Body Structure (Source: TESLA)</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yG-CvJQLsyy9_BgywgMAmA.jpeg" /><figcaption>From the garage of The Grove (Los Angeles, CA)</figcaption></figure><p>Getting into a Model 3 was confusing at first. The car doors don’t appear to have any handles. Then you realize that you have to push the actual handles for it to pop out. You then pull on the handle to open the door. How about making a push button rather than retractable handles? The design just adds an extra step to the whole process of opening the door. Car’s today already have their door handles exposed so it is a simpler and quicker process. The doors won’t open if you don’t have the app installed in your smartphone or mobile device or if you don’t have the key card.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*U11djE2ezMtZdEbNLCTRkQ.jpeg" /><figcaption>The door handle on the Model 3 requires pushing against it to pop out the handle.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*X3zXrS5uHNVMAUNHCCuwMg.png" /><figcaption>The Model 3 Touch Screen Console</figcaption></figure><p>Once inside you will notice this is not like any ordinary car interior. Their dashboard looks very clean, no instrumentation panel, odometer, speedometer or tachometer. Instead you get a spacious front panel, which are actually air conditioning vents with the steering wheel and at the center of the dashboard is a large touch screen. From this screen you control and monitor your Model 3. This is where you find the instrumentation, infotainment system, side mirrors, glove box and configuration settings you want to make to your car. This is also where you control the air conditioning for cooling or heating, access navigation, battery capacity status and all the other information displays related to the car. No more physical controls with your hands other than the steering wheel. It may seem tedious for those who don’t come from the smartphone touch screen generation. All your controls are handled from the screen and for beginners and even novices, this can be a bit of a learning curve. The interior was quite roomy, even though it looks small from the outside. There was plenty of leg room for both driver and even passengers, something I noticed can be lacking in similar compact sedans.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xgntrjc5GTdrLAXiAjIKvw.png" /><figcaption>Model 3 interior with the touch screen console (Source: TESLA)</figcaption></figure><p>The Model 3 also has a low center of gravity which is required because of the weight of the batteries that provide power. They are placed at the bottom of the car and contribute to much of the weight. It has to be balanced so as not to affect the car’s stability and aerodynamics. The car is powered by an array of batteries (Li-Ion) that provide the range that a Model 3 can drive. The standard Model 3 has a battery capacity of 50 kWh for a range of 220 miles while the long range battery has a capacity of 75 kWh range of 310 miles. These values are estimates since the load on the battery for everyone would likely be different, but these are the rated capacity for reference. An EV will be much like using a smartphone, it requires recharging and uses advanced electronics.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*_DvgzUU3IzEc0qcxzRcRMg.jpeg" /><figcaption>Front of the Tesla Model 3</figcaption></figure><p>Now I am ready to take it for a spin. I sit back on the nice leather seats and do not have to put a key in the ignition or push to start anything. It is not like you are ready to drive because there is no sound coming from an engine either, this is all electric. So just step on the acceleration pedal and it quietly moves forward and the thing is you don’t feel much effort. That is the advantage of electro-magnetic induction motors. It creates instant torque for forward motion. You can feel the acceleration going from 0–40 (street legal) like you were in a roller coaster because it does it much faster than a conventional internal combustion automobile. You can also feel how the tires grip the road during braking. It was explained to me that on the console screen you can adjust how you want the regenerative braking to be. The harder you brake, the more power you feed the battery from the kinetic energy from the car’s motion. The braking converts that energy into power that is stored back to the battery, unlike with conventional cars which actually inefficiently burns more gas when braking. The streets were also a good place to test the braking system, and it did not fail. I was not allowed to test any emergency braking feature because of the liability that can cause, but for the most part the car came to a complete halt at stop signs and slow the car down when there are crossing pedestrians.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RruVGgWsvlcCiRNjKoPD-g.png" /><figcaption>Model 3 Demo Using Enhanced Autopilot Features (Source: TESLA)</figcaption></figure><p>I briefly tested the Enhanced Autopilot feature which was similar to the Model S. Sure enough when you want to switch lanes in traffic, all you need to do is use the turn signal and the software in the Autopilot takes over. I don’t advise using this in crowded traffic areas, like main streets. It is more suitable for highways and freeways where there is more space. Let us not forget that Tesla may have the hardware ready for fully autonomous driving, but at the moment the Model 3 and other Tesla cars are not driverless. They are semi-autonomous only until the L5 fully autonomous feature is enabled (no official word on this as of writing). It is also wise to keep the hands on the steering wheel and engage when needed. The Autopilot can warn the driver when they need to take over. It has been controversial due to accidents that have occurred, but it still remains a point of discussion because it does not happen to all drivers. I have used the Enhanced Autopilot feature many times and have not had any unfortunate incidents. I use plenty of caution while doing so and never irresponsibly take my eyes off the road or assume that the Tesla will suddenly drive by itself. What I do find the Autopilot feature that is most useful right now would be for adaptive cruise control during long drives on the freeway. I have heard many testimonials of how it helps drivers relax more during road trips.</p><p>While waiting at traffic lights, the Model 3 has a feature that allows you to apply the brakes without your foot on the pedal. This is useful, especially for drivers who are on an incline or steep elevation. Instead of “hanging” where drivers balance gas and clutch pedal on stick shift systems or keeping the foot on the pedal for automatic systems, you can just press down on the Model 3 brake pedal once and it will hold the brake for you. You then step on the accelerate pedal to move forward again.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*t5KDRV7BVqMG86wu30FuBw.png" /><figcaption>Driving an EV on the streets of Los Angeles was not bad.</figcaption></figure><p>The Model 3 makes tight turns, left or right, allowing the driver to not have to turn the wheel too much. It has electric power assisted steering as well so the driver doesn’t have to apply too much effort when making turns. This is good for people with arthritic conditions or temporary injuries that affect the joints or elbow. Another thing about the steering wheel is that it contains scroll buttons that allow you to control settings for your car rather than using the touch screen console. That is one way to add some ergonomics to the design, but you can also use voice activation commands for more convenience while driving. This seems to me to be a work in progress as Tesla continues to define functions for the scroll buttons and how voice activation can be further improved. An actual driving assistant that works like Siri or Alexa is perhaps the goal.</p><p>There were early complaints on the Model 3 regarding its suspension as being too stiff and the overall ride quality being uncomfortable for some drivers. The reviews seem pretty mixed between those who are more into comfort and ride to those who are more into the technology and drive. I did find the suspension rather stiff at first, but that was due to the regenerative braking feature. Once this was adjusted I felt the suspension a little more loose for my driving, but at the expense of feeding less power back to the battery during braking. I did run over some bumps and this is where I notice the difference in comfort between the Model S and Model 3. No doubt the Model S has a much better shock absorbing system with smoother drive on bad road surfaces as well. The Model 3 was not meant to be a luxury sedan, so it felt much like driving a conventional car sedan. I did drive the long range performance version which also has dual motors, so it may be a little more comfortable to drive than the standard version.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*n4pEwFskf5U8fT_rbRe6_Q.png" /><figcaption>The touch screen console centralizes all functions and controls of the Model 3.</figcaption></figure><p>I learned later on that the Model 3 does not have a spare tire in the trunk or anywhere in the car. It may not be a big deal since many people don’t need to ever replace a flat tire themselves. However it may not go well with other drivers. The Tesla agents did explain that the Model 3 will notify the driver from the console to pull over when the tire or tires are losing lots of air pressure. Tesla will then provide road side assistance to help replace a flat tire. I think that is ok, but what if you’re in the middle of nowhere? I don’t have any satisfactory answer for that other than that it is Tesla’s commitment to get to car owners wherever they might get stuck.</p><p>Overall I liked the Model 3 version I drove. I do look forward to more autonomous self-driving features that make use of upcoming standards like V2X. I do not however like the door handles having to pop out and retract, no spare tire and putting all the instrumentation and controls on the console. I do prefer to at least see the odometer and speedometer center front of the steering wheel. I know that there are driverless cars being designed that remove the steering wheel entirely and everything is operated from a console screen like the Model 3. The Hop On driverless shuttle in Las Vegas is an example of this. It will seem really weird at first that all the controls are done on the console touch screen, but usually after using it many times people get accustomed to these features. For those planning to get a Model 3 it is worth looking into even if you don’t end up buying one. If not for the advanced features, it must be for the economy in savings on fuel and it is also more eco-friendly to drive.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*_Wy14xbQbGWvfq_rziKvoA.jpeg" /><figcaption>Eco-friendly, zero emission electric vehicle</figcaption></figure><p><em>Note: The article is meant for reference and entertainment only, and is the author’s personal opinion on the subject. In no way is this a paid advertising or promotion.</em></p><p><strong>Further Reading:</strong></p><ul><li><a href="https://www.forbes.com/sites/brookecrothers/2018/09/02/tesla-model-3-owner-review-quality-issues-consumer-reports-customer-satisfaction/#122b983c37a4">Passion For Tesla Model 3 Tops Quality Concerns</a></li><li><a href="https://electrek.co/2018/09/02/tesla-misses-model-3-production-goal-overall-q3-goal/">Tesla misses Model 3 production goal of 6,000 units per week, but on track for overall Q3 goal</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6d378d511aae" width="1" height="1"><hr><p><a href="https://medium.com/self-driving-cars/i-took-a-tesla-model-3-for-a-road-test-6d378d511aae">I Took A Tesla Model 3 For A Road Test</a> was originally published in <a href="https://medium.com/self-driving-cars">Self-Driving Cars</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><b><a href="https://medium.com/self-driving-cars/i-took-a-tesla-model-3-for-a-road-test-6d378d511aae?source=rss----bdc6e635b3c0---4" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2509</wp:post_id>
		<wp:post_date><![CDATA[2018-09-03 10:55:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-03 10:55:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[i-took-a-tesla-model-3-for-a-road-test]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driving-cars]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driving-cars/i-took-a-tesla-model-3-for-a-road-test-6d378d511aae?source=rss----bdc6e635b3c0---4]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>It’s Not Just an Autonomous Vehicle Revolution—It’s a Connected Vehicle Revolution</title>
		<link>https://fifthlevel.ai/archives/2512</link>
		<pubDate>Thu, 06 Sep 2018 15:41:32 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://veniam.com/?p=1940</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The race is well underway on for both automakers and tech companies to produce the first generation of autonomous vehicles, with optimistic minds expecting the first commercial models to hit the road by 2020.</p>
<p>But  a deeper look reveals that autonomous vehicle technology isn’t confined simply to passenger cars—and the technology is about so much more than being able to take our hands off the steering wheel.</p>
<p>We are entering a new era of transportation, in which connected vehicles of all kinds will share the road in the form of city buses, airport shuttles, subways, public utility and maintenance vehicles, even long-distance freight trucks on interstate highways.</p>
<p>The lifeblood of this transportation revolution is data—generated and consumed by all those vehicles, and connected and shared to a constellation of outside parties and companies. Automakers, mobility operators and cities, in turn, will need to convert that data into economic opportunities.</p>
<p>The first generation of driverless vehicles, for instance, will throw off more than 4,000 GBs of data per day from internal sensors like GPS, Lidar, cameras, radar and other on-board technology. City managers will eventually be able to use that data for a range of urban applications, such as reducing traffic congestion, improving accident or emergency response, monitoring air pollution, and managing fleets of city AVs to optimize public services.</p>
<p>It shouldn’t be hard to imagine, because a number of cities across the globe have already taken the first steps.</p>
<p>In Porto, Portugal, more than 500 buses, taxis, and city service vehicles were outfitted to create the largest mesh network of connected vehicles in the world, supporting 300,000 riders. Buses offered free wi-fi services to riders, while garbage trucks connected with trash-can sensors to optimize pickup routes.</p>
<p>In Amsterdam, researchers from MIT and the Amsterdam Institute for Advanced Metropolitan Solutions recently began deploying a fleet of autonomous watercraft for the city’s canals—data from which will be used to track water quality, pollution, and predictive applications for public health. The boats are ingenious, but it’s the connected data that truly powers their potential.</p>
<p>In Canada, Alphabet’s Sidewalk Labs last year struck a deal with the city of Toronto to develop 800 acres of waterfront property into a kind of urban-digital utopia—a place to test a network of self-driving cars, public Wi-Fi, new health care delivery solutions, “and other city planning advances that modern technology makes possible,” according to a Verge report.</p>
<p>The common theme here is the emerging potential of connected vehicles. But as people in these and other cities will soon discover, it promises a lot more than getting from point A to point B. It may be the catalyst in reinventing modern cities altogether.</p>
<p><a href="https://www.linkedin.com/pulse/its-just-autonomous-vehicle-revolutionits-connected-jo%C3%A3o-barros/?published=t" target="_blank" rel="noopener">LinkedIn Version</a></p> <p><b><a href="https://veniam.com/its-not-just-an-autonomous-vehicle-revolution-its-a-connected-vehicle-revolution/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2512</wp:post_id>
		<wp:post_date><![CDATA[2018-09-06 15:41:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-06 15:41:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[its-not-just-an-autonomous-vehicle-revolution-its-a-connected-vehicle-revolution]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://veniam.com/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://veniam.com/its-not-just-an-autonomous-vehicle-revolution-its-a-connected-vehicle-revolution/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>What Do Alphabet, Baidu And Yandex Have In Common? Search Engines And Self-Driving Cars</title>
		<link>https://fifthlevel.ai/archives/3202</link>
		<pubDate>Fri, 31 Aug 2018 21:49:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b89acc964aaf95d2721ac01</guid>
		<description></description>
		<content:encoded><![CDATA[Alphabet, Baidu and Yandex all have search engines and self-driving cars. Time will tell if the autonomous vehicle industry matures to resemble the search engine industry or the automotive manufacturing industry. <p><b><a href="https://www.forbes.com/sites/davidsilver/2018/08/31/alphabet-baidu-and-yandex-all-have-search-engines-and-self-driving-cars/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3202</wp:post_id>
		<wp:post_date><![CDATA[2018-08-31 21:49:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-31 21:49:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[what-do-alphabet-baidu-and-yandex-have-in-common-search-engines-and-self-driving-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/08/31/alphabet-baidu-and-yandex-all-have-search-engines-and-self-driving-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>comma.ai Affiliate Program</title>
		<link>https://fifthlevel.ai/archives/3251</link>
		<pubDate>Fri, 31 Aug 2018 02:22:27 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/b21ef6bde838</guid>
		<description></description>
		<content:encoded><![CDATA[<p>omg it’s sales and marketing</p><h3>The problem with marketing</h3><p>The e-mails come. They promise you seven times industry metrics over average responses in your sector. They claim eyeballs in coveted demographics, valuable age ranges, and targeting beating AdWords, the Disney Channel, and Microsoft Azure. Combined for hyperlocal growth hacking. Business messaging on par with your brand identity, big data internet of things based tracking using IOTA, and high signal actionable analytics.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/483/1*YzgnPFZX0c9eZFGMJmSTHQ.png" /><figcaption>This passive language really grinds my gears</figcaption></figure><p>And they understand you are a busy person, but they just want to get on your calendar for a quick intro call. I’d rather go to the dentist.</p><h3>This is garbage</h3><p>These people haven’t spent 10 minutes to understand our brand. I don’t think they could if they tried. They have sent the same spam to everybody. They track metrics that can’t begin to measure quality. And they expect me to pay them upfront and trust that they can sell our products.</p><p>There has to be a better way.</p><h3>Rewarding the right people</h3><p>Since the start of the comma.ai community, we’ve seen incredible value added by our users. The affiliate program is a way for users to earn money from that value creation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/737/1*UAQpZ7yeIf3SMk3gy8mK6g.png" /><figcaption>VirtuallyChris has made videos showcasing openpilot</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*X3x0AZ3aFcRmhzYe_YaTQg.png" /><figcaption>Joshua has made medium posts helping users get started</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/736/1*OteBGprDjPLkOnsY6ATA_A.png" /><figcaption>And openpilot itself has gotten tons of community contributions</figcaption></figure><h3>How the program works</h3><p>As far as affiliate programs go, we want to be generous. We want to encourage the comma.ai ecosystem.</p><ul><li>Have people click your custom affiliate link and make orders.</li><li><strong>15% of order subtotal is paid out to you. </strong>If someone clicks your link and buys an EON, panda, and Giraffe, you get $143.70. That’s over $100!</li><li>Cookie based tracking, so if the user purchases within 3 days of clicking your link (assuming they didn’t click someone else’s!), you get credit for the sale.</li><li>Payout by Bank Transfer, PayPal, or Bitcoin. Minimum $500 before payout.</li></ul><p>This is an experiment. We’ll keep it running through September and October, and reevaluate its success at that point.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/660/1*hidm2xh1N9a1Ls3DctCAIQ.png" /></figure><p>It’s the community that gives comma a lot of its value. This is one way we can promote growth in an incentives aligned way. Friends bring in friends!</p><h3>How to get started</h3><p>E-mail affiliate at our website with your slack username and a request to join. If accepted, we’ll send you a link. Currently we are only accepting requests from people who have been inside our community for a bit. No link spammers! And no mobbing the new slack users!</p><h3>And always (for everyone)</h3><ol><li><a href="http://slack.comma.ai">Join our slack</a></li><li>F<a href="https://twitter.com/comma_ai">ollow us on Twitter</a></li><li>And of course, <a href="https://comma.ai/shop/?ref=2">buy things through my affiliate link</a>!</li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b21ef6bde838" width="1" height="1"> <p><b><a href="https://medium.com/@comma_ai/comma-ai-affiliate-program-b21ef6bde838?source=rss-330bac69b283------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3251</wp:post_id>
		<wp:post_date><![CDATA[2018-08-31 02:22:27]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-31 02:22:27]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[comma-ai-affiliate-program]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@comma_ai/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@comma_ai/comma-ai-affiliate-program-b21ef6bde838?source=rss-330bac69b283------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Autonomous Autobahn-anza: Array of Driverless Vehicles Coming to GTC Europe</title>
		<link>https://fifthlevel.ai/archives/15</link>
		<pubDate>Fri, 28 Sep 2018 03:31:57 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=40697</guid>
		<description></description>
		<content:encoded><![CDATA[As the birthplace of the automobile, Germany — the location of next month’s <a href="https://www.nvidia.com/en-eu/gtc/">GTC Europe</a> — is the ideal backdrop to map out the future of mobility and transportation.

Global automotive and tech leaders will gather at the event in Munich on Oct. 9-11 to share insights, engage in AI training and show off innovations in autonomous driving. The latest autonomous driving concepts will also be on display, with vehicle exhibitions and demos.

The show’s more than 3,000 attendees will get a sneak peek from dozens of speakers, demos and hands-on classes into how autonomous driving technology will move from R&amp;D to the Autobahn.
<h2><b>Hear from Experts on Autonomy’s Front Line</b></h2>
From design to business strategy to engineering, GTC Europe speakers will give updates on their AV technology development and how they’re preparing for deployment. A few session highlights:
<ul>
 	<li>Michael E. Hafner, head of automated driving at Daimler, will explore safety aspects in the development of autonomous vehicles.</li>
</ul>
<ul>
 	<li>Ivo Muth, vice president of user experience at Audi, will describe how advances in in-vehicle AI are influencing the development of automotive interiors.</li>
</ul>
<ul>
 	<li>Christian Schumacher, vice president of program management systems at Continental, will provide an overview of the supplier’s autonomous driving strategy, as well as plans for using simulation for platform validation.</li>
</ul>
<ul>
 	<li>Stan Boland, CEO of Five AI, will detail how the robotaxi startup is developing its self-driving driving platform and its plans to deploy a shared autonomous vehicle fleet.</li>
 	<li>Gareth Rogers, head of design at BMW Group, will discuss how the automaker is using immersive VR in the design process for its next generation of vehicles.</li>
</ul>
<h2><b>See the Future on the Show Floor</b></h2>
With renowned automotive exhibitors showing production vehicles, prototypes and technology demos, all it takes is a walk across the GTC Europe show floor to see the future of transportation.
<figure id="attachment_40701" class="wp-caption alignright" style="width: 400px;"><a href="https://blogs.nvidia.com/wp-content/uploads/2018/09/IMG_8949.jpg"><img class="size-medium wp-image-40701" src="https://blogs.nvidia.com/wp-content/uploads/2018/09/IMG_8949-400x267.jpg" sizes="(max-width: 400px) 100vw, 400px" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/09/IMG_8949-400x267.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2018/09/IMG_8949-768x512.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2018/09/IMG_8949-672x448.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2018/09/IMG_8949-675x450.jpg 675w, https://blogs.nvidia.com/wp-content/uploads/2018/09/IMG_8949-323x215.jpg 323w, https://blogs.nvidia.com/wp-content/uploads/2018/09/IMG_8949-150x100.jpg 150w, https://blogs.nvidia.com/wp-content/uploads/2018/09/IMG_8949-1280x853.jpg 1280w" alt="" width="400" height="267" /></a><figcaption class="wp-caption-text">Vehicles from global automakers and startups lined the floor at last year’s GTC Europe.</figcaption></figure>
From high-tech luxury from automakers such as <b>Porsche</b>, <b>Mercedes-Benz</b> and <b>Audi</b> to innovators for new mobility solutions, including <b>share2drive </b>and <b>Einride</b>, autonomous vehicles of all shapes and sizes will be represented.

Driverless vehicles with a need for speed can also be found at the event — autonomous racing company <b>Roborace</b> will be exhibiting its <a href="https://blogs.nvidia.com/blog/2018/07/18/robocar-climbs-to-new-heights-at-goodwood-festival-of-speed/">Robocar racecar</a> and DevBot racing development vehicle. Academic and research partners such as <b>Fraunhofer</b> and <b>Virtual Vehicle</b> will showcase their autonomous vehicles in live demos.
<h2><b>Learn from the Best</b></h2>
Take advantage of the AI resources at GTC Europe to learn more about deep learning for autonomous driving. NVIDIA will host a <a href="https://www.nvidia.com/en-us/deep-learning-ai/education/">Deep Learning Institute</a> lab at the conference focused on how to design, train and deploy deep neural networks for self-driving cars, with a focus on vehicle perception.

<a href="https://www.nvidia.com/en-eu/gtc/attend/why-attend/">Register for GTC Europe</a> and check out the upcoming stops on the <a href="https://www.nvidia.com/en-us/gtc/">2018 GTC world tour</a> in Israel and Washington, D.C.

The post <a href="https://blogs.nvidia.com/blog/2018/09/26/autonomous-vehicles-gtc-europe/" rel="nofollow">Autonomous Autobahn-anza: Array of Driverless Vehicles Coming to GTC Europe</a> appeared first on <a href="https://blogs.nvidia.com" rel="nofollow">The Official NVIDIA Blog</a>.
<div class="feedflare"><a href="http://feeds.feedburner.com/~ff/nvidiablog?a=M9nwAFDJTwA:cZm3_21hKBQ:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/nvidiablog?d=yIl2AUoC8zA" border="0" /></a> <a href="http://feeds.feedburner.com/~ff/nvidiablog?a=M9nwAFDJTwA:cZm3_21hKBQ:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/nvidiablog?i=M9nwAFDJTwA:cZm3_21hKBQ:V_sGLiPBpWU" border="0" /></a></div>
<img src="http://feeds.feedburner.com/~r/nvidiablog/~4/M9nwAFDJTwA" alt="" width="1" height="1" />]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>15</wp:post_id>
		<wp:post_date><![CDATA[2018-09-28 03:31:57]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-28 03:31:57]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[autonomous-autobahn-anza-array-of-driverless-vehicles-coming-to-gtc-europe-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feeds.feedburner.com/nvidiablog]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feedproxy.google.com/~r/nvidiablog/~3/M9nwAFDJTwA/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-09-26]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[898]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Leading the Charge to an Electric Future: Audi Unveils E-Tron SUV</title>
		<link>https://fifthlevel.ai/archives/35</link>
		<pubDate>Fri, 28 Sep 2018 03:34:52 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=40646</guid>
		<description></description>
		<content:encoded><![CDATA[Some companies ride the current of innovation. Others lead the charge into new technologies.

Amid a crowd of 1,600 reporters and guests in the San Francisco Bay Area, NVIDIA partner Audi unveiled its latest trailblazing vehicle: the e-tron SUV, built to accelerate the age of electric powertrains.

The automaker’s first all-electric vehicle has a stunning cockpit that’s the byproduct of a deep partnership between <a href="https://www.nvidia.com/en-us/self-driving-cars/partners/audi/">Audi and NVIDIA</a>, beginning more than a decade ago with infotainment, extending to piloted driving, and now the next generation of luxury vehicles.

“With the e-tron, we are merging the new world of electromobility with more than 100 years of manufacturing premium cars,” said Audi CEO Abraham Schot at the vehicle launch Monday night.

Audi first teased the e-tron electrified product line as a concept in 2009, building on its dedication to sustainability with a variety of electric vehicle model ideas through the years. As the company’s first all-electric production car, the e-tron marks the beginning of Audi’s next phase as an industry leader.
<h2><b>A ‘Smart Space’ for Everyday Traffic</b></h2>
From fast charging to luminous displays, the e-tron makes daily commutes an effortless experience.

With a 95 kWh battery, the electric SUV has a range of nearly 250 miles on a single charge, based on European lab tests. It can also access Electrify America high-speed charging stations, reaching 80 percent charge in just a half hour. This long-range capability means the e-tron can go anywhere, anytime. No range anxiety necessary.

For navigation, entertainment and cabin controls, the e-tron leverages the newest Audi MMI infotainment system, powered by NVIDIA, bringing comfort and convenience to the driver’s fingertips. The broad screen displaying crystal-clear graphics ensures information is easily accessible at all times.
<figure id="attachment_40652" class="wp-caption alignright" style="width: 400px;"><a href="https://blogs.nvidia.com/wp-content/uploads/2018/09/Screen-Shot-2018-09-17-at-11.29.14-PM.png"><img class="size-medium wp-image-40652" src="https://blogs.nvidia.com/wp-content/uploads/2018/09/Screen-Shot-2018-09-17-at-11.29.14-PM-400x301.png" sizes="(max-width: 400px) 100vw, 400px" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/09/Screen-Shot-2018-09-17-at-11.29.14-PM-400x301.png 400w, https://blogs.nvidia.com/wp-content/uploads/2018/09/Screen-Shot-2018-09-17-at-11.29.14-PM-768x577.png 768w, https://blogs.nvidia.com/wp-content/uploads/2018/09/Screen-Shot-2018-09-17-at-11.29.14-PM-665x500.png 665w, https://blogs.nvidia.com/wp-content/uploads/2018/09/Screen-Shot-2018-09-17-at-11.29.14-PM-599x450.png 599w, https://blogs.nvidia.com/wp-content/uploads/2018/09/Screen-Shot-2018-09-17-at-11.29.14-PM-286x215.png 286w, https://blogs.nvidia.com/wp-content/uploads/2018/09/Screen-Shot-2018-09-17-at-11.29.14-PM-133x100.png 133w, https://blogs.nvidia.com/wp-content/uploads/2018/09/Screen-Shot-2018-09-17-at-11.29.14-PM-1280x962.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2018/09/Screen-Shot-2018-09-17-at-11.29.14-PM.png 1482w" alt="" width="400" height="301" /></a><figcaption class="wp-caption-text">The Audi MMI infotainment system, powered by NVIDIA, brings comfort and convenience to the driver’s fingertips.</figcaption></figure>
“The layout is driver-oriented, it’s an orchestration of shape, of materials, of light and sound,” said Audi chief designer Marc Lichte. “It’s a smart space.”
<h2><b>A Superior Interior</b></h2>
Without a space-consuming engine, the e-tron boasts a roomy interior that seats five adults comfortably. It brings in-car technology to the next level with features like virtual side-view mirror displays and an integrated toll module, freeing drivers from hanging electronic toll tags. Adding another layer of innovation to the luxurious space is a high-tech digital cluster.

The e-tron’s sleek dashboard builds on Audi’s floating virtual cockpit design featured in its A8 and A7 sedans. With NVIDIA technology at its heart, the system displays driving and entertainment features with silky smooth graphics.

A product of industry-leading technology and an innovative partnership with NVIDIA, the Audi e-tron amps up the drive for electric vehicle development.

The post <a href="https://blogs.nvidia.com/blog/2018/09/18/audi-unveils-e-tron-electric-suv/" rel="nofollow">Leading the Charge to an Electric Future: Audi Unveils E-Tron SUV</a> appeared first on <a href="https://blogs.nvidia.com" rel="nofollow">The Official NVIDIA Blog</a>.
<div class="feedflare"><a href="http://feeds.feedburner.com/~ff/nvidiablog?a=aQjKiBj4W2g:iTK43626Hnc:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/nvidiablog?d=yIl2AUoC8zA" border="0" /></a> <a href="http://feeds.feedburner.com/~ff/nvidiablog?a=aQjKiBj4W2g:iTK43626Hnc:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/nvidiablog?i=aQjKiBj4W2g:iTK43626Hnc:V_sGLiPBpWU" border="0" /></a></div>
<img src="http://feeds.feedburner.com/~r/nvidiablog/~4/aQjKiBj4W2g" alt="" width="1" height="1" />]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>35</wp:post_id>
		<wp:post_date><![CDATA[2018-09-28 03:34:52]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-28 03:34:52]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[leading-the-charge-to-an-electric-future-audi-unveils-e-tron-suv]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="audi"><![CDATA[Audi]]></category>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feeds.feedburner.com/nvidiablog]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feedproxy.google.com/~r/nvidiablog/~3/aQjKiBj4W2g/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-09-18]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[899]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Tactile Mobility gives autonomous cars AI to measure road conditions</title>
		<link>https://fifthlevel.ai/archives/40</link>
		<pubDate>Wed, 26 Sep 2018 13:30:32 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2398313</guid>
		<description></description>
		<content:encoded><![CDATA[<img class="attachment-single-feed size-single-feed wp-post-image" src="https://venturebeat.com/wp-content/uploads/2017/11/avs-and-society-e1511929443101.jpeg?fit=578%2C325&amp;strip=all" alt="" width="578" height="325" />

<hr />

Most autonomous cars sport a bevy of sensors that help them navigate patchy pavement, but some firms contend these aren’t precise enough to handle faded markers, buckling pavement, potholes, and other road-level hazards on their own. That poses a real problem in countries like the U.S., where the annual investment required to maintain roads,…<a href="https://venturebeat.com/2018/09/26/tactile-mobility-gives-autonomous-cars-ai-to-measure-road-conditions/" target="_blank" rel="noopener">Read More</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>40</wp:post_id>
		<wp:post_date><![CDATA[2018-09-26 13:30:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-26 13:30:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[tactile-mobility-gives-autonomous-cars-ai-to-measure-road-conditions]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/09/26/tactile-mobility-gives-autonomous-cars-ai-to-measure-road-conditions/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[172]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Arm unveils 7nm Cortex-A76AE, the ‘world’s first autonomous-class’ car processor</title>
		<link>https://fifthlevel.ai/archives/41</link>
		<pubDate>Fri, 28 Sep 2018 00:00:16 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2398159</guid>
		<description></description>
		<content:encoded><![CDATA[<img class="attachment-single-feed size-single-feed wp-post-image" src="https://venturebeat.com/wp-content/uploads/2018/09/6759d148-bc8a-453c-b57d-663245e0822e.png?fit=578%2C382&amp;strip=all" alt="ARM Cortex-A76AE" width="578" height="382" />

<hr />

Arm Holdings, the Softbank-owned British semiconductor and software company, is perhaps best known for its role in designing billions of smartphone, tablet, and smartwatch chips sold worldwide. System-on-chips built on Arm architectures have an estimated 90-95 percent share of the smartphone processing market, and Mali, Arm’s graphics process…<a href="https://venturebeat.com/2018/09/26/arm-unveils-7nm-cortex-a76ae-the-worlds-first-autonomous-class-car-processor/" target="_blank" rel="noopener">Read More</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>41</wp:post_id>
		<wp:post_date><![CDATA[2018-09-28 00:00:16]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-28 00:00:16]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[arm-unveils-7nm-cortex-a76ae-the-worlds-first-autonomous-class-car-processor]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/09/26/arm-unveils-7nm-cortex-a76ae-the-worlds-first-autonomous-class-car-processor/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-09-26]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[170]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Self-Driving Cars Will Keep Getting Better Forever</title>
		<link>https://fifthlevel.ai/archives/230</link>
		<pubDate>Sun, 30 Sep 2018 13:20:25 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b8dc21864aaf95d2721b143</guid>
		<description></description>
		<content:encoded><![CDATA[Self-driving cars have the potential to improve both speed and road capacity well beyond what human drivers can accomplish. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>230</wp:post_id>
		<wp:post_date><![CDATA[2018-09-30 13:20:25]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-30 13:20:25]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[self-driving-cars-will-keep-getting-better-forever-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/09/04/self-driving-cars-will-keep-getting-better-forever/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-09-04]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>What Do Alphabet, Baidu And Yandex Have In Common? Search Engines And Self-Driving Cars</title>
		<link>https://fifthlevel.ai/archives/231</link>
		<pubDate>Sun, 30 Sep 2018 13:20:28 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b89acc964aaf95d2721ac01</guid>
		<description></description>
		<content:encoded><![CDATA[Alphabet, Baidu and Yandex all have search engines and self-driving cars. Time will tell if the autonomous vehicle industry matures to resemble the search engine industry or the automotive manufacturing industry. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>231</wp:post_id>
		<wp:post_date><![CDATA[2018-09-30 13:20:28]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-30 13:20:28]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[what-do-alphabet-baidu-and-yandex-have-in-common-search-engines-and-self-driving-cars-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/08/31/alphabet-baidu-and-yandex-all-have-search-engines-and-self-driving-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-08-31]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>An Apple Car Would Require An Automotive Foxconn</title>
		<link>https://fifthlevel.ai/archives/233</link>
		<pubDate>Sun, 30 Sep 2018 13:20:37 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b73baf364aaf942a5e2f99c</guid>
		<description></description>
		<content:encoded><![CDATA[Apple products are typically designed in California and assembled in China. But it's not clear which companies would even be capable of assembling an Apple self-driving car. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>233</wp:post_id>
		<wp:post_date><![CDATA[2018-09-30 13:20:37]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-30 13:20:37]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[an-apple-car-would-require-an-automotive-foxconn-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/08/16/building-an-apple-car-would-require-an-automotive-foxconn/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-08-16]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>One man’s flying car dream is taking off, thanks to MIT</title>
		<link>https://fifthlevel.ai/archives/538</link>
		<pubDate>Wed, 26 Sep 2018 21:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/mit-professional-education-student-felipe-varon-flying-car-dream-taking-off-0926</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Most children dream about fabulous flying machines. For electrical engineer Felipe Varon, it was a flying car. Now, a prototype he's developed is making test flights in his native Colombia, thanks to his experience with MIT Professional Education.</p> <p>“As a child, I dreamed about flying,” says Varon, a graduate of MIT Professional Education’s&nbsp;<a href="https://professional.mit.edu/programs/short-programs/professional-certificate-program-innovation-and-technology">Professional Certificate Program in Innovation and Technology</a>. “But I don’t want just a cool toy. I want something with social impact to help people and cities. Something people can use today, not in some future time.”</p> <p>Varon says MIT Professional Education (PE) provided the knowledge, training, and ideas he needed to upscale his invention in size, power and capability, and for strategies to finance, market, and mass produce it. In 2018, he completed courses including Beyond Smart Cities and Radical Innovation, Mastering Innovation and&nbsp;Design-Thinking, and Precision Engineering Principles for Mechanical Design.</p> <p>MIT PE Executive Director Bhaskar Pant says entrepreneurs and innovators like Varon&nbsp;“are at the heart of our student population.”</p> <p>“He is a great example of how people use knowledge gained from our certificate programs to drive innovation and leadership towards meaningful change,” Pant says.</p> <p>A flying car was the subject of Varon’s 2006 graduate thesis at the Universidad Externo de Colombia.</p> <p>“I put together this machine,” he says. “I knew a motor and propellers could make it fly, kind of like a table with four legs.”</p> <p>Varon could be describing a drone, and the skies were already full of them. But he&nbsp;took drone technology to the next level. The company he founded with two partners, <a href="http://varonvehicles.com/" target="_blank">Varon Vehicles Corporation</a>, built a prototype flying car designed to&nbsp;travel in its own lane, at low altitudes, safely clear of both land-bound and aeronautic traffic.</p> <p>The car looks like&nbsp;a shiny red, two-seated blend of a Batmobile and Agent 007’s Aston Martin. The vehicle is entirely electric, with neither wheels nor wings, and Varon’s company logo — a multi-layered “V” —&nbsp;on the hood.</p> <p>“It’s very simple,” he says. “It doesn’t have any dials, buttons or strange pilot stuff. It steers just like a car. We’re trying to make it drivable by anybody. A computer does all the work.”</p> <p>The design of the car&nbsp;has the sheen of power and luxury, which belies the high-flying altruistic purposes Varon and his partners foresee for their low-flying dream pod.</p> <p>“We’re not focused on designing and building flying cars to sell them,” Varon says. “It would be for a service. And if I can get away with it, I would like the service to be free.”</p> <p>He says it could go where traffic and congestion are a problem, or there’s a lack of public transportation.</p> <p>“In developing countries, you have areas with low accessibility, low quality of life,” he says. “Nutritious food and other necessities can’t get to those in need. It would take an hour and a half to reach them. A flying car would take only 17 to 20 minutes.”</p> <p>Varon and his partners did a soft-launch for the prototype in Colombia and received positive feedback. He says he’s also been invited to launch it&nbsp;in European countries and is in conversation with aeronautical regulatory authorities there. Similarly, he hopes to approach the Federal Aviation Administration in the U.S., and is looking at a possible test site in Texas.</p> <p>“We’ve tried to identify a market niche within an industry that hasn’t even appeared yet,” Varon says.&nbsp;</p> <p>Varon is still searching for a clean power source.&nbsp;</p> <p>“We’re clean at the point where we charge,” he says, “but what happens behind the grid?”</p> <p>He envisions someday sharing assets with a hydro-electric power entity. “We don’t want to have a (negative) environmental impact,” he says. “We want to have a favorable social and economic impact, even providing jobs. We’re going to have a fleet of cars, so we’re going to need a fleet of drivers.”</p> <p><b><a href="http://news.mit.edu/2018/mit-professional-education-student-felipe-varon-flying-car-dream-taking-off-0926" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>538</wp:post_id>
		<wp:post_date><![CDATA[2018-09-26 21:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-26 21:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[one-mans-flying-car-dream-is-taking-off-thanks-to-mit]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/mit-professional-education-student-felipe-varon-flying-car-dream-taking-off-0926]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>MIT alumnus and GM engineer returns to campus to inspire student innovation</title>
		<link>https://fifthlevel.ai/archives/540</link>
		<pubDate>Sun, 09 Sep 2018 13:59:57 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/mit-alumnus-will-dickson-gm-student-innovation-0909</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Will Dickson ’14 has parked General Motors’ first self-driving vehicle, the Cruise AV, on campus and invited MIT students to think flexibly about its design opportunities. “You are future engineers and thought leaders in the area of new machines,” he says. “How do you design future vehicles like this one better for a safe and autonomous experience?”</p> <p>Fostering innovative thinking is at the heart of Dickson’s career as an innovation champion for General Motors on campus. Dickson bridges the gap between GM’s advanced engineering teams and students on campus by creating opportunities to work on technical problems together. He recruits students, builds partnerships, and scours the MIT startup community and beyond for collaborations both unconventional and fitting.</p> <p>“I pride myself on attempting nontraditional things and pulling in tons of stakeholders. I am the person trying something first and clearing out the hurdles and blazing new trails,” says Dickson, who studied materials science and engineering at MIT and during graduate work at the University of California at Berkeley before joining GM and thriving in a series of positions, including currently as innovation champion within iHub, General Motors’ innovation incubator and consultancy.</p> <p>Among other things, Dickson is transcending traditional corporate engagement in higher education with its heavy reliance on job fairs and research sponsorship. He’s instead focusing his energy, which is described as engaging and dogged, on working with students, faculty, and administrators within some of MIT’s most innovative programs.</p> <p><strong>Real projects and real machines</strong></p> <p>MIT’s new project-centric cross-departmental program, the New Engineering Education Transformation (NEET), launched as a pilot last year and is redefining engineering education, says Dickson. Its hands-on focus on applying fundamental and systems engineering to real-world projects inspired General Motors to sponsor NEET’s project thread on “Autonomous Machines” this fall. (There are now over 120 sophomores and juniors in this and the other three NEET threads: Clean Energy Systems; Advanced Materials Machines; and Living Machines.)</p> <p>“I’m excited for you to be exposed to real projects that our engineers are kicking around and looking for a new perspective on,” says Dickson to NEET students in a pizza-filled classroom near the autonomous test vehicle parked outside. “We’re going to bring engineering leaders to campus to see you in action. To see you working on projects. To see you doing stuff in teams. To interact with you.”</p> <p>One of Dickson’s talents involves helping other young people with engineering backgrounds bridge the gap between technical expertise and creative and meaningful application in industry. “It’s not just about being the smartest person in the room,” he tells the rapt students.</p> <p>“It’s about who can let the other people talk when they need to. Who can lead? Who can be a great project manager? Who can communicate their technical findings to people without the same background as you? Who can identify the right problem to be solving?”</p> <p>At 6 feet 8 inches, Dickson towers over most of the young people in the room. He speaks to them with friendly confidence and a level of industry knowledge that sets him apart despite the slim difference in age between he and them.</p> <p>“Will engages with students in such a personable manner,” whispers NEET’s executive director Amitava "Babi" Mitra as he watches Dickson from the back of the classroom. “As an MIT alum, he’s passionate about NEET. He wants to do right by MIT and by GM,” says Mitra, describing Dickson as instrumental in securing the new sponsorship from GM and in working closely with NEET to help create project and other opportunities for NEET students. He adds with a smile: “Will may look a little intimidating at that height but he’s extremely approachable.”</p> <p><strong>Making spots at GM</strong></p> <p>MIT student Sebastian Uribe knows Dickson’s mentorship is of great impact. Last winter, Uribe was among four winners of a hackathon organized by Dickson and sponsored by GM during Independent Activities Period (IAP) this past January. He and his teammates earned a summer internship that involved automating complex test protocols, working with engineers on autonomous vehicles, and with sensors, Super Cruise, and other innovations.</p> <p>Now Uribe is enrolled in the “Autonomous Machines” thread of the NEET program. Today Dickson shares the second-year student’s success story with others in the room as a kind of lesson in nontraditional learning and networking. “Sebastian here had an internship with us this past summer,” says Dickson with a smile toward Uribe in the second row.</p> <p>“At a career fair, we wouldn’t have looked at Sebastian. There are just too many people — but Sebastian and his team killed it during our inaugural BlacktopBuild during IAP.”&nbsp;</p> <p>Dickson tells the room that during the hackathon, he shared a real engineering problem with students in a tent built in a parking lot at MIT in the middle of a New England winter, which resulted in powerful solutions that were well-received by engineering leaders within GM.&nbsp;</p> <p>“Leaders at GM wanted this team to come back for an internship,” says Dickson. “GM created spots out of nothing for them — outside of the ordinary process — which I think is wild.” Then Dickson pauses for impact and adds, “After the fact, I was like, “Oh, by the way, they’re freshmen.” The classroom fills with laugher and a sense of promise.</p> <p><strong>Popular mentor and role model</strong></p> <p>“Will knows his way around MIT and has positively influenced my professional career,” says Uribe in a later interview. He says networking as a first-year student was “very much a nervous, sweaty-hands environment” but Dickson’s advice made him “far more confident in a professional space.”</p> <p>Today Uribe describes Dickson as a friend. “Will encourages me to build a network in which I can actively reach out and share thoughts and ideas just as he does,” says Uribe. “It wasn’t difficult to get along with Will. He is very social and outgoing and inspiring.”</p> <p>Dickson’s high energy is the first quality that comes to mind for Jinane Abounadi, executive director of MIT Sandbox, which opens pathways for student innovators by connecting them with educational experiences, mentoring, and funding.</p> <p>Dickson played an important role in signing up GM as a sponsor of Sandbox and has been an important connection to all the teams looking to develop technologies with relevance to the automotive industry, she says. He held workshops providing important perspectives for students on how a big company like GM works with suppliers and on the process of innovation that he’s been a champion for at GM. He mentored dozens of Sandbox teams and plays an important role in providing a deep understanding of real-world problems to the teams.&nbsp;</p> <p>“It is quite impressive that Will is able to play the leadership role within GM at such a young professional age,“ says Abounadi. “He’s able to create meaningful connections between GM and MIT students, developing a nice model for direct industry engagement with students and faculty. Adds Mitra, “Will is truly the consummate engineer and people person.”</p>
<p><b><a href="http://news.mit.edu/2018/mit-alumnus-will-dickson-gm-student-innovation-0909" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>540</wp:post_id>
		<wp:post_date><![CDATA[2018-09-09 13:59:57]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-09 13:59:57]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[mit-alumnus-and-gm-engineer-returns-to-campus-to-inspire-student-innovation]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/mit-alumnus-will-dickson-gm-student-innovation-0909]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Renault EZ-Pro Concept Imagines The Autonomous Delivery Future</title>
		<link>https://fifthlevel.ai/archives/628</link>
		<pubDate>Wed, 19 Sep 2018 12:57:12 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/266619/renault-ez-pro-delivery-concept/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[Renault shaped the EZ-Pro to blend into the cityscape rather than stick out. <p><b><a href="https://www.motor1.com/news/266619/renault-ez-pro-delivery-concept/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>628</wp:post_id>
		<wp:post_date><![CDATA[2018-09-19 12:57:12]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-19 12:57:12]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[renault-ez-pro-concept-imagines-the-autonomous-delivery-future]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/266619/renault-ez-pro-delivery-concept/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Embrace Change. It’s What Keeps Us Moving Forward</title>
		<link>https://fifthlevel.ai/archives/1027</link>
		<pubDate>Mon, 17 Sep 2018 18:01:03 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/b380f0e1e63a</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>Thoughts from Sameep Tandon on Drive.ai’s path ahead</em></p><p><strong>Driving Into the Future</strong></p><p>Drive.ai has come a long way in three short years. What began as an idea concepted among friends and labmates is now one of the few companies that has self-driving vehicles out on public roads, driving members of the public from place to place, autonomously. Together, we have made self-driving cars into a reality. What lies ahead for the company is the goal of generating revenue and scaling our self-driving deployments — setting up more programs in more cities, ultimately making self-driving a reality for more people.</p><p><strong>New Role, Same Commitment</strong></p><p>Drive.ai is focused on solving the most challenging problems facing autonomous driving, and putting industry-leading technology on the road. And while the next phase of the company is focused on deploying more vehicles, I will be turning my attention to one of my passions — technology. As I take a more technology-centered role at Drive.ai, I am proud to announce that Dr. Bijit Halder, an inspired colleague who I have already learned so much from this year, will take on the role of CEO.</p><p><strong>A Leader Focused on Growth</strong></p><p>Bijit understands the vision of Drive.ai. He is intimately familiar with the product and service we’re building. He has navigated the team through critical decisions that have enabled us to be at the place where we are today. As the new chief executive officer, Bijit will successfully guide Drive.ai through the next phase of growth as we deploy more self-driving programs.</p><p>With two decades of experience as entrepreneur and executive, Bijit has built and led diverse teams, worked with wide range of advanced technologies, and created products and services in highly competitive markets. He combines a deep understanding of technology and market trends to create a clear, strategic vision and focuses on efficient execution to deliver delightful products and services that solve real world problems. Bijit’s talent and leadership have been a driving force on the Drive.ai team since he joined, and he was a pivotal player in bringing self-driving technology to the public this year.</p><p>For over a year, the company has also been privileged to have the active guidance and support of board member Andrew Ng. With Bijit stepping into the CEO role, Andrew will continue to lend his support by working closely with the company and leadership team going forward.</p><p><strong>Embracing Change</strong></p><p>As a company in one of the most rapidly evolving industries, we at Drive.ai know a thing or two about change. Every day, we push the self-driving movement forward by inciting change — whether advancing our technology, deploying self-driving programs in new cities, or making self-driving technology accessible to more people, change is the only constant.</p><p>Driving change is the common element that brings the Drive.ai team together. This change is just what Drive.ai needs to push the boundaries and strive toward the greater mission of making self-driving a reality for even more people. Together, we have the opportunity to change the face of mobility in meaningful ways, and, in our own way, change the world for the better.</p><p>As for me, I can’t wait to roll up my sleeves and dig in on what I’m passionate about — building the technology that’s enabling this change.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b380f0e1e63a" width="1" height="1"> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1027</wp:post_id>
		<wp:post_date><![CDATA[2018-09-17 18:01:03]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-17 18:01:03]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[embrace-change-its-what-keeps-us-moving-forward]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@drive.ai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@drive.ai/embrace-change-its-what-keeps-us-moving-forward-b380f0e1e63a?source=rss-37851f177ff8------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@drive.ai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@drive.ai/embrace-change-its-what-keeps-us-moving-forward-b380f0e1e63a?source=rss-37851f177ff8------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[941]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Roadstar.ai to use Renovo AWare as OS for its Automated Driving System</title>
		<link>https://fifthlevel.ai/archives/1321</link>
		<pubDate>Fri, 28 Sep 2018 07:53:49 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14173</guid>
		<description></description>
		<content:encoded><![CDATA[<p align="left">Roadstar.ai, a SAE level-4 autonomous technology pioneer, announced that it has selected AWare, Renovo’s production-grade OS for automated mobility, as its development and commercialisation platform.</p>
<p align="left">
<p align="left">Roadstar.ai gains access to the ecosystem of AWare-compatible vehicles, sensors, and other third-party technologies and services that interact with its automated driving system (ADS). By providing automotive safety infrastructure and a full microservice abstraction of the vehicle, compute and sensors, AWare allows Roadstar.ai to focus on its core HeteroSync &amp; DeepFusion ADS technologies, while safely, reliably, and cost-effectively interfacing with third-party technologies and services across a wide range of vehicles. Roadstar.ai’s first AWare-powered vehicle model is the Chrysler Pacifica Hybrid minivan.</p>
<p align="left">
<p align="left">“Roadstar.ai’s mission is to enable fully autonomous robo-taxi services for an intelligent, efficient and safe mobility experience,” said Dr. Xianqiao Tong, founder and CEO of Roadstar.ai. “We selected Renovo’s AWare OS for development and as a commercialisation platform because of its technical capabilities, the company’s focus on deployment at scale, access to the AWare ecosystem and as a go-to-market path for our ADS with North American and European fleet operators.”</p>
<p align="left">
<p align="left">The Renovo AWare OS provides a vehicle-independent abstraction API and range of microservices that allows developers of a range of technologies necessary for commercial automated mobility services, to write applications that can be deployed at scale. AWare has the necessary performance to run today’s most demanding Automated Driving Systems (ADS) as well as the safety and security features required for automotive validation. AWare is a trusted layer in functionally safe architectures and features multiple computational domains including low-level safety controllers. Renovo partners with a wide range of hardware and component suppliers to build systems that can be deployed at scale on road today.</p>
<p align="left">
<p align="left">“Roadstar.ai is a pioneering creator of full stack Level 4 automated driving technology, and we are delighted they have selected AWare as a development and production platform,” said Chris Heiser, Renovo CEO and Co-Founder. “Roadstar.ai and Renovo share a common focus on taking automated mobility to scale in a safe and economical manner as quickly as possible.”</p>
<p align="left">
<p align="left">Despite a highly competitive and fast-moving market, Roadstar.ai already achieved several leading benchmarks in the industry. It won the 1st place of the computer vision benchmark at Cityscapes dataset and ranked top 3 in KITTI 3D detection dataset.</p>
<p align="left">
<p align="left">Roadstar.ai team is an impressive roster of talent that includes engineers who worked in top autonomous driving companies including Google, Tesla, Apple, Nvidia, and Baidu USA, etc. In May 2018, Roadstar.ai announced it had raised $128M in series A financing to support and accelerate the development and deployment of its automated driving technology.</p>
<p align="left">
<p align="left">Roadstar.ai has been testing its automated vehicle technology on public roads under its California DMV Autonomous Vehicle Testing permit in the US and on public roads in Shenzhen, China. Roadstar.ai was named best AI Startup and best overall Startup at CES Asia 2018.</p>
<p align="left">
<p align="left">AWare is today powering automated vehicle fleets including that of Voyage.auto and Renovo’s own on public roads today. Roadstar.ai joins the AWare ecosystem along with a growing list of leading companies in the automated mobility sector including Affectiva, Argus Cyber Security, BestMile, CARMERA, Civil Maps, EdgeConneX, INRIX, Metamoto, NIRA Dynamics, Parsons, Perceptive Automata, Phantom.auto, Samsung, Silexica, Speak With Me, Stanford University, understand.ai, Velodyne LiDAR and Verizon.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/roadstar-ai-use-renovo-aware-os-automated-driving-system/">Roadstar.ai to use Renovo AWare as OS for its Automated Driving System</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/roadstar-ai-use-renovo-aware-os-automated-driving-system/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1321</wp:post_id>
		<wp:post_date><![CDATA[2018-09-28 07:53:49]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-28 07:53:49]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[roadstar-ai-to-use-renovo-aware-as-os-for-its-automated-driving-system]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/roadstar-ai-use-renovo-aware-os-automated-driving-system/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>A message from the new CEO of comma.ai</title>
		<link>https://fifthlevel.ai/archives/3250</link>
		<pubDate>Fri, 14 Sep 2018 17:23:19 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/4db20da0f670</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Earlier this week, George Hotz, founder and former CEO of comma.ai, became the Head of Research Team and promoted me, Riccardo Biasini, from VP of Quality to CEO.</p><p>Before joining comma, I received my master’s degree in automotive engineering from the University of Pisa, Italy, in 2010. I then moved to the US, to work at Tesla from 2011 to 2016. At Tesla I led the development of the Traffic Aware Cruise Control, along with other safety and convenience driver assistance features. I then became responsible for the architecture of controls, safety, and functional behavior of the electric propulsion system. I joined comma in 2016, where I developed the automated lateral and longitudinal controls for comma’s first self driving car. I was promoted to VP of Quality in 2017. It’s now with immense excitement that I accept the role of CEO to lead comma’s future.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/889/1*k5IfxeAji8miwLbcfcNsvg.jpeg" /><figcaption>George and Riccardo in 2016, out of the garage where comma started</figcaption></figure><p><em>“My oldest son is 11, this means that in four and a half years he is going to be able to get his driver license. My team now is committed to make sure that that does not happen.”</em></p><p><em>“In about 2 years, summon should work anywhere connected by land and not blocked by borders, e.g. you’re in LA and the car is in NY.”</em></p><p>The two quotes above are from Chris Urmson (ex-Google, now Aurora) and Elon Musk (Tesla) from 2015 and 2016 respectively. Now, about three years later, those goals don’t seem much closer. Driverless cars have a long way to go before they start affecting the lives of the 140 million daily US commuters and it’s unrealistic to assume that this will change anytime soon. Even apart from the technical challenges, driverless rides are nowhere near beating ridesharing services in either cost or practicality.</p><p>At comma we think that most self driving startups and automakers are overcommitting to driverless technology, while missing the focus of what’s problematic in today’s way of driving.</p><p>So, how do we make driving better?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NrHXRTpjyO_Y2J3gLJI_zA.jpeg" /><figcaption>A comma EON development kit running openpilot</figcaption></figure><p>Comma’s mission is to provide a solution to enhance people’s drives, on cars they already own. We believe that driver assistance features, like automated highway driving, in conjunction with a monitoring system that checks the driver’s attention, will be a significant step forward in reducing driving stress and road accidents.</p><p>In fact, most of the new cars are already equipped with actuators capable of accelerating, braking and steering without inputs from the driver. Unfortunately, they lack the software and the computing power to provide high quality driving assistance features. They also lack the capability of receiving over-the-air software updates. Despite an average lifespan of 15 years for new cars, their features soon become obsolete. In most cases, new car models are equipped with computing units and software that were developed years before their launch.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*fOKZU0K0Vsk4LDDVC0RkXQ.png" /><figcaption>The <a href="https://en.wikipedia.org/wiki/FLOPS#Hardware_costs">cost of computing</a> has decreased by more than 100 times in the last 10 years</figcaption></figure><p>At comma, we believe that software and driver assistance computing units should be replaceable and upgradable parts in cars. Cameras are easily upgradable too and we think they are the only sensors necessary for the car to accurately perceive the surrounding environment.</p><p>Almost two years ago, comma launched an open source software project called <a href="https://github.com/commaai/openpilot">openpilot</a>, which includes a comprehensive suite of driver assistance features.</p><p>Openpilot is compatible with almost every new Honda and Toyota sold in the US. Moreover, a selected set of Lexus, Acura, Hyundai, and Chevrolet models are also supported. Combined, this amounts to approximately 5 millions cars in the US alone. As older cars gets replaced by newer ones, we estimate this number to grow to tens of millions over the next 2–3 years.</p><p>A year ago, comma also released a hardware platform called <a href="https://shop.comma.ai/products/eon-dashcam-devkit">EON</a>, which is a dashcam that optionally connects to the car’s communication system to record the car’s data in sync with driving videos. An extra camera facing the driver is used to monitor the driver’s attention. EON’s users can then review their drives on the <a href="https://my.comma.ai/">comma Explorer</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/995/1*74DcqRRvOHukgM8Bqs-SLA.png" /><figcaption>comma.ai explorer</figcaption></figure><p>Thanks to EON users, comma has already collected over 5 millions miles of driving data. We use this data to constantly train and update our driving models.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/1*7wsBjuH2UZ-9hRt1fh0AUA.png" /><figcaption>Our segnet model automatically labels roads, lane markings, vehicles and undrivable surrounding.</figcaption></figure><p>Data are also used to build HD maps in order to help self driving cars localize themselves on the world and navigate. Such maps must be a centimeter-accurate description of the real world, containing all things relevant to road navigation: lanes, curvature, road boundaries and world features. In the future, we plan on making the maps available to the public.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*E2o2oLRRg0maVEUt9ZztNQ.gif" /><figcaption>World features from the Bay Bridge</figcaption></figure><p>In addition, openpilot is compatible with the EON hardware platform, so developers have the possibility of using EONs as development kits. Developers all over the world have significantly contributed to improve openpilot.</p><p>As CEO of comma, I will work on scaling comma’s self driving solution by making openpilot the Android of the driver assistance systems for cars. Comma will seek partnerships with automakers and tier 1 driver assistance system suppliers which embrace our same vision.</p><p>George will lead comma’s research team, focusing on machine learning and developing the models for openpilot, which are used to make driving decisions and track the driver’s attention.</p><p>If you want to get involved:</p><ul><li>Follow us on <a href="https://twitter.com/comma_ai">Twitter</a> and join our <a href="https://slack.comma.ai">Slack</a>. It’ll be fun!</li><li>Checkout and contribute to <a href="https://github.com/commaai/openpilot">openpilot</a>.</li><li>Apply to our <a href="https://comma.ai/jobs/">open positions</a>.</li></ul><p>Looking forward!</p><p>Riccardo</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1/0*M5chBXmh1P0zfE96" /></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4db20da0f670" width="1" height="1"> <p><b><a href="https://medium.com/@comma_ai/a-message-from-the-new-ceo-of-comma-ai-4db20da0f670?source=rss-330bac69b283------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3250</wp:post_id>
		<wp:post_date><![CDATA[2018-09-14 17:23:19]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-14 17:23:19]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[a-message-from-the-new-ceo-of-comma-ai]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@comma_ai/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@comma_ai/a-message-from-the-new-ceo-of-comma-ai-4db20da0f670?source=rss-330bac69b283------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The Lyft Level 5 Challenge</title>
		<link>https://fifthlevel.ai/archives/3271</link>
		<pubDate>Fri, 14 Sep 2018 23:30:47 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/389f733814d8</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*J9PBIAKbH-3_z1agqWgxXA.png" /></figure><p>We are thrilled to announce the <a href="https://codeforces.com/lyft2018">Lyft Level 5 Challenge</a> hosted by Codeforces and sponsored by Lyft. This international competition consists of two, timed rounds, where competitors will solve multiple algorithmic puzzles much like the ones we’re solving at Level 5, Lyft’s self-driving division.</p><p><strong>Round 1:</strong> Elimination</p><p>On Sunday, October 7, 2018 at 10:00 PDT, the competition kicks off with a standard two-hour round of five problems. The top 100 participants will win a Lyft Level 5 Challenge t-shirt. The top 30 contestants located in the San Francisco Bay area will be invited to Round 2.</p><p><strong>Round 2:</strong> Final at Level 5</p><p>On Sunday, November 4, 2018 at 10:00 PST, round 2 kicks off with a standard two-hour round of five problems. Up to 30 contestants will compete for prizes at the Level 5 Engineering Center in Palo Alto, California. Codeforces will host a parallel round for offsite participants. After the challenge, stick around for a Level 5 team social.</p><p><strong>Round 2 Prizes</strong></p><ul><li>First place: $2000</li><li>Second place: $1000</li><li>Third place: $500</li></ul><p><a href="https://docs.google.com/document/d/1u_7WYIE-mL1ZJJuScrVVV3-eCtxP4the-lipJlSaf9Q"><em>Terms and conditions</em></a><em> apply.</em></p><h3>About Level 5</h3><p>Level 5 is Lyft’s self-driving division based in Palo Alto, California, and Munich, Germany, where we’re building a full self-driving system. Engineers at Level 5 are equipping vehicles with the latest advancements in sensors and applying deep learning to predict and perceive the world around us. We’re leveraging route by route data from the 10 million Lyft rides completed weekly to both rapidly advance our technology and train our vehicles.</p><p>Every member of our fast-moving, collaborative team has the opportunity to have an outsized influence on our self-driving development. And we’re passionate problem solvers, too. Meet Vladimir Iglovikov [<a href="https://www.kaggle.com/iglovikov">kaggle</a>], Viktor Barinov [<a href="http://codeforces.com/profile/vsb">cf</a>, <a href="https://www.topcoder.com/members/vsb/">tc</a>], and Alex Lapin [<a href="http://kaggle.com/alapin">kaggle</a>, <a href="http://codeforces.com/profile/vatr">cf</a>, <a href="https://www.topcoder.com/members/vatr/">tc</a>].</p><p>Help define the future of self-driving technology with us at Level 5. See our open roles at <a href="https://www.lyft.com/self-driving-vehicles/engineers">lyft.com/level5</a>, and follow us on Twitter <a href="https://twitter.com/LyftLevel5">@LyftLevel5</a>.</p><h3>What is Lyft?</h3><p>Lyft is the fastest growing ridesharing company in the United States with coverage for over 95% of Americans in all 50 states. We’re working toward our mission of improving people’s lives with the world’s best transportation.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=389f733814d8" width="1" height="1"> <p><b><a href="https://medium.com/@LyftLevel5/the-lyft-level-5-challenge-389f733814d8?source=rss-d6f431a02b8c------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3271</wp:post_id>
		<wp:post_date><![CDATA[2018-09-14 23:30:47]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-14 23:30:47]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[the-lyft-level-5-challenge]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@LyftLevel5/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@LyftLevel5/the-lyft-level-5-challenge-389f733814d8?source=rss-d6f431a02b8c------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Software finds the best way to stick a Mars landing</title>
		<link>https://fifthlevel.ai/archives/3569</link>
		<pubDate>Wed, 26 Sep 2018 04:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/software-finds-best-mars-landing-0926</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Selecting a landing site for a rover headed to Mars is a lengthy process that normally involves large committees of scientists and engineers. These committees typically spend several years weighing a mission’s science objectives against a vehicle’s engineering constraints, to identify sites that are both scientifically interesting and safe to land on.</p> <p>For instance, a mission’s science team may want to explore certain geological sites for signs of water, life, and habitability. But engineers may find that those sites are too steep for a vehicle to land safely, or the locations may not receive enough sunlight to power the vehicle’s solar panels once it has landed. Finding a suitable landing site therefore involves piecing together information collected over the years by past Mars missions. These data, though growing with each mission, are patchy and incomplete.</p> <p>Now researchers at MIT have developed a software tool for computer-aided discovery that could help mission planners make these decisions. It automatically produces maps of favorable landing sites, using the available data on Mars’ geology and terrain, as well as a list of scientific priorities and engineering constraints that a user can specify.</p> <p>As an example, a user can stipulate that a rover should land in a site where it can explore certain geological targets, such as open-basin lakes. At the same time, the landing site should not exceed a certain slope, otherwise the vehicle would topple over while attempting to land. The program then generates a “favorability map” of landing sites that meet both constraints. These locations can shift and change as a user adds additional specifications.</p> <p>The program can also lay out possible paths that a rover can take from a given landing site to certain geological features. For instance, if a user specifies that a rover should explore sedimentary rock exposures, the program produces paths to any such nearby structures and calculates the time that it would take to reach them.</p> <p>Victor Pankratius, principal research scientist in MIT’s Kavli Institute for Astrophysics and Space Research, says mission planners can use the program to quickly and efficiently consider different landing and exploratory scenarios.</p> <p>“This is never going to replace the actual committee, but it can make things much more efficient, because you can play with different scenarios while you’re talking,” Pankratius says.</p> <p>The team’s study was published online on Aug. 31 by <em>Earth and Space Science</em> and is part of the journal’s Sept. 8 online issue.</p> <p><strong>New sites</strong></p> <p>Pankratius and postdoc Guillaume Rongier, in MIT’s Department of Earth, Atmospheric and Planetary Sciences, created the program to identify favorable landing sites for a conceptual mission similar to NASA’s Mars 2020 rover, which is engineered to land in horizontal, even, dust-free areas and aims to explore an ancient, potentially habitable, site with magmatic outcrops.</p> <p>They found the program identified many landing sites for the rover that have been considered in the past, and it highlighted other promising landing sites that were rarely proposed. “We see there are sites we could explore with existing rover technologies, that landing site committees may want to reconsider,” Pankratius says.</p> <p>The program could also be used to explore engineering requirements for future generations of Mars rovers. “Assuming you can land on steeper curves, or drive faster, then we can derive which new regions you can explore,” Pankratius says.</p> <p><strong>A fuzzy landing</strong></p> <p>The software relies partly on “fuzzy logic,” a mathematical logic scheme that groups things not in a binary fashion like Boolean logic, such as yes/no, true/false, or safe/unsafe, but in a more fluid, probability-based fashion.</p> <p>“Traditionally this idea comes from mathematics, where instead of saying an element belongs to a set, yes or no, fuzzy logic says it belongs with a certain probability,” thus reflecting incomplete or imprecise information, Pankratius explains.</p> <p>In the context of finding a suitable landing site, the program calculates the probability that a rover can climb a certain slope, with the probability decreasing as the a location becomes more steep.</p> <p>“With fuzzy logic we can expresses this probability spatially — how bad is it if I’m this steep, versus this steep,” Pankratius says. “It’s is a way to deal with imprecision, in a way.”</p> <p>Using algorithms related to fuzzy logic, the team creates raw, or initial, favorability maps of possible landing sites over the entire planet. These maps are gridded into individual cells, each representing about 3 square kilometers on the surface of Mars. The program calculates, for each cell, the probability that it is a favorable landing site, and generates a map that is color-graded to represent probabilities between 0 and 1. Darker cells represent sites with a near-zero probability of being a favorable landing site, while lighter locations have a higher chance of a safe landing with interesting scientific prospects.</p> <p>Once they generate a raw map of possible landing sites, the researchers take into account various uncertainties in the landing location, such as changes in trajectory and potential navigation errors during descent. Considering these uncertainties, the program then generates landing ellipses, or circular targets where a rover is likely to land to maximize safety and scientific exploration.</p> <p>The program also uses an algorithm known as fast marching to chart out paths that a rover can take over a given terrain once it’s landed. Fast marching is typically used to calculate the propagation of a front, such as how fast a front of wind reaches a shore if traveling at a given speed. For the first time, Pankratius and Rongier applied fast marching to compute a rover’s travel time as it travels from a starting point to a geological structure of interest.</p> <p>“If you are somewhere on Mars and you get this processed map, you can ask, ‘From here, how fast can I go to any point in my surroundings? And this algorithm will tell you,” Pankratius says.</p> <p>The algorithm can also map out routes to avoid certain obstacles that may slow down a rover’s trip, and chart out probabilities of hitting certain types of geological structures in a landing area.</p> <p>“It’s more difficult for a rover to drive through dust, so it’ll go at a slower pace, and dust isn’t necessarily everywhere, just in patches,” Rongier says. “The algorithm will consider such obstacles when mapping out the fastest traverse paths.”</p> <p>The teams says operators of current rovers on the Martian surface can use the software program to direct the vehicles more efficiently to sites of scientific interest. In the future, Pankratius envisions this technique or something similar to be integrated into increasingly autonomous rovers that don’t require humans to operate the vehicles all the time from Earth.</p> <p>“One day, if we have fully autonomous rovers, they can factor in all these things to know where they can go, and be able to adapt to unforeseen situations,” Pankratius says. “You want autonomy, otherwise it can take a long time to communicate back and forth when you have to make critical decisions quickly.”</p> <p>The team is also looking into applications of the techniques in geothermal site exploration on Earth in collaboration with the MIT Earth Resources Lab in the Department of Earth, Atmospheric and Planetary Sciences.</p> <p>“It’s a very similar problem,” Pankratius says. “Instead of saying ‘Is this a good site, yes or no?’ you can say, ‘Show me a map of all the areas that would likely be viable for geothermal exploration.’”</p> <p>As data improve, both for Mars and for geothermal structures on Earth, he says that that data can be fed into the existing program to provide more accurate analyses.</p> <p>“The program is incrementally enhanceable,” he says.</p> <p>This research was funded, in part, by NASA and the National Science Foundation.</p>
<p><b><a href="http://news.mit.edu/2018/software-finds-best-mars-landing-0926" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3569</wp:post_id>
		<wp:post_date><![CDATA[2018-09-26 04:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-26 04:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[software-finds-the-best-way-to-stick-a-mars-landing]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/software-finds-best-mars-landing-0926]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Senate Commerce Committee Holds Hearing on Transportation of Tomorrow Including Unmanned Shipping and Hyperloop Tech</title>
		<link>https://fifthlevel.ai/archives/3572</link>
		<pubDate>Wed, 26 Sep 2018 09:15:17 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.ipwatchdog.com/?p=101483</guid>
		<description></description>
		<content:encoded><![CDATA[<p>One difficulty holding back the further implementation of hyperloop infrastructure is the fact that the new mode of transportation doesn’t fit neatly into existing regulatory framework. In response to a question from Sen. Thune on that subject, Raycroft noted that hyperloop systems were currently under the jurisdiction of the Federal Railroad Administration (FRA). This is despite the fact that certain aspects of hyperloop technology don’t fit neatly into the FRA’s regulatory framework for railroads, including vehicle bodies which are more similar to commercial aircraft. Raycroft said that engagement between the FRA and other agencies within the Department of Transportation could help speed the regulatory process while ensuring that passenger safety remained a top priority. In response to a later question from Sen. Catherine Cortez Masto (D-NV), Raycroft said that, at the current pace, hyperloop technologies would be ready for widespread passenger use during the mid-2020s.</p>
<p>The post <a rel="nofollow" href="http://www.ipwatchdog.com/2018/09/26/senate-commerce-committee-transportation-unmanned-shipping-hyperloop-tech/id=101483/">Senate Commerce Committee Holds Hearing on Transportation of Tomorrow Including Unmanned Shipping and Hyperloop Tech</a> appeared first on <a rel="nofollow" href="http://www.ipwatchdog.com">IPWatchdog.com | Patents &amp; Patent Law</a>.</p> <p><b><a href="http://www.ipwatchdog.com/2018/09/26/senate-commerce-committee-transportation-unmanned-shipping-hyperloop-tech/id=101483/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3572</wp:post_id>
		<wp:post_date><![CDATA[2018-09-26 09:15:17]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-26 09:15:17]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[senate-commerce-committee-holds-hearing-on-transportation-of-tomorrow-including-unmanned-shipping-and-hyperloop-tech]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/tag/autonomous-vehicles/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/2018/09/26/senate-commerce-committee-transportation-unmanned-shipping-hyperloop-tech/id=101483/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Exclusive with Roberta Romano-Götsch, Chief Operating Officer of Mobility and Mechatronics at EPO</title>
		<link>https://fifthlevel.ai/archives/3573</link>
		<pubDate>Sun, 16 Sep 2018 14:15:03 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.ipwatchdog.com/?p=101399</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I recently had the opportunity to go on the record with Roberta Romano-Götsch, the chief operating officer of Mobility and Mechatronics at the European Patent Office (EPO). In a wide ranging, two-part interview we discussed the new technology areas at the EPO, autonomous driving, engineering education, examiner training, what quality means to the EPO and more.</p>
<p>The post <a rel="nofollow" href="http://www.ipwatchdog.com/2018/09/16/roberta-romano-gotsch-mobility-mechatronics-epo/id=101399/">Exclusive with Roberta Romano-Götsch, Chief Operating Officer of Mobility and Mechatronics at EPO</a> appeared first on <a rel="nofollow" href="http://www.ipwatchdog.com">IPWatchdog.com | Patents &amp; Patent Law</a>.</p> <p><b><a href="http://www.ipwatchdog.com/2018/09/16/roberta-romano-gotsch-mobility-mechatronics-epo/id=101399/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3573</wp:post_id>
		<wp:post_date><![CDATA[2018-09-16 14:15:03]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-16 14:15:03]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[exclusive-with-roberta-romano-gotsch-chief-operating-officer-of-mobility-and-mechatronics-at-epo]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="patents"><![CDATA[Patents]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/tag/autonomous-vehicles/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.ipwatchdog.com/2018/09/16/roberta-romano-gotsch-mobility-mechatronics-epo/id=101399/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Sorry, I don’t know whether OpenCV can leverage GPU.</title>
		<link>https://fifthlevel.ai/archives/3582</link>
		<pubDate>Sat, 29 Sep 2018 17:21:58 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/873917821e6d</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Sorry, I don’t know whether OpenCV can leverage GPU. But you definitely can use Python with TensorFlow or PyTorch to run computation on GPU.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=873917821e6d" width="1" height="1"> <p><b><a href="https://medium.com/@surmenok/sorry-i-dont-know-whether-opencv-can-leverage-gpu-873917821e6d?source=rss-d002f056f8c------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3582</wp:post_id>
		<wp:post_date><![CDATA[2018-09-29 17:21:58]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-29 17:21:58]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[sorry-i-dont-know-whether-opencv-can-leverage-gpu]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@surmenok]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@surmenok/sorry-i-dont-know-whether-opencv-can-leverage-gpu-873917821e6d?source=rss-d002f056f8c------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>I see.</title>
		<link>https://fifthlevel.ai/archives/3583</link>
		<pubDate>Fri, 28 Sep 2018 03:17:55 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/8bcf9d28693a</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I see.</p><p>No, I don’t know how to balance it well. Maybe need to try SGDRestart on few different models with different max LR to see if any rule can be derived.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8bcf9d28693a" width="1" height="1"> <p><b><a href="https://medium.com/@surmenok/i-see-8bcf9d28693a?source=rss-d002f056f8c------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3583</wp:post_id>
		<wp:post_date><![CDATA[2018-09-28 03:17:55]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-28 03:17:55]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[i-see]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@surmenok]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@surmenok/i-see-8bcf9d28693a?source=rss-d002f056f8c------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>If lr 1e-4 gave you a faster decrease of loss than 1e-2 then it would be logical to use 1e-4…</title>
		<link>https://fifthlevel.ai/archives/3584</link>
		<pubDate>Thu, 27 Sep 2018 16:53:47 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/27949eb5175</guid>
		<description></description>
		<content:encoded><![CDATA[<p>If lr 1e-4 gave you a faster decrease of loss than 1e-2 then it would be logical to use 1e-4, right? If 1e-4 is “too small to help us escape” then 1e-2 will be even worse because loss decreases slower there.</p><p>There is a difference between an assumption that larger LR helps to drive loss down faster and practical observation of how fast loss decreases with different values of LR (that’s what we do with LR finder here).</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=27949eb5175" width="1" height="1"> <p><b><a href="https://medium.com/@surmenok/if-lr-1e-4-gave-you-a-faster-decrease-of-loss-than-1e-2-then-it-would-be-logical-to-use-1e-4-27949eb5175?source=rss-d002f056f8c------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3584</wp:post_id>
		<wp:post_date><![CDATA[2018-09-27 16:53:47]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-27 16:53:47]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[if-lr-1e-4-gave-you-a-faster-decrease-of-loss-than-1e-2-then-it-would-be-logical-to-use-1e-4]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@surmenok]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@surmenok/if-lr-1e-4-gave-you-a-faster-decrease-of-loss-than-1e-2-then-it-would-be-logical-to-use-1e-4-27949eb5175?source=rss-d002f056f8c------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Tesla Model 3 Autopilot Outperforms Competitors In Lane-Keeping Test</title>
		<link>https://fifthlevel.ai/archives/234</link>
		<pubDate>Sun, 30 Sep 2018 13:20:40 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b73b25c64aaf942a5e2f994</guid>
		<description></description>
		<content:encoded><![CDATA[In a test of lane-keeping ability, the Tesla Model 3 outperformed competitors from other manufacturers and even the Tesla Model S. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>234</wp:post_id>
		<wp:post_date><![CDATA[2018-09-30 13:20:40]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-30 13:20:40]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[tesla-model-3-autopilot-outperforms-competitors-in-lane-keeping-test-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/08/15/tesla-model-3-autopilot-outperforms-model-s-competitors-in-lane-keeping-test/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-08-15]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Three Companies Vying For Traction In Self-Driving Software Platform Race</title>
		<link>https://fifthlevel.ai/archives/236</link>
		<pubDate>Sun, 30 Sep 2018 13:20:46 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5b63e9dc64aaf94bc0d832f2</guid>
		<description></description>
		<content:encoded><![CDATA[Apollo, Autoware, and DRIVE are three self-driving car platforms available as foundations for building a self-driving car. Keep an eye on how much traction they get. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>236</wp:post_id>
		<wp:post_date><![CDATA[2018-09-30 13:20:46]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-30 13:20:46]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[three-companies-vying-for-traction-in-self-driving-software-platform-race-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/08/03/three-companies-that-offer-self-driving-software-platforms/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-08-03]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The auto industry is channeling billions into autonomous vehicle technology</title>
		<link>https://fifthlevel.ai/archives/239</link>
		<pubDate>Sun, 30 Sep 2018 13:20:43 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.driverless-future.com/?p=1173</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The self-driving car industry is growing up. Valuations of self-driving car companies and private investment in these companies are exploding.  Bloomberg reports that private investment in self-driving and connected car companies in the second quarter of 2018 is more than the<a href="https://www.bloomberg.com/news/articles/2018-08-09/ford-is-far-from-first-in-driverless-vehicles-and-investors-want-in"> total private investment in this sector in the prior 4 years combined</a>!  Morgan Stanley has raised its valuation of Waymo from 70 billion in 2017 to <a href="https://markets.businessinsider.com/news/stocks/google-stock-price-waymo-worth-100-billion-more-than-before-morgan-stanley-2018-8-1027439248">175 billion</a>.</p>
<p>But this is only the tip of the iceberg. Below the surface, a major restructuring of the auto industry is underway where self-driving car companies are emerging as the pivotal element in the strategies for future mobility. Over the past years, different approaches to integrating self-driving car technology into auto- and mobility companies have been tried, ranging from various types of acquisitions (GM-Cruise, Ford-ArgoAI, Aptiv(prior: Delphi)-Nutonomy, Intel-MobilEye) to partnerships (Bosch/Daimler, Daimler/BMW, Baidu/Apollo) and go-it alone strategies (Waymo, Zoox, Uber and many others).</p>
<p>Leaving aside Waymo, GM may have found a winning formula, which is increasingly copied by its competitors: When it acquired Cruise Automation in 2016, it allowed the new subsidiary to continue to operate in a highly autonomous mode, its growth and speed largely unencumbered by the rest of GM. Successful collaboration with GM around the electric Chevy Bolt brightened the prospects of both companies and initially led to a significant increase in in GM&#8217;s stock price (which since then has fizzled out). In 2018 Cruise attracted a 2.25 billion USD investment from Softbank&#8217;s Vision fund. Being able to attract outside investment (as well as employees through stock options in Cruise) while having close connections to the resources of the parent company should be an ideal position for Cruise to quickly shift from start-up/development mode to commercialization. At the same time Cruise is insulated from all concerns related to building legacy cars and from the headwinds that classical car companies will have to face from the revolutionary changes in the auto industry. Other auto makers seem to be copying GM&#8217;s strategy. Ford has <a href="https://www.engadget.com/2018/07/25/ford-autonomous-vehicle-llc/">created a self driving division</a> (which includes ArgoAI) and will also be open for outside investment. Volkswagen seems to have been in talks to <a href="https://www.bloomberg.com/news/articles/2018-08-22/vw-is-said-to-have-approached-self-driving-startup-aurora">buy Aurora</a>, but was rebuffed. Daimler who was an early leader in self-driving technology is relying on a partnership with Bosch but is also splitting the company into three separate parts (cars, trucks, mobility (which includes self-driving technology). This has the effect of insulating the less vulnerable parts of the business (trucks and mobility) from potentially dramatic changes in the auto industry. Only Toyota, which has always been late to the self-driving race has chosen a different path by <a href="https://www.reuters.com/article/us-uber-toyota/toyota-to-invest-500-million-in-uber-for-self-driving-cars-idUSKCN1LC203">investing 500 billion USD in Uber</a>, which minimizes its ability to leverage the opportunities associated self-driving car technology.</p>
<p>The last 6 months have shown that the auto (and mobility) industry is now finding ways to channel billions of dollars into the commercialization of self-driving car technology. Given the extent of changes, the capital associated with these changes and the increased ability of translating advances in the technology into actual products and services (which don&#8217;t have to be full fledged drive-autonomously-anyhwere solutions but can be very targeted) it won&#8217;t take several years until we see the first real impact on the streets&#8230;</p> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>239</wp:post_id>
		<wp:post_date><![CDATA[2018-09-30 13:20:43]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-30 13:20:43]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[the-auto-industry-is-channeling-billions-into-autonomous-vehicle-technology-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?feed=rss2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.driverless-future.com/?p=1173]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_date]]></wp:meta_key>
			<wp:meta_value><![CDATA[2018-08-31]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Aeva raises $45 million for autonomous car sensors that can measure velocity</title>
		<link>https://fifthlevel.ai/archives/296</link>
		<pubDate>Mon, 01 Oct 2018 13:00:16 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2399572</guid>
		<description></description>
		<content:encoded><![CDATA[<img width="578" height="386" src="https://venturebeat.com/wp-content/uploads/2018/10/aeva-fortmason-5776.jpg?fit=578%2C386&amp;strip=all" class="attachment-single-feed size-single-feed wp-post-image" alt="Aeva" /><hr />Myriad sensors go into making autonomous cars tick. Drive.ai&#8216;s fleet of Nissan NV200 vans pack lidar (laser-based sensors that measure the distance between themselves and objects) in addition to radar, GPS, and inertial measurement units. Google spinoff Waymo&#8217;s Chrysler Pacifica Hybrid Vans are similarly equipped with lidar, radar, and&hellip;<a href="https://venturebeat.com/2018/10/01/aeva-raises-45-million-for-autonomous-car-sensors-that-can-measure-velocity/" target="_blank">Read More</a> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>296</wp:post_id>
		<wp:post_date><![CDATA[2018-10-01 13:00:16]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-01 13:00:16]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[aeva-raises-45-million-for-autonomous-car-sensors-that-can-measure-velocity]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/10/01/aeva-raises-45-million-for-autonomous-car-sensors-that-can-measure-velocity/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[964]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>LG and Robotis develop ‘self-driving modules’ for robots</title>
		<link>https://fifthlevel.ai/archives/332</link>
		<pubDate>Mon, 01 Oct 2018 17:30:23 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2400128</guid>
		<description></description>
		<content:encoded><![CDATA[<img class="attachment-single-feed size-single-feed wp-post-image" src="https://venturebeat.com/wp-content/uploads/2018/01/cloi.jpg?fit=578%2C388&amp;strip=all" alt="" width="578" height="388" />

<hr />

LG seeks to build modules that’ll help machines navigate autonomously. It today announced a service agreement with Robotis that’ll see the two develop a “self-driving module” for mobile robots. As part of the arrangement, Yonhap News reports, the two companies will contribute software and hardware for robots equipped with…<a href="https://venturebeat.com/2018/10/01/lg-and-robotis-develop-self-driving-modules-for-robots/" target="_blank" rel="noopener">Read More</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>332</wp:post_id>
		<wp:post_date><![CDATA[2018-10-01 17:30:23]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-01 17:30:23]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[lg-and-robotis-develop-self-driving-modules-for-robots]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/10/01/lg-and-robotis-develop-self-driving-modules-for-robots/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[966]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Ford proposes standard language for automated vehicle system intent</title>
		<link>https://fifthlevel.ai/archives/354</link>
		<pubDate>Tue, 02 Oct 2018 18:57:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2400609</guid>
		<description></description>
		<content:encoded><![CDATA[<img width="578" height="385" src="https://venturebeat.com/wp-content/uploads/2018/06/1_BSTnbFMT1gfJLB3oXsxYjQ.jpeg?fit=578%2C385&amp;strip=all" class="attachment-single-feed size-single-feed wp-post-image" alt="Ford Transit Connect" /><hr />Self-driving cars have a perception problem. In a pair of surveys published by the American Automobile Association in January and Gallup in May, 63 percent of people reported feeling afraid to ride in a fully self-driving (Level 5) vehicle. And three subsequent studies this summer &#8212; by the Brookings Institution, the think tank HNTB, and the A&hellip;<a href="https://venturebeat.com/2018/10/02/ford-proposes-standard-language-for-automated-vehicle-systems/" target="_blank">Read More</a> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>354</wp:post_id>
		<wp:post_date><![CDATA[2018-10-02 18:57:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-02 18:57:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[ford-proposes-standard-language-for-automated-vehicle-system-intent]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/10/02/ford-proposes-standard-language-for-automated-vehicle-systems/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[975]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Honda steers $2 billion into GM’s Cruise self-driving cars</title>
		<link>https://fifthlevel.ai/archives/371</link>
		<pubDate>Wed, 03 Oct 2018 15:05:58 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2401062</guid>
		<description></description>
		<content:encoded><![CDATA[<img class="attachment-single-feed size-single-feed wp-post-image" src="https://venturebeat.com/wp-content/uploads/2017/02/Screen-Shot-2017-02-08-at-9.00.45-PM.png?fit=578%2C255&amp;strip=all" alt="" width="578" height="255" />

<hr />

(Reuters) — Honda will invest $2 billion and take a 5.7 percent stake in General Motors’ Cruise self-driving vehicle unit, to jointly develop self-driving vehicles for deployment in ride services fleets around the world. Honda, which has lagged behind many of its rivals in developing self-driving vehicles, is paying $750 million upfront…<a href="https://venturebeat.com/2018/10/03/honda-steers-2-billion-into-gms-cruise-self-driving-cars/" target="_blank" rel="noopener">Read More</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>371</wp:post_id>
		<wp:post_date><![CDATA[2018-10-03 15:05:58]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-03 15:05:58]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[honda-steers-2-billion-into-gms-cruise-self-driving-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/10/03/honda-steers-2-billion-into-gms-cruise-self-driving-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[976]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Seeing the Light: Our Call for a Standard Self-Driving Car Language to Communicate Intent</title>
		<link>https://fifthlevel.ai/archives/382</link>
		<pubDate>Tue, 02 Oct 2018 12:01:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/3f3628cc7b2</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By John Shutko, Ford Human Factors Technical Specialist for Self-Driving Vehicles</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*26qt095OHd8ZxdesEffk-Q.jpeg" /><figcaption>The Ford-developed light bar, located just above the windshield, uses three different light signals to indicate it is yielding, in active driving mode, or preparing to accelerate from a stop.</figcaption></figure><p>In the journey to develop and deploy self-driving vehicles, there’s a tendency to focus most on the customers who will be riding in these vehicles. At Ford, we’re working to earn the trust of everyone involved, including all road users and entire communities where self-driving vehicles will be operating. For this technology to be successful, it’s critical it be integrated into society in a way that makes everyone confident in how it works to serve people and business.</p><p>The idea that pedestrians, cyclists and scooter users should change their behavior to accommodate self-driving cars couldn’t be further from our vision of how this technology should be integrated. It’s why we’ve been hard at work developing an interface we believe will help self-driving vehicles seamlessly integrate with other road users.</p><p>Today, we’re calling on all self-driving vehicle developers, automakers and technology companies who are committed to deploying SAE level-4 vehicles — and believe these vehicles should communicate intent — to join us and share ideas to create an industry standard for communicating driving intent, whether it be driving, yielding or accelerating from a stop. The work we’ve already done is now open to others through a memorandum of understanding that is intended make it easy for us all to work together.</p><p>Why is this the best approach? We want everyone to trust self-driving vehicles — no matter if they are riders in these vehicles themselves or pedestrians, cyclists, scooter users or other drivers sharing the road. Having one, universal communication interface people across geographies and age groups can understand is critical for the successful deployment of self-driving technology.</p><p><strong>Testing the self-driving intent interface</strong></p><p>Last year, <a href="https://medium.com/self-driven/how-self-driving-cars-could-communicate-with-you-in-the-future-e814d276937f">we worked with Virginia Tech Transportation Institute (VTTI) to conduct a real-world study</a> of what we call a self-driving intent interface, a light-bar mounted to the top of a windshield of a Ford Transit Connect van. We took this step after initial design and testing in virtual reality scenarios confirmed the learnability of the signal patterns we had developed.</p><p><a href="https://medium.com/self-driven/how-self-driving-cars-could-communicate-with-you-in-the-future-e814d276937f">How Self-Driving Cars Could Communicate with You in the Future</a></p><p>The VTTI team designed a seat suit that concealed an actual human driver to simulate the van operating on its own to determine if the signal patterns communicating its intent were successful.</p><p>We tested three different lighting scenarios, as well as a baseline condition where the lights were off, to observe how pedestrians and other road users responded to the vehicle signaling its intent:</p><blockquote><strong>Yielding:</strong> Two white lights moving side to side to indicate vehicle is about to come to a full stop</blockquote><blockquote><strong>Active driving mode:</strong> Solid white light to indicate vehicle intends to proceed on its current course (although can respond appropriately to objects and other road users in the course of its travel)</blockquote><blockquote><strong>Start-to-go:</strong> Rapidly blinking white light to indicate vehicle is beginning to accelerate from a stop</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*P4dVPXrGMCZfoiAHF5DAgg.gif" /><figcaption>The light bar used as part of the study with the Virginia Tech Transportation Institute.</figcaption></figure><p>We outfitted the Transit Connect with multiple cameras that allowed us to observe hours of road user response to various signaling of the vehicle’s actions over the course of more than 2,000 miles. The VTTI team cataloged all the footage and found that the light signal interface did not encourage any unsafe behavior by other road users. The results prove there is a baseline for us to build from in terms of the potential to improve acceptance of self-driving vehicles and trust in the technology.</p><p>We then conducted another study in the virtual reality space to test the trust and acceptance hypothesis we’d arrived at. In the digital world, we placed study participants on a street corner to observe and gauge reaction to a complex mix of vehicles driving through an intersection, some equipped with the intent interface light signals and some without. With no prior explanation of what the different signals meant, we found it took about two exposures for participants to learn what a single signal meant and between five and 10 exposures to understand the meaning of all three lighting patterns.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*NICBOD7xrYbrRX1C_3Rbtw.jpeg" /><figcaption>An example of the virtual reality study.</figcaption></figure><p>What’s most encouraging is that the signals had a positive effect on people’s trust in self-driving vehicles, with participants reporting the light signals increased their understanding of what a self-driving vehicle will do.</p><p><strong>What’s next</strong></p><p>Now, we’re ready to take our learnings from the virtual world back into the real world. We’re installing the self-driving intent interface on a small fleet of our Fusion Hybrid self-driving development vehicles to be used by Argo AI in Miami-Dade County. Ongoing testing will continue to expose pedestrians and other road users to the light bar so we can observe their reactions.</p><p>We’re also conducting research in Europe to understand how the same signals are received there so we can ensure they are universally understood across regions and cultures.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VipJTTuaYR_Qo2H5NuXcvA.jpeg" /><figcaption>A Ford self-driving test vehicle indicates to a bicyclist that it is yielding to those in the crosswalk.</figcaption></figure><p>In addition to the proposal to accelerate the industry coming together to work toward standardization, we continue to work in parallel with the International Organization of Standardization (ISO) and the Society of Automotive Engineers (SAE) to create a unified communication interface for self-driving vehicles. Our goal is to reach an agreement in three core areas — placement of the signals on a vehicle, design of the signals and the color of the light signals themselves. To help anyone interested in collaborating, we’re open to sharing the scenarios developed for our virtual reality study, which we’ve already done with some universities and other companies.</p><p>Of course, we recognize some design elements may need to change as we move forward, and we’re open to working together to do that if necessary to find the best possible communication interface. It’s critical that the communications method we agree upon is as readily understandable as a brake light or turn signal indicator.</p><p>After all, ensuring self-driving vehicles are integrated into society without overwhelming or confusing anyone is what success looks like. So to do that, we’ve just got one simple request: Let’s all work together to make it happen.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3f3628cc7b2" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/seeing-the-light-our-call-for-a-standard-self-driving-car-language-to-communicate-intent-3f3628cc7b2">Seeing the Light: Our Call for a Standard Self-Driving Car Language to Communicate Intent</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/seeing-the-light-our-call-for-a-standard-self-driving-car-language-to-communicate-intent-3f3628cc7b2?source=rss----7192a82ca09a---4" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>382</wp:post_id>
		<wp:post_date><![CDATA[2018-10-02 12:01:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-02 12:01:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[seeing-the-light-our-call-for-a-standard-self-driving-car-language-to-communicate-intent]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driven/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/seeing-the-light-our-call-for-a-standard-self-driving-car-language-to-communicate-intent-3f3628cc7b2?source=rss----7192a82ca09a---4]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[906]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Why Ford Is Taking a Quantum Leap into the Future of Computing</title>
		<link>https://fifthlevel.ai/archives/392</link>
		<pubDate>Mon, 01 Oct 2018 11:31:01 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/453128a2ea9f</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Dr. Ken Washington, Vice President, Ford Research and Advanced Engineering, and Chief Technology Officer</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KnvP6EWRDdqA303s77VJ7g.jpeg" /></figure><p>We live in an era where technology breakthroughs are paving the way for major changes in the way we move. Electrification is changing how we power our cars and self-driving vehicles will change how we get around, but there are other game-changing technologies that — if we can properly wrap our minds around them — hold great potential to transform our transportation systems as well.</p><p>One of these is quantum computing, which enables us to process great amounts of information much faster than we can with traditional computers today. We’re always striving at Ford to explore the potential of new technology, and this is just one of several areas in advanced computing that’s creating new opportunities in how we process, manage and store information.</p><p>So while we’re still in the discovery phase when it comes to quantum computing, we know enough to believe that its potential can help us solve real problems that affect the people and businesses using our vehicles. That’s why we’ve hired our own quantum specialists and are beginning to collaborate with experts in the field.</p><p>One of our first collaborations in this space is with NASA. We’ll be working with experts there to better understand how we can frame problems in a way that yields benefits from quantum computing.</p><p>Over the next year, we’ll be working with NASA’s Quantum Artificial Intelligence Laboratory at its Ames Research Center in Silicon Valley. We’ll be using the quantum annealer hosted at Ames — which is shared between NASA, Google and the Universities Space Research Association — to see how we can apply the technology to complex problems that cannot be solved by today’s computers.</p><p>In the scenario we’re testing with NASA, we’re exploring quantum computing’s ability to help commercial fleet owners more efficiently manage the total energy consumption of their large number of vehicles. The scenario involves designing criteria-based vehicle-to-route mappings for diesel delivery vehicles.</p><p>Diesel engines have particulate filters that must be managed for overall efficient operation of the vehicle and environmental compliance, which is achieved when the vehicles are operating in the most optimal drive cycles. In some delivery situations, it’s difficult for vehicles to achieve the right drive cycles due to traffic flow patterns and speed changes, which impacts the efficiency of the filter management process and ultimately, engine performance. So our scenario entails finding the optimal route for a single delivery vehicle making stops at multiple locations carrying out a specific task, then applying that to all vehicles in the fleet.</p><p>As you would expect, a large number of complex variables come into play. When working to design a scheduling system with multiple pick-up and drop-off spots, any location you choose will have an impact on congestion, time management and people’s experiences.</p><p>Right now, using traditional computing methods, you can model these options for a limited number of vehicles and locations. But as soon as you increase the number of vehicles, relevant locations and potential routes, it quickly becomes a very large and expensive problem, and the number of different scenarios needing to be analyzed becomes intractable.</p><p>So this is where quantum computing comes in. Unlike traditional computing methods, which only translate information into a 1 or a 0, a quantum computer can translate information into multiple states. It can even understand something as being in multiple states at the same time, so instead of a computer bit that is either a 1 or a 0, a quantum bit can be both at once.</p><p>If this is all rather mind-blowing, think of it like the difference between a light bulb that’s either on or off, versus a light bulb that can be dimmed. It all boils down to the fact that we can store more information in a quantum bit, or qubit, than we can a traditional computer bit, and process all of it simultaneously.</p><p>Our work with NASA entails encoding all these options into qubits to simulate the efficiency of each to determine the best fit. So in our scenario, all available options in terms of number of locations, density of locations and route timing can be thought of as different states, or measurements. Because qubits can process so much information all at once, we believe such complex problems as fleet route planning can be solved faster relative to computing methods that rely on traditional bits.</p><p>Beyond route planning, we believe quantum computing can make an impact in a number of other areas as the technology evolves, including materials development, manufacturing and battery chemistry optimization.</p><p>Albert Einstein told us curiosity has its own reason for existing. “The important thing is not to stop questioning,” he said. For us, working with NASA creates an opportunity to learn more about the potential of quantum computing to identify quantum-level problems to ensure we know how to design the proper scenarios to put to the test. Once we understand the right way to ask questions in a quantum framework, there’s no telling the power we’ll have to solve potential problems in the future as we work to transform our transportation systems.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=453128a2ea9f" width="1" height="1"> <p><a href="https://medium.com/@ford/why-ford-is-taking-a-quantum-leap-into-the-future-of-computing-453128a2ea9f?source=rss-db92c082f24a------2" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>392</wp:post_id>
		<wp:post_date><![CDATA[2018-10-01 11:31:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-01 11:31:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[why-ford-is-taking-a-quantum-leap-into-the-future-of-computing]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@ford]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@ford/why-ford-is-taking-a-quantum-leap-into-the-future-of-computing-453128a2ea9f?source=rss-db92c082f24a------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[901]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Honda joins forces with Cruise and General Motors to build new Autonomous Vehicle</title>
		<link>https://fifthlevel.ai/archives/1316</link>
		<pubDate>Wed, 03 Oct 2018 14:21:16 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14252</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Cruise and General Motors Co. announced that they have joined forces with Honda to pursue the shared goal of transforming mobility through the large-scale deployment of autonomous vehicle technology.</p> <p>Honda will work jointly with Cruise and General Motors to fund and develop a purpose-built autonomous vehicle for Cruise that can serve a wide variety of use cases and be manufactured at high volume for global deployment. In addition, Cruise, General Motors and Honda will explore global opportunities for commercial deployment of the Cruise network.</p> <p>Honda will contribute approximately $2 billion over 12 years to these initiatives, which, together with a $750 million equity investment in Cruise, brings its total commitment to the project to $2.75 billion.</p> <p>In addition to the recently announced SoftBank investments, this transaction brings the post-money valuation of Cruise to $14.6 billion.</p> <p>“This is the logical next step in General Motors and Honda’s relationship, given our joint work on electric vehicles, and our close integration with Cruise,” said General Motors Chairman and CEO Mary Barra. “Together, we can provide Cruise with the world’s best design, engineering and manufacturing expertise, and global reach to establish them as the leader in autonomous vehicle technology – while they move to deploy self-driving vehicles at scale.”</p> <p>“Honda chose to collaborate with Cruise and General Motors based on their leadership in autonomous and electric vehicle technology and our shared vision of a zero-emissions and zero-collision world,” said Honda Executive Vice President and Representative Director COO Seiji Kuraishi. “We will complement their strengths through our expertise in space efficiency and design to develop the most desirable and effective shared autonomous vehicle.”</p> <p>“With the backing of General Motors, SoftBank and now Honda, Cruise is deeply resourced to accomplish our mission to safely deploy autonomous technology across the globe,” said Cruise CEO Kyle Vogt. “The Honda partnership paves the way for massive scale by bringing a beautiful, efficient, and purpose-built vehicle to our network of shared autonomous vehicles.”</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/honda-joins-forces-cruise-general-motors-build-new-autonomous-vehicle/">Honda joins forces with Cruise and General Motors to build new Autonomous Vehicle</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/honda-joins-forces-cruise-general-motors-build-new-autonomous-vehicle/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1316</wp:post_id>
		<wp:post_date><![CDATA[2018-10-03 14:21:16]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-03 14:21:16]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[honda-joins-forces-with-cruise-and-general-motors-to-build-new-autonomous-vehicle]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/honda-joins-forces-cruise-general-motors-build-new-autonomous-vehicle/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Vayyar’s new automotive sensor kit bridges the gap between LiDar and Radar</title>
		<link>https://fifthlevel.ai/archives/1317</link>
		<pubDate>Wed, 03 Oct 2018 13:15:21 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14249</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Vayyar Imaging, the global leader in 3D radar imaging technology, announced the launch of its new Automotive Evaluation Kit (EVK), representing a significant leap forward in the application of radar technology. The company’s new sensor kit delivers an advanced System on a Chip (SOC) for mmWave 3D imaging to the automotive industry.</p> <p>With this new sensor kit, Vayyar brings point cloud technology to radar, increasing radar’s resolution and capabilities, while reducing costs and minimising the need for other sensors within the vehicle.</p> <p>Raviv Melamed, co-founder, CEO, and Chairman of Vayyar, said: “The automotive industry has long sought a technology that can bridge the gap between radar and LiDar. I’m excited to announce that Vayyar is the first to address that need with the creation of the world’s most-advanced radio frequency sensor technology. Our automotive sensors are powerful enough to enable radar with point cloud capabilities and have high enough resolution to differentiate between objects even in bad lighting or harsh weather conditions. Our technology will help protect drivers and prevent accidents even in the most severe conditions.”</p> <p>Vayyar’s new automotive sensor kit includes Vayyar’s 77-81Ghz ASIC, including 48 transceivers on a chip with internal DSP. It includes a sophisticated array of antennas and a USB interface. The company’s powerful sensors reduce the overall cost and number of sensors needed for the vehicle, resulting in a better driving experience and increasing the possibilities for in-car sensor capabilities for tier 1 customers and original equipment manufacturers (OEMs).</p> <p>Vayyar’s Automotive Evaluation Kit enables:</p> <ul>
<li>Breathing monitoring</li>
<li>Location monitoring and activity level indication</li>
<li>Emergency life detection in the case of an accident (including number of people and their status)</li>
<li>Passenger counting</li>
<li>Assisted parking and driving</li>
<li>Point cloud representation</li>
<li>Lane switch and blind spot detection</li>
</ul>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/vayyars-new-automotive-sensor-kit-bridges-gap-lidar-radar/">Vayyar&#8217;s new automotive sensor kit bridges the gap between LiDar and Radar</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/vayyars-new-automotive-sensor-kit-bridges-gap-lidar-radar/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1317</wp:post_id>
		<wp:post_date><![CDATA[2018-10-03 13:15:21]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-03 13:15:21]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[vayyars-new-automotive-sensor-kit-bridges-the-gap-between-lidar-and-radar]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/vayyars-new-automotive-sensor-kit-bridges-gap-lidar-radar/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Idemia and Altran present a new Assisted Driving Car Concept in Paris</title>
		<link>https://fifthlevel.ai/archives/1318</link>
		<pubDate>Wed, 03 Oct 2018 09:33:42 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14243</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Idemia and Altran are teaming up to develop a new user experience based on driverless vehicles interacting with the human environment and with different infrastructures called “Columbia”, a connected, autonomous, and multi-use vehicle integrating Artificial Intelligence.</p> <p>Idemia&#8217;s technological innovations and solutions, combined with Altran experience and knowhow, aimed at creating two services in one:</p> <ul>
<li>An assisted driving car for personal use based on a shared transport concept</li>
<li>A transport solution for professional users, including dropping off and collecting items along the route.</li>
</ul> <p>Idemia, an expert of leading edge biometric and facial recognition technologies, provides security and enhanced user convenience by creating a strong digital identity. Biometrics ensure reliable data recording when users sign up for the service, and strong authentication of individual vehicle users.</p> <p>Idemia brings different technologies to the concept,</p> <ul>
<li>The e-KYC (know your customer) system is a digital user enrolment pathway that creates an identity</li>
<li>A digital car key solution allows the end-user to use a digital key stored in their smartphone to open and start the vehicle. This solution is cloud-based to securely send the digital key to the smartphone, with a secured smartphone environment to store the digital car key and a facial recognition to enable the digital car key</li>
<li>The DMS (driving monitoring system) is an on-board camera system that monitors driver behavior and interacts with the driver to return control of the vehicle to the human driver in the event of unforeseen circumstances.</li>
</ul> <p>Idemia teamed up with Altran to design the vehicle and developed several applications, including:</p> <ul>
<li>Scalable and innovative vehicle platform using more than 20 sensors embedded</li>
<li>Plug and play Electric Electronic architecture enabling autonomous mobility systems</li>
<li>System of light interactions for the vehicle, front &amp; rear, with light bands and LED screens</li>
<li>Intelligent dashboard to interact with the vehicle and services</li>
<li>Mobile application called “I-ris” enabling user to command the vehicle, send a package and manage shared transport, thanks to Idemia facial recognition system, providing secure access to the vehicle</li>
</ul> <p>&#8220;Idemia is proud to be presenting its cutting edge technologies as part of the Colombia project demonstration at the Paris Motor Show. Idemia&#8217;s experience and knowhow guarantee the unique nature of client identities (for both businesses and private individuals) and the reliability of their digital identity. Our priority is guaranteeing convenience and security for users,&#8221; explains Yves Portalier, Executive Vice-President for Connected Objects at Idemia.</p> <p>&#8220;Altran is proud to be working closely with Idemia and contribute significantly to the evolution of mobility and resource-sharing with this two-in-one service. This new service allows us to meet the needs of both end-users and B2B players in one combined solution,&#8221;<i> </i>explains Pascal Brier, Executive Vice-President for Strategy, Technology &amp; Innovation at Altran.</p> <p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/idemia-altran-present-new-assisted-driving-car-concept-paris/">Idemia and Altran present a new Assisted Driving Car Concept in Paris</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/idemia-altran-present-new-assisted-driving-car-concept-paris/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1318</wp:post_id>
		<wp:post_date><![CDATA[2018-10-03 09:33:42]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-03 09:33:42]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[idemia-and-altran-present-a-new-assisted-driving-car-concept-in-paris]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/idemia-altran-present-new-assisted-driving-car-concept-paris/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Opel demonstrates highly Automated Driving as a part of the research project Ko-HAF</title>
		<link>https://fifthlevel.ai/archives/1319</link>
		<pubDate>Tue, 02 Oct 2018 11:52:07 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14220</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Opel has successfully demonstrated a future that promises fewer traffic accidents and freedom for the driver to read, write or relax instead of continuously monitoring the car.</p> <p>The automaker is part of Ko-HAF<em> – </em>“Kooperatives hochautomatisiertes Fahren” – a German project researching cooperative highly automated driving, which began in June 2015. The Opel Insignia prototype displayed the functions of cooperative highly automated driving during the presentation of Ko-HAF’s final results at the Opel Test Center Rodgau-Dudenhofen.</p> <p>Cooperative highly automated driving systems do not need to be supervised by the driver all the time. Drivers can perform other tasks, but when prompted by the system they must be able to take control of the vehicle within a certain time period. The vehicle must therefore be able to “see” further ahead than possible with its own sensors.</p> <p>In Ko-HAF, vehicles send information about their current road environment, such as construction sites, traffic jams and accidents, to a Safety Server. The information is collected and processed by the Safety Server, so that a precise map is available to vehicles when they request it – like an artificial horizon that delivers a highly detailed preview of the road ahead.</p> <p>Opel’s role in Ko-HAF focused on the computerised maps and the process of disengaging the car from the automated driving condition, thus returning control to the driver. The engineers from Rüsselsheim designed the architecture, interfaces and protocols of the Safety Server, which were evaluated in the project.</p> <p>An additional core-task was the development of a self-localisation method for the vehicle. Opel designed algorithms for visual mapping and localization which are merged with information from back-end and onboard maps, onboard sensors and the Global Navigation Satellite System (GNSS). The localisation method was validated on the Opel Insignia test car at the Opel Test Centre and on the Ko-HAF test route on the motorways around Frankfurt am Main.</p> <p>Opel’s second area of focus concerned the driver’s actions, so-called non-driving tasks. The company’s engineers developed software and a system of sensors to detect and classify the driver’s actions while the car is driving automatically.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/opel-demonstrates-highly-automated-driving-part-research-project-ko-haf/">Opel demonstrates highly Automated Driving as a part of the research project Ko-HAF</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/opel-demonstrates-highly-automated-driving-part-research-project-ko-haf/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1319</wp:post_id>
		<wp:post_date><![CDATA[2018-10-02 11:52:07]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-02 11:52:07]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[opel-demonstrates-highly-automated-driving-as-a-part-of-the-research-project-ko-haf]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/opel-demonstrates-highly-automated-driving-part-research-project-ko-haf/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Pioneer starts shipping MEMS Mirror-Type 3D-LiDAR sensors</title>
		<link>https://fifthlevel.ai/archives/1320</link>
		<pubDate>Tue, 02 Oct 2018 09:19:46 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14217</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Pioneer Corporation (Pioneer) announced that from late September, the company will sequentially start shipping three types and four models of 3D-LiDAR sensors, which adopt the MEMS mirror method and differ in measurement distance.</p> <p>The 3D-LiDAR sensor uses laser beams to measure distances to objects accurately and grasps information on distances and surroundings in real time and in three dimensions. It is regarded as an essential device for vehicle use for level 3 and higher autonomous driving. Pioneer is developing a high-performance, downsizing, lower price 3D-LiDAR using a MEMS mirror, aiming for mass production in the 2020s. Pioneer provided its first sample for testing September 2017 to companies in Japan and overseas.</p> <p>The 3D-LiDAR sensors that will start shipping adopt the raster scanning method using MEMS mirrors without a motor drive unit. There are three types of products with different measurement distances: “Telescopic LiDAR”, “Medium-range LiDAR” and “Short-range LiDAR”. In addition, “Medium-range LiDAR” offers a dual type model, which combines two units of 3D-LiDAR sensors and supports wider measurement. A combination of these three types is possible according to different usages, allowing operation verification through installation in an actual use environment, such as a vehicle.</p> <p>Pioneer will provide these models to customers related to the automotive industry, ICT and a broad range of fields, and study specifications for a diverse range of needs. Using these models, Pioneer will develop a more accurate object recognition algorithm and vehicle position estimation algorithm. It will aim for use on Level 4 autonomous vehicles, which are expected to be realized soon in restricted areas, and for Level 3 autonomous vehicles on general roads, as well as for utilisation with next-generation GIS services.</p> <p>3D-LiDAR sensors under development adopt MEMS mirrors without a motor drive unit, which is expected to be small-sized, light weight and with high durability. Pioneer aims to reduce costs at the time of mass production with a flexible system configuration utilising generic components, and enhances the measurement capability of black objects and distant objects with original digital signal processing technology and optimised algorithms.</p> <p>In response to diverse customer needs, Pioneer is developing the wobbling scanning method of “Wide-view LiDAR” in addition to three types with a raster scanning method.</p> <p>Pioneer is developing 3D-LiDAR sensors as well as a map for autonomous driving. Utilizing them, it will also develop/propose an efficient maintenance/operation system (“data ecosystem”) which automatically collects surrounding information from passenger vehicles and then updates and distributes the map data for autonomous driving.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/pioneer-starts-shipping-mems-mirror-type-3d-lidar-sensors/">Pioneer starts shipping MEMS Mirror-Type 3D-LiDAR sensors</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/pioneer-starts-shipping-mems-mirror-type-3d-lidar-sensors/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1320</wp:post_id>
		<wp:post_date><![CDATA[2018-10-02 09:19:46]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-02 09:19:46]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[pioneer-starts-shipping-mems-mirror-type-3d-lidar-sensors]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/pioneer-starts-shipping-mems-mirror-type-3d-lidar-sensors/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Ford&#039;s Self-Driving Cars Try Telling The World What They&#039;re Up To</title>
		<link>https://fifthlevel.ai/archives/1354</link>
		<pubDate>Tue, 02 Oct 2018 23:55:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5bb3e91164aaf93e2ca3dee9</guid>
		<description></description>
		<content:encoded><![CDATA[Ford's experiments in communicating intent between self-driving cars and the outside world have the potential to coalesce into a standard for vehicular communication. <p><b><a href="https://www.forbes.com/sites/davidsilver/2018/10/02/ford-experiments-with-communicating-self-driving-intent/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1354</wp:post_id>
		<wp:post_date><![CDATA[2018-10-02 23:55:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-02 23:55:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[fords-self-driving-cars-try-telling-the-world-what-theyre-up-to]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/10/02/ford-experiments-with-communicating-self-driving-intent/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Volkswagen chooses Microsoft Azure platform as the foundation for Automotive Cloud</title>
		<link>https://fifthlevel.ai/archives/2446</link>
		<pubDate>Mon, 01 Oct 2018 11:52:15 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14208</guid>
		<description></description>
		<content:encoded><![CDATA[<div class="page-row is-nth-3">
<div class="container">
<div class="row">
<div id="item-2f156f4c-8c5c-478b-bd1c-3ca0fe1e6923" class="page-item--column page-item--last-item-in-row col-sm-6">
<div class="custom-padding page-row--layout-block">
<div class="page-row is-nth-1">
<div id="item-a5e61776-d990-4480-a513-5dcd1a8d19cd" class="page-item--intro">
<div class="intro-text">
<p>Entering into a strategic partnership, Volkswagen and Microsoft Corp. will collaborate to develop the ‘Volkswagen Automotive Cloud’, one of the largest dedicated automotive industry clouds for all future Volkswagen digital services and mobility offerings.</p> <p>The Supervisory Board of Volkswagen AG approved the conclusion of an agreement to this effect between Volkswagen and Microsoft on September 28, 2018. Volkswagen therefore continues to forge ahead with its digital transformation at full speed. With Microsoft as its strategic partner, the company is taking a decisive step in its digital transformation into a mobility provider with a fully connected vehicle fleet and the digital ecosystem “Volkswagen We”.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="page-row-spacer"></div>
<div class="page-row is-nth-4">
<div class="container">
<div id="item-c28a5875-b76e-4496-afb0-8a53b869edb6" class="page-item--text">
<div class="style-links-for-body-text">
<p>“The strategic partnership with Microsoft will turbocharge our digital transformation,” said Dr. Herbert Diess, CEO of Volkswagen AG. “Volkswagen, as one of the world’s largest automakers, and Microsoft, with its unique technological expertise, are outstandingly well-matched. Together, we will play a key role in shaping the future of auto-mobility.”</p> <p>“Volkswagen is harnessing technology to digitally transform and deliver innovative new connected car services to its customers,” said Satya Nadella, CEO of Microsoft. “The world’s leading companies run on Azure, and we are thrilled that Volkswagen has chosen Microsoft. Together we will reimagine the driving experience for people everywhere.”</p> <p>From 2020 onwards, more than 5 million new Volkswagen brand vehicles per year will be fully connected and will be part of the Internet of Things (IoT) in the cloud. The profound partnership between the two companies will lay the foundation for combining the global cloud expertise of Microsoft with the experience of Volkswagen as an automaker with a global market presence.</p> <p>Together, the two companies will develop the technological basis for a comprehensive industrial automotive cloud. In the future, all in-car services for vehicles of the core Volkswagen brand as well as the Group-wide cloud-based platform (also known as One Digital Platform, ODP) will be built on Microsoft’s Azure cloud platform and services as well as Azure IoT Edge. This will dramatically streamline the technical landscape.</p> <p>Via the Volkswagen Automotive Cloud, Volkswagen will considerably optimise the interconnection of vehicle, cloud-based platform and customer-centric services for all brands, such as the “Volkswagen We” ecosystem.</p> <p>By building the Volkswagen Automotive Cloud, Volkswagen will be able to leverage consistent mobility services across its entire portfolio and to provide new services and solutions such as in-car consumer experiences, telematics, and securely connect data between the car and the cloud.</p> <p>As part of the new entity, Volkswagen will establish a new automotive cloud development office in North America near Microsoft’s headquarters.</p> <p>To help usher in a new wave of automotive transformation, Microsoft will provide hands-on support to Volkswagen as it ramps up its new automotive cloud development office, including resources to help drive hiring, human resources management and consulting services. The workforce is expected to grow to about 300 engineers in the near future.</p> <p>Beyond the technological rationale of the partnership, Microsoft provides access to cloud expertise across their organisation so Volkswagen developers and engineers can benefit and learn from Microsoft’s strong culture of collaboration and agility and can transfer those experiences into the core Volkswagen organisation.</p> <p>In the long term, the solutions developed through the strategic partnership will be rolled out to other Volkswagen Group brands in all regions of the world, building the foundation for all customer-centric services of the brands. This includes the Volkswagen ID. electric family as well as conventionally-powered models. In the future, Volkswagen’s fleet of cars will become mobile ‘internet of things’ hubs linked by Microsoft Azure.</p>
</div>
</div>
</div>
</div>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/connected-car/volkswagen-chooses-microsoft-azure-platform-foundation-automotive-cloud/">Volkswagen chooses Microsoft Azure platform as the foundation for Automotive Cloud</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/connected-car/volkswagen-chooses-microsoft-azure-platform-foundation-automotive-cloud/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2446</wp:post_id>
		<wp:post_date><![CDATA[2018-10-01 11:52:15]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-01 11:52:15]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[volkswagen-chooses-microsoft-azure-platform-as-the-foundation-for-automotive-cloud]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/connected-car/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/connected-car/volkswagen-chooses-microsoft-azure-platform-foundation-automotive-cloud/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Riding a Self-Driving Shuttle in Edmonton</title>
		<link>https://fifthlevel.ai/archives/2503</link>
		<pubDate>Tue, 02 Oct 2018 20:28:42 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/cccb098f5a84</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*AuzTo6M0fnll96KWAQgriQ.jpeg" /></figure><p>I love seeing a new self-driving vehicle hit the streets that the general public can ride. So I am excited to read about <a href="https://www.ridewithela.ca/">ELA</a>, a shuttle powered by <a href="http://www.easymile.com/">EasyMile</a>.</p><p>Ela is testing this month in the great Canadian north — Edmonton and Calgary, Aberta. It’s a limited time trial, but the good news is that anybody can sign up.</p><p>All you have to do is get to Edmonton. It’s a hike, but they have an awesome mall there.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FXKA_Ijbodq4%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DXKA_Ijbodq4&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FXKA_Ijbodq4%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/45aa764a0b4679d203ca49ddae53b9a0/href">https://medium.com/media/45aa764a0b4679d203ca49ddae53b9a0/href</a></iframe><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cccb098f5a84" width="1" height="1"><hr><p><a href="https://medium.com/self-driving-cars/riding-a-self-driving-shuttle-in-edmonton-cccb098f5a84">Riding a Self-Driving Shuttle in Edmonton</a> was originally published in <a href="https://medium.com/self-driving-cars">Self-Driving Cars</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><b><a href="https://medium.com/self-driving-cars/riding-a-self-driving-shuttle-in-edmonton-cccb098f5a84?source=rss----bdc6e635b3c0---4" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2503</wp:post_id>
		<wp:post_date><![CDATA[2018-10-02 20:28:42]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-02 20:28:42]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[riding-a-self-driving-shuttle-in-edmonton]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driving-cars]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driving-cars/riding-a-self-driving-shuttle-in-edmonton-cccb098f5a84?source=rss----bdc6e635b3c0---4]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Full Self-Driving Teslas</title>
		<link>https://fifthlevel.ai/archives/2504</link>
		<pubDate>Tue, 02 Oct 2018 01:09:17 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/434198b0c517</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1001/1*iLjMNXMYNiSHPraYHBpnrw.jpeg" /></figure><blockquote>“Musk wrote in an email obtained by Bloomberg News that Tesla needed about 100 more employees to join an internal testing program linked to rolling out the full self-driving capability. Any worker who buys a Tesla and agrees to share 300 to 400 hours of driving feedback with the company’s Autopilot team by the end of next year won’t have to pay for full self-driving — an $8,000 saving — or for a premium interior, which normally costs $5,000, Musk wrote.”</blockquote><p><a href="https://www.bloomberg.com/news/articles/2018-09-28/tesla-enlists-employees-to-be-full-self-driving-beta-testers">Full story here.</a></p><p>This is so exciting!</p><p>On the other hand, as <em>CleanTechnica </em>reminds, Tesla has struggled to fulfill Autopilot promises in the past. So take with a grain of salt.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=434198b0c517" width="1" height="1"><hr><p><a href="https://medium.com/self-driving-cars/full-self-driving-teslas-434198b0c517">Full Self-Driving Teslas</a> was originally published in <a href="https://medium.com/self-driving-cars">Self-Driving Cars</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><b><a href="https://medium.com/self-driving-cars/full-self-driving-teslas-434198b0c517?source=rss----bdc6e635b3c0---4" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2504</wp:post_id>
		<wp:post_date><![CDATA[2018-10-02 01:09:17]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-02 01:09:17]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[full-self-driving-teslas]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driving-cars]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driving-cars/full-self-driving-teslas-434198b0c517?source=rss----bdc6e635b3c0---4]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Optimizing advertising with the precise consumer journey through space</title>
		<link>https://fifthlevel.ai/archives/2548</link>
		<pubDate>Tue, 02 Oct 2018 15:04:29 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://360.here.com/optimizing-advertising-with-the-precise-consumer-journey-through-space</guid>
		<description></description>
		<content:encoded><![CDATA[<div class="hs-featured-image-wrapper"> <a href="https://360.here.com/optimizing-advertising-with-the-precise-consumer-journey-through-space" title="" class="hs-featured-image-link"> <img src="https://360.here.com/hubfs/HEREblog_placesfootprints_hero1.jpg?t=1539367720552" alt="HEREblog_placesfootprints_hero1" class="hs-featured-image" style="width:auto !important; max-width:50%; float:left; margin:0 15px 15px 0;"> </a> </div> <h3>A new level of precision is coming to the advertising market. We’ve built the tech that helps you understand where and when consumers are within a location.</h3> <p>When all of us began carrying GPS &amp; WiFi enabled devices at all times of the day, naturally, the advertising industry foresaw a huge opportunity.</p> <img src="https://track.hubspot.com/__ptq.gif?a=2174253&amp;k=14&amp;r=https%3A%2F%2F360.here.com%2Foptimizing-advertising-with-the-precise-consumer-journey-through-space&amp;bu=https%253A%252F%252F360.here.com&amp;bvt=rss" alt="" width="1" height="1" style="min-height:1px!important;width:1px!important;border-width:0!important;margin-top:0!important;margin-bottom:0!important;margin-right:0!important;margin-left:0!important;padding-top:0!important;padding-bottom:0!important;padding-right:0!important;padding-left:0!important; "> <p><b><a href="https://360.here.com/optimizing-advertising-with-the-precise-consumer-journey-through-space" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2548</wp:post_id>
		<wp:post_date><![CDATA[2018-10-02 15:04:29]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-02 15:04:29]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[optimizing-advertising-with-the-precise-consumer-journey-through-space]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://360.here.com/rss.xml]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://360.here.com/optimizing-advertising-with-the-precise-consumer-journey-through-space]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Model helps robots navigate more like humans do</title>
		<link>https://fifthlevel.ai/archives/3568</link>
		<pubDate>Thu, 04 Oct 2018 04:00:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://news.mit.edu/2018/model-helps-robots-navigate-like-humans-1004</guid>
		<description></description>
		<content:encoded><![CDATA[<p>When moving through a crowd to reach some end goal, humans can usually navigate the space safely without thinking too much. They can learn from the behavior of others and note any obstacles to avoid. Robots, on the other hand, struggle with such navigational concepts.</p> <p>MIT researchers have now devised a way to help robots navigate environments more like humans do. Their novel motion-planning model lets robots determine how to reach a goal by exploring the environment, observing other agents, and exploiting what they’ve learned before in similar situations. A paper describing the model was presented at this week’s IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).</p> <p>Popular motion-planning algorithms will create a tree of possible decisions that branches out until it finds good paths for navigation. A robot that needs to navigate a room to reach a door, for instance, will create a step-by-step search tree of possible movements and then execute the best path to the door, considering various constraints. One drawback, however, is these algorithms rarely learn: Robots can’t leverage information about how they or other agents acted previously in similar environments.</p> <p>“Just like when playing chess, these decisions branch out until [the robots] find a good way to navigate. But unlike chess players, [the robots] explore what the future looks like without learning much about their environment and other agents,” says co-author Andrei Barbu, a researcher at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Center for Brains, Minds, and Machines (CBMM) within MIT’s McGovern Institute. “The thousandth time they go through the same crowd is as complicated as the first time. They’re always exploring, rarely observing, and never using what’s happened in the past.”</p> <p>The researchers developed a model that combines a planning algorithm with a neural network that learns to recognize paths that could lead to the best outcome, and uses that knowledge to guide the robot’s movement in an environment.</p> <p>In their paper, “<a href="http://arxiv.org/abs/1810.00804" target="_blank">Deep sequential models for sampling-based planning</a>,” the researchers demonstrate the advantages of their model in two settings: navigating through challenging rooms with traps and narrow passages, and navigating areas while avoiding collisions with other agents. A promising real-world application is helping autonomous cars navigate intersections, where they have to quickly evaluate what others will do before merging into traffic. The researchers are currently pursuing such applications through the Toyota-CSAIL Joint Research Center.</p> <p>“When humans interact with the world, we see an object we’ve interacted with before, or are in some location we’ve been to before, so we know how we’re going to act,” says Yen-Ling Kuo, a PhD student in CSAIL and first author on the paper. “The idea behind this work is to add to the search space a machine-learning model that knows from past experience how to make planning more efficient.”</p> <p>Boris Katz, a principal research scientist and head of the InfoLab Group at CSAIL, is also a co-author on the paper.</p> <p><strong>Trading off exploration and exploitation</strong></p> <p>Traditional motion planners explore an environment by rapidly expanding a tree of decisions that eventually blankets an entire space. The robot then looks at the tree to find a way to reach the goal,&nbsp;such as a door. The researchers’ model, however, offers “a tradeoff between exploring the world and exploiting past knowledge,” Kuo says.</p> <p>The learning process starts with a few examples. A robot using the model is trained on a few ways to navigate similar environments. The neural network learns what makes these examples succeed by interpreting the environment around the robot, such as the shape of the walls, the actions of other agents, and features of the goals. In short, the model “learns that when you’re stuck in an environment, and you see a doorway, it’s probably a good idea to go through the door to get out,” Barbu says.</p> <p>The model combines the exploration behavior from earlier methods with this learned information. The underlying planner, called RRT*, was developed by MIT professors Sertac Karaman and Emilio Frazzoli. (It’s a variant of a widely used motion-planning algorithm known as Rapidly-exploring Random Trees, or&nbsp; RRT.) The planner creates a search tree while the neural network mirrors each step and makes probabilistic predictions about where the robot should go next. When the network makes a prediction with high confidence, based on learned information, it guides the robot on a new path. If the network doesn’t have high confidence, it lets the robot explore the environment instead, like a traditional planner.</p> <p>For example, the researchers demonstrated the model in a simulation known as a “bug trap,” where a 2-D robot must escape from an inner chamber through a central narrow channel and reach a location in a surrounding larger room. Blind allies on either side of the channel can get robots stuck. In this simulation, the robot was trained on a few examples of how to escape different bug traps. When faced with a new trap, it recognizes features of the trap, escapes, and continues to search for its goal in the larger room. The neural network helps the robot find the exit to the trap, identify the dead ends, and gives the robot a sense of its surroundings so it can quickly find the goal.</p> <p>Results in the paper are based on the chances that a path is found after some time, total length of the path that reached a given goal, and how consistent the paths were. In both simulations, the researchers’ model more quickly plotted far shorter and consistent paths than a traditional planner.</p> <p>“This model is interesting because it allows a motion planner to adapt&nbsp;to what it sees in the environment,” says Stephanie Tellex, an assistant professor of computer science at Brown University, who was not involved in the research. “This can enable dramatic&nbsp;improvements in planning speed by customizing the planner to what the robot knows. Most planners don't adapt to the environment at all. Being able to&nbsp;traverse long, narrow passages is notoriously difficult for a&nbsp;conventional planner, but they can solve it. We need more ways that&nbsp;bridge this gap.”</p> <p><strong>Working with multiple agents</strong></p> <p>In one other experiment, the researchers trained and tested the model in navigating environments with multiple moving agents, which is a useful test for autonomous cars, especially navigating intersections and roundabouts. In the simulation, several agents are circling an obstacle. A robot agent must successfully navigate around the other agents, avoid collisions, and reach a goal location, such as an exit on a roundabout.</p> <p>“Situations like roundabouts are hard, because they require reasoning about how others will respond to your actions, how you will then respond to theirs, what they will do next, and so on,” Barbu says. “You eventually discover your first action was wrong, because later on it will lead to a likely accident. This problem gets exponentially worse the more cars you have to contend with.”</p> <p>Results indicate that the researchers’ model can capture enough information about the future behavior of the other agents (cars) to cut off the process early, while still making good decisions in navigation. This makes planning more efficient. Moreover, they only needed to train the model on a few examples of roundabouts with only a few cars. “The plans the robots make take into account what the other cars are going to do, as any human would,” Barbu says.</p> <p>Going through intersections or roundabouts is one of the most challenging scenarios facing autonomous cars. This work might one day let cars learn how humans behave and how to adapt to drivers in different environments, according to the researchers. This is the focus of the Toyota-CSAIL Joint Research Center work.</p> <p>“Not everybody behaves the same way, but people are very stereotypical. There are people who are shy, people who are aggressive. The model recognizes that quickly and that’s why it can plan efficiently,” Barbu says.</p> <p>More recently, the researchers have been applying this work to robots with manipulators that face similarly daunting challenges when reaching for objects in ever-changing environments.</p>
<p><b><a href="http://news.mit.edu/2018/model-helps-robots-navigate-like-humans-1004" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3568</wp:post_id>
		<wp:post_date><![CDATA[2018-10-04 04:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-04 04:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[model-helps-robots-navigate-more-like-humans-do]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/rss/topic/autonomous-vehicles]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://news.mit.edu/2018/model-helps-robots-navigate-like-humans-1004]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Toyota and SoftBank partner to develop self-driving car services</title>
		<link>https://fifthlevel.ai/archives/481</link>
		<pubDate>Thu, 04 Oct 2018 08:33:35 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2401439</guid>
		<description></description>
		<content:encoded><![CDATA[<img width="578" height="330" src="https://venturebeat.com/wp-content/uploads/2018/10/s2.reutersmedia.net_.jpg?fit=578%2C330&amp;strip=all" class="attachment-single-feed size-single-feed wp-post-image" alt="Toyota President Akio Toyoda and Executive Vice President Shigeki Tomoyama pose for a photograph with SoftBank Chairman and CEO Masayoshi Son and SoftBank Representative Director and CTO Junichi Miyakawa during joint news conference in Tokyo." /><hr />(Reuters) &#8212; Toyota and SoftBank said they will team up to develop car services that rely on self-driving technology, such as hospital shuttles, as they envision a future in which fewer people drive their own vehicles. The partnership between Japan’s top automaker and its most influential tech giant shows that even big well-funded players fear&hellip;<a href="https://venturebeat.com/2018/10/04/toyota-and-softbank-partner-to-develop-self-driving-car-services/" target="_blank">Read More</a> <p><b><a href="https://venturebeat.com/2018/10/04/toyota-and-softbank-partner-to-develop-self-driving-car-services/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>481</wp:post_id>
		<wp:post_date><![CDATA[2018-10-04 08:33:35]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-04 08:33:35]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[toyota-and-softbank-partner-to-develop-self-driving-car-services]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/10/04/toyota-and-softbank-partner-to-develop-self-driving-car-services/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[978]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>U.S. will revise road safety rules for fully self-driving cars</title>
		<link>https://fifthlevel.ai/archives/489</link>
		<pubDate>Thu, 04 Oct 2018 17:20:25 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2401611</guid>
		<description></description>
		<content:encoded><![CDATA[<img width="578" height="325" src="https://venturebeat.com/wp-content/uploads/2018/07/DSC00559.jpg?fit=578%2C325&amp;strip=all" class="attachment-single-feed size-single-feed wp-post-image" alt="Waymo Jaguar I-Paces" /><hr />(Reuters) &#8212; The National Highway Traffic Safety Administration (NHTSA) is moving ahead with plans to revise safety rules that bar fully self-driving cars from the roads without equipment like steering wheels, pedals, and mirrors, according to a document seen by Reuters. The auto safety agency, known as NHTSA, &#8220;intends to reconsider the&hellip;<a href="https://venturebeat.com/2018/10/04/u-s-will-revise-road-safety-rules-for-fully-self-driving-cars/" target="_blank">Read More</a> <p><b><a href="https://venturebeat.com/2018/10/04/u-s-will-revise-road-safety-rules-for-fully-self-driving-cars/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>489</wp:post_id>
		<wp:post_date><![CDATA[2018-10-04 17:20:25]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-04 17:20:25]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[u-s-will-revise-road-safety-rules-for-fully-self-driving-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/10/04/u-s-will-revise-road-safety-rules-for-fully-self-driving-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[979]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Introducing Software Version 9.0</title>
		<link>https://fifthlevel.ai/archives/1253</link>
		<pubDate>Fri, 05 Oct 2018 20:37:40 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://fifthlevel.ai/?p=1253</guid>
		<description></description>
		<content:encoded><![CDATA[<div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item even" property="content:encoded"><p>This week, Tesla owners across North America are waking up to a car that is smarter, safer and more intuitive than ever before. Our most substantial update yet, Software Version 9.0, introduces a refined and simplified user interface, along with entirely new features for Model S, Model X, and Model 3, as well as on Tesla’s mobile app.</p>
</div></div></div><div class="form-item form-type-item"> <label class="form-label">Language </label> Undefined
</div> <p><b><a href="https://www.tesla.com/blog/introducing-software-version-9" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1253</wp:post_id>
		<wp:post_date><![CDATA[2018-10-05 20:37:40]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-05 20:37:40]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[introducing-software-version-9-0]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.tesla.com/blog/feed]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.tesla.com/blog/introducing-software-version-9]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Tesla Q3 2018 Vehicle Safety Report</title>
		<link>https://fifthlevel.ai/archives/1254</link>
		<pubDate>Thu, 04 Oct 2018 17:36:41 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://fifthlevel.ai/?p=1254</guid>
		<description></description>
		<content:encoded><![CDATA[<div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item even" property="content:encoded"><p>At Tesla, the safety of our customers is our top priority, which is why it’s critical that we design and build the safest cars in the world. Not only do we conduct extensive in-house testing and simulation to ensure our vehicles achieve top safety performance before they ever reach the road, we are also uniquely positioned to leverage the hundreds of thousands of miles of real-world data our fleet collects every month to continuously improve our vehicles and develop a more complete picture of safety over time.</p></div></div></div><div class="form-item form-type-item"> <label class="form-label">Language </label> Undefined
</div> <p><b><a href="https://www.tesla.com/blog/q3-2018-vehicle-safety-report" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1254</wp:post_id>
		<wp:post_date><![CDATA[2018-10-04 17:36:41]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-04 17:36:41]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[q3-2018-vehicle-safety-report]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.tesla.com/blog/feed]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.tesla.com/blog/q3-2018-vehicle-safety-report]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>SOS LAB secures $6M in Series A Funding</title>
		<link>https://fifthlevel.ai/archives/1314</link>
		<pubDate>Fri, 05 Oct 2018 13:24:21 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14275</guid>
		<description></description>
		<content:encoded><![CDATA[<p>SOS LAB, a LiDAR development company focusing on innovative scanning solutions for autonomous vehicles, recently announced the completion of their Series A funding, securing over $6M from several major investment firms in South Korea.</p> <p>SOS LAB’s Series A strategic investor is Mando, a top-tier automotive supplier with a well-established global network in the field of mass-producing automotive parts. Other investors in this round include KDB Capital, Mirae Asset, Hancom Investment, Hyundai Investment Partners, SL Investment, WM Investment, BA Partners, RyukyungPSG Asset Management and Future Play.</p> <p>Through this round of funding, SOS LAB has become the first startup company to ever receive an investment by Hancom Investment Corporation, who recently formed a joint investment association with Mirae Asset to focus on full-scale investment in industry 4.0.</p> <p>&#8220;Through our Series A funding, we have created an opportunity to focus on R&amp;D technology and make an all-out effort to attract sales targets and global investors for the next year,&#8221; said Jiseong Jeong, CEO of SOS LAB.</p> <p>Following this successful round of funding, SOS LAB is strengthening its position in global markets beyond South Korea. At the recent AutoSens Conference in Belgium, CEO Jeong hosted a roundtable discussion and presented on a panel showcasing how SOS LAB’s products distinguish the company from other major players in the field. SOS LAB also took first place at the KIC China &amp; Beijing University Incubator Program, raising the possibility of business advancement in China, and was chosen as one of six final teams at the 2018 &#8216;IP Startup Demo-Day’ in Korea, receiving funding for overseas applications and design development.</p> <p>SOS LAB is also actively participating in global conferences, heavily focusing on finding global investors and partners in the United States, China, Japan, Hong Kong, and Finland.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/sos-lab-secures-6m-series-funding/">SOS LAB secures $6M in Series A Funding</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/sos-lab-secures-6m-series-funding/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1314</wp:post_id>
		<wp:post_date><![CDATA[2018-10-05 13:24:21]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-05 13:24:21]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[sos-lab-secures-6m-in-series-a-funding]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/sos-lab-secures-6m-series-funding/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>NXP launches new automotive radar solution</title>
		<link>https://fifthlevel.ai/archives/1315</link>
		<pubDate>Thu, 04 Oct 2018 11:59:22 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14260</guid>
		<description></description>
		<content:encoded><![CDATA[<p>NXP Semiconductors N.V. an automotive radar manufacturer, has expanded its radar ecosystem with an automotive radar solution that combines its S32R processors, RF Transceiver and Antenna design on a new reference platform.</p> <p>Created through a partnership with Colorado Engineering, the solution is the only currently available automotive-grade radar development platform designed to meet the stringent functional, performance, and safety requirements of the industry. This new radar solution is designed to accelerate the development and deployment of radar into production vehicles and includes a full ecosystem of tools that aim to lower development costs and spur radar application adoption worldwide.</p> <p>Current automotive market analysis projects that by 2020, radar technology will be in 50% of all newly produced cars. The safety related benefits of automotive radar, new autonomous vehicle development requirements and emerging safety requirements from organizations such as the New Car Assessment Program (NCAP), have triggered rapid growth with steep implementation challenges for car makers and other radar focused developers. Adoption of these once premium safety features into mainstream production lines is fueling a need for faster time to market</p> <p>The NXP radar solution, featuring the S32R27 processor, the TEF810x CMOS transceiver, and the FS8410 Power management IC, is designed to help customers accelerate time to market by lowering the barriers of entry to radar application development with hardware, software and tools that ease radar implementations.</p> <p>Built in collaboration with Colorado Engineering, the new RDK-S32R274 radar solution is targeted to help developers rapidly prototype high-performance automotive radar using NXP technology. The open and flexible development platform uses a modular architecture and includes a NXP S32R processor and a NXP transceiver along with an innovative radar software development kit. Expansion and antenna modules can be optimised to create a customised development platform suited to specific customer application requirements.</p> <p>“NXP’s market leading portfolio of radar processors and our end-to-end system development and integration expertise creates a unique solution that accelerates time-to-market for customers,” said Dr. Larry Scally, CEO of Colorado Engineering Inc. “By leveraging the highly-optimised radar accelerators that are integrated in the S32R27, customers are well positioned to implement extremely complex automotive radar applications.”</p> <p>As a technology and a market leader in radar processing, NXP’s highly integrated radar processors offer customers a scalable family of products including the previously announced S32R27 and S32R37. These devices offer 10x performance/watt improvement over a traditional DSP by integrating highly-efficient radar accelerators. This enables longer range, higher resolution and accuracy for safety critical applications such as collision avoidance, lane change assist, autonomous emergency braking.</p> <p>The automotive grade Radar Software Development Kit provides customers an extensive radar algorithm library to quickly build and optimise applications without having to invest resources to hand tune accelerator software.  NXP’s extensive ecosystem of compilers, development environments, MCALS, and both free and commercial RTOS support provides customers the tools needed for faster development.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/nxp-launches-new-automotive-radar-solution/">NXP launches new automotive radar solution</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/nxp-launches-new-automotive-radar-solution/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1315</wp:post_id>
		<wp:post_date><![CDATA[2018-10-04 11:59:22]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-04 11:59:22]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[nxp-launches-new-automotive-radar-solution]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/nxp-launches-new-automotive-radar-solution/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>At GTC Munich: The Power of Virtualization Multiplied with Enhanced vGPU Solutions</title>
		<link>https://fifthlevel.ai/archives/1814</link>
		<pubDate>Tue, 09 Oct 2018 15:00:57 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=40770</guid>
		<description></description>
		<content:encoded><![CDATA[<h2>Multi-GPU performance with virtual workstations enables designers and engineers to bring their creations to life and speed up the design process.</h2>
<p>The world’s most powerful virtual workstation just upped the ante. The latest enhancements to the <a href="https://www.nvidia.com/en-us/design-visualization/quadro-vdws/">Quadro Virtual Data Center Workstation</a> (Quadro vDWS) deliver the highest performance of a virtual workstation to accelerate demanding graphics and compute workflows.</p>
<p>Quadro vDWS with multi-GPU performance enables professionals to work remotely on any device, while their designs and intellectual property are secured in the data center.</p>
<p>Creative professionals working on remote, virtual workstations can render compelling, photoreal visualizations 94 percent faster with the added power of two Tesla V100 Tensor Core GPUs instead of a single Tesla V100. Engineers and designers can complete simulations nearly 7x faster with two Tesla V100 GPUs compared to a CPU-only system.</p>
<p>The newest edition of <a href="https://www.nvidia.com/en-us/design-visualization/technologies/virtual-gpu/">NVIDIA Virtual GPU software</a> ensures reliability and ease of management with features like live migration. New capabilities in the NVIDIA vGPU October 2018 release (Quadro vDWS and GRID software) include:</p>
<ul>
<li><b>Run Multi-GPU Workloads</b> <b>with NVIDIA Quadro vDWS</b> — Experience monumental improvement in virtual GPU performance by aggregating the power of up to four NVIDIA Tesla GPUs in a single virtual machine (VM) for the most graphics- and compute-intensive rendering, simulation and design workflows. With this release, NVIDIA virtual GPU products now support aggregation of multiple Tesla GPUs and GPU sharing across multiple VMs. <a href="https://www.redhat.com/en/topics/virtualization">Red Hat</a> is the first virtualization platform to include this capability with its <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> 7.5 and Red Hat Virtualization 4.2 <a href="https://www.redhat.com/en/topics/virtualization/what-is-KVM">KVM</a> release.</li>
<li><b>Live Migration with VMware vMotion</b> — IT can migrate live, NVIDIA GPU-accelerated VMs without impacting users or requiring scheduled downtime, saving valuable time and resources. IT teams are freed up to focus on more strategic projects and drive business transformation. This new feature is now supported on the Quadro vDWS and GRID vPC and GRID vApps software products with VMware vMotion, vSphere 6.7 u1.</li>
<li><b>Support for NVIDIA </b><a href="https://www.nvidia.com/en-us/data-center/tesla-t4/"><b>Tesla T4 GPUs</b></a> — Get 2x the framebuffer in the same low-profile, single-slot form factor as the previous generation Tesla P4. When combined with multi-GPU support, the new 70W Tesla T4 enables ever more demanding workflows in a virtual desktop infrastructure environment, including advanced rendering, simulation and design.</li>
<li><b>AI workloads on VMs with </b><a href="https://www.nvidia.com/en-us/gpu-cloud/"><b>NVIDIA GPU Cloud</b></a> — NGC empowers AI researchers with GPU-accelerated deep learning containers for TensorFlow, PyTorch, MXNet, TensorRT and more. The ready-to-run deep learning containers from NGC are now tested with the latest release of Quadro vDWS. These pre-integrated, GPU-accelerated containers include NVIDIA CUDA Toolkit, NVIDIA deep learning libraries and an operating system.</li>
</ul>
<p><img class="size-medium wp-image-40773 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/Slide3-400x225.jpg" alt="" width="400" height="225" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/Slide3-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Slide3-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Slide3-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Slide3-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Slide3-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Slide3-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Slide3.jpg 1152w" sizes="(max-width: 400px) 100vw, 400px" /></p>
<p>&nbsp;</p>
<p><b>Broad Ecosystem Support from ISVs and OEMs</b></p>
<p><b>“</b>Simulation-driven product development takes engineering simulation to another level and the ability to scale multiple GPUs really kicks it up a notch. With the latest NVIDIA vGPU, our users can work faster, more smoothly and more securely from just about anywhere — and in some cases, for significantly less cost than a vCPU only solution.” — Sunil Sathe, lead software developer at ANSYS</p>
<p>“Inspiration can strike anywhere. Autodesk Maya users are now able to utilize up to four NVIDIA Tesla GPUs in a single virtual machine, so the workstation can create and render faster photorealistic scenes regardless of when or where our customers are.” — Chris Vienneau, senior director of Media &amp; Entertainment at Autodesk</p>
<p>“Cisco UCS and HyperFlex provide users with a proven, flexible and scalable platform for running GPU-accelerated virtual desktops and applications. The new NVIDIA capabilities for multiple GPUs support on a single VM will give customers a significantly enhanced experience for their GPU-enabled high-performance graphics applications, while support for VMware vGPU vMotion will insure seamless maintenance windows for the IT staff.” — Siva Sivakumar, senior director of UCS Solutions in the Cisco Data Center Group</p>
<p>“In the past, GPU virtualization was limited to developers on remote sites and automated testing, where a little planned downtime is acceptable. With vMotion technology support on NVIDIA virtual GPU, we can now expand virtualization to our production environments, where failure and downtime is not an option. Patching, load balancing and upgrades of hardware and software have become a breeze, and end-users don’t notice a thing.” — Ouissame Bekada, system, virtualization and storage engineer at Dassault Systèmes</p>
<p>“With the latest release of the NVIDIA vGPU software that supports up to four GPUs per virtual machine that can be accessed remotely, we are enabling our customers, no matter where they are, to deliver best-in-class performance for the most challenging workloads across simulation, modeling and design.” — Bill Mannel, vice president and general manager of HPC and AI at HPE</p>
<p>“Through our close collaboration with NVIDIA, Red Hat Virtualization is now the first virtualization platform to enable multi-GPU workflows in support of NVIDIA’s Quadro Virtual Data Center Workstation software. Now customers can use NVIDIA’s newest GPUs on a fully open, enterprise-ready virtual foundation built on the backbone of the world’s leading enterprise Linux platform in Red Hat Enterprise Linux.” — Joe Fernandes, vice president, Products, Cloud Platforms Business Unit, Red Hat</p>
<p>“The flexibility of the new multi-GPU feature available with the NVIDIA Quadro Virtual Data Center Workstation opens up powerful new rendering workflows to SOLIDWORKS Visualize users. The near-linear performance scaling means they can iterate on their designs at lightning speed on professional virtual workstations, allowing our customers to arrive at their best design in the shortest amount of time.” — Brian Hillner, product portfolio manager at SOLIDWORKS</p>
<p>Learn more about the latest innovations with NVIDIA vGPU software by registering for the “<a href="http://info.nvidia.com/vgpu-fall-2018-release-reg-page.html">What’s New with NVIDIA vGPU Solutions Fall 2018</a>” webinar.</p>
<p><b>Experience NVIDIA vGPU Solutions</b></p>
<p>The new capabilities were announced today at the<a href="https://www.nvidia.com/en-eu/gtc/"> GPU Technology Conference in Europe</a>, taking place Oct. 9-11 in Munich, Germany, where we’ll be talking about how NVIDIA vGPU October 2018 release helps deliver the agile data center. Come see us in Room 22 on Wednesday, Oct. 10 at 2:30 pm (<a href="https://2018gtceurope.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=250428">Session E8513</a>).</p>
<p>Stop by the NVIDIA booth and see how with multi-GPU support in a single VM users can bring up multiple apps that rely on compute and graphics in the VM, including SolidWorks Visualize to run a rendering workflow and ANSYS Discovery Live to run real-time, interactive simulation. The demo will be running Quadro vDWS software on two Tesla V100 GPUs.</p>
<p><b>Availability</b></p>
<p>Support for multi-GPU, VMware VMotion and NGC containers is expected in late fall. NVIDIA Tesla T4 support with NVIDIA vGPU software is expected by year-end.</p>
<p>Learn more about accelerating your digital workplace with NVIDIA virtual GPU products.</p>
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="https://blogs.nvidia.com/blog/2018/10/09/the-power-of-virtualization-multiplied-with-enhanced-vgpu-solutions/">At GTC Munich: The Power of Virtualization Multiplied with Enhanced vGPU Solutions</a> appeared first on <a rel="nofollow" href="https://blogs.nvidia.com">The Official NVIDIA Blog</a>.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/nvidiablog?a=m7Oww73ZWNE:fmpwYSeKNLE:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/nvidiablog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/nvidiablog?a=m7Oww73ZWNE:fmpwYSeKNLE:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/nvidiablog?i=m7Oww73ZWNE:fmpwYSeKNLE:V_sGLiPBpWU" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/nvidiablog/~4/m7Oww73ZWNE" height="1" width="1" alt=""/> <p><b><a href="http://feedproxy.google.com/~r/nvidiablog/~3/m7Oww73ZWNE/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1814</wp:post_id>
		<wp:post_date><![CDATA[2018-10-09 15:00:57]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-09 15:00:57]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[at-gtc-munich-the-power-of-virtualization-multiplied-with-enhanced-vgpu-solutions]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feeds.feedburner.com/nvidiablog]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feedproxy.google.com/~r/nvidiablog/~3/m7Oww73ZWNE/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Model 3 achieves the lowest probability of injury of any vehicle ever tested by NHTSA</title>
		<link>https://fifthlevel.ai/archives/1996</link>
		<pubDate>Mon, 08 Oct 2018 01:59:41 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://fifthlevel.ai/?p=1996</guid>
		<description></description>
		<content:encoded><![CDATA[<div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item even" property="content:encoded"><p>Based on the advanced architecture of Model S and Model X, which were previously found by the National Highway Traffic Safety Administration (NHTSA) to have the lowest and second lowest probabilities of injury of all cars ever tested, we engineered Model 3 to be the safest car ever built. Now, not only has Model 3 achieved a perfect 5-star safety rating in every category and sub-category, but NHTSA’s tests also show that it has the lowest probability of injury of all cars the safety agency has ever tested.</p></div></div></div><div class="form-item form-type-item"> <label class="form-label">Language </label> Undefined
</div> <p><b><a href="https://www.tesla.com/blog/model-3-lowest-probability-injury-any-vehicle-ever-tested-nhtsa" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1996</wp:post_id>
		<wp:post_date><![CDATA[2018-10-08 01:59:41]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-08 01:59:41]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[model-3-achieves-the-lowest-probability-of-injury-of-any-vehicle-ever-tested-by-nhtsa]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.tesla.com/blog/feed]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.tesla.com/blog/model-3-lowest-probability-injury-any-vehicle-ever-tested-nhtsa]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>What Hurdles Do Self-Driving Cars Face As Waymo Gets Ready For Prime Time?</title>
		<link>https://fifthlevel.ai/archives/2028</link>
		<pubDate>Fri, 05 Oct 2018 19:39:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://5bb6769b64aaf93e2ca3e649</guid>
		<description></description>
		<content:encoded><![CDATA[Several issues have emerged with self-driving car technology in the run-up to Waymo's public launch later this year. Here is a point-by-point analysis. <p><b><a href="https://www.forbes.com/sites/davidsilver/2018/10/05/what-hurdles-do-self-driving-cars-face-as-waymo-gets-ready-for-prime-time/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2028</wp:post_id>
		<wp:post_date><![CDATA[2018-10-05 19:39:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-05 19:39:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[what-hurdles-do-self-driving-cars-face-as-waymo-gets-ready-for-prime-time]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.forbes.com/sites/davidsilver/2018/10/05/what-hurdles-do-self-driving-cars-face-as-waymo-gets-ready-for-prime-time/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>NVIDIA Launches GPU-Acceleration Platform for Data Science, Volvo Selects NVIDIA DRIVE</title>
		<link>https://fifthlevel.ai/archives/2051</link>
		<pubDate>Wed, 10 Oct 2018 11:15:33 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=40784</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Big data is bigger than ever. Now, thanks to GPUs, it will be faster than ever, too.</p>
<p>NVIDIA founder and CEO Jensen Huang took the stage Wednesday in Munich to introduce <a href="https://nvidianews.nvidia.com/news/nvidia-introduces-rapids-open-source-gpu-acceleration-platform-for-large-scale-data-analytics-and-machine-learning">RAPIDS</a>, accelerating &#8220;big data, for big industry, for big companies, for deep learning,&#8221; Huang told a packed house of more than 3,000 developers and executives gathered for the three-day <a href="https://www.gputechconf.eu/">GPU Technology Conference in Europe</a>.</p>
<p>Already backed by Walmart, IBM, Oracle, Hewlett-Packard Enterprise and some two dozen other partners, the open-source GPU-acceleration platform promises 50x speedups on the <a href="https://www.nvidia.com/en-us/data-center/dgx-2/">NVIDIA DGX-2 AI supercomputer</a> compared with CPU-only systems, Huang said.</p>
<p>The result is an invaluable tool as companies in every industry look to harness big data for a competitive edge, Huang explained as he detailed how RAPIDS will turbo-charge the work of the world’s data scientists.</p>
<p>&#8220;We&#8217;re accelerating things by 1000x in the domains we focus on,&#8221; Huang said. &#8220;When we accelerate something 1000x in ten years, if your demand goes up 100 times your cost goes down by 10 times.&#8221;</p>
<p>Over the course of a keynote packed with news and demos, Huang detailed how NVIDIA is bringing that 1000x acceleration to bear on challenges ranging from autonomous vehicles to robotics to medicine.</p>
<p>Among the highlights: Volvo Cars had selected the <a href="https://www.nvidia.com/en-us/self-driving-cars/drive-platform/hardware/">NVIDIA DRIVE AGX Xavier computer</a> for its next generation of vehicles; King’s College London is adopting NVIDIA’s Clara medical platform; and startup Oxford Nanopore will use Xavier to build the world’s first handheld, low-cost, real-time DNA sequencer.</p>
<h2>Big Gains for GPU Computing</h2>
<p>Huang opened his talk by detailing the eye-popping numbers driving the adoption of accelerated computing — gains in computing power of 1,000x over the past 10 years.</p>
<p>“In ten years time, while Moore’s law has ended, our computing approach has resulted in a 1000x increase in computing performance.” Huang said. “It’s now recognized as the path forward.”</p>
<p>Huang also spoke about how NVIDIA’s new Turing architecture — launched in August — brings AI and computer graphics together.</p>
<p>Turing combines support for next-generation rasterization, real-time ray-tracing and AI to drive big performance gains in gaming with <a href="https://www.nvidia.com/en-us/geforce/20-series/">NVIDIA GeForce RTX GPUs</a>, visual effects with new <a href="https://www.nvidia.com/en-gb/design-visualization/quadro-desktop-gpus/">NVIDIA Quadro RTX pro graphics cards</a>, and hyperscale data centers with the new <a href="https://www.nvidia.com/en-us/data-center/tesla-t4/">NVIDIA Tesla T4 GPU</a>, the world’s first universal deep learning accelerator.</p>
<h2>One Small Step for Man&#8230;</h2>
<p>With a stunning demo, Huang showcased how our latest NVIDIA RTX GPUs — which enable real-time ray-tracing for the first time — allowed our team to digitally rebuild the scene around one of the lunar landing’s iconic photographs, that of astronaut Buzz Aldrin clambering down the lunar module’s lander.</p>
<p>The demonstration puts to rest the assertion that the photo can’t be real because Buzz Aldrin is lit too well as he climbs down to the surface of the moon while in the shadow of the lunar lander. Instead the simulation shows how the reflectivity of the surface of the moon accounts for exactly what’s seen in the controversial photo.</p>
<p>&#8220;This is the benefit of NVIDIA RTX, using this type of rendering technology we can simulate light physics and things are going to look the way things should look,” Huang said.</p>
<h2>&#8230;One Giant Leap for Data Science</h2>
<p>Bringing GPU computing back down to Earth, Huang announced a plan to accelerate the work of data scientists at the world’s largest enterprises.</p>
<p>RAPIDS open-source software gives data scientists facing complex challenges a giant performance boost. These challenges range from predicting credit card fraud to forecasting retail inventory and understanding customer buying behavior, Huang explained.</p>
<p>Analysts estimate the server market for data science and machine learning at $20 billion. Together with scientific analysis and deep learning, this pushes up the value of the high performance computing market to approximately $36 billion.</p>
<p>Developed over the past two years by NVIDIA engineers in close collaboration with key open-source contributors, RAPIDS offers a suite of open-source libraries for GPU-accelerated analytics, machine learning and, soon, data visualization.</p>
<p>RAPIDS has already won support from tech leaders such as Hewlett-Packard Enterprise, IBM and Oracle as well as open-source pioneers such as Databracks and Anaconda, Huang said.</p>
<p>&#8220;We have integrated RAPIDS into basically the world&#8217;s data science ecosystem, and companies big and small, their researchers can get into machine learning using RAPIDS and be able to accelerate it and do it quickly, and if they want to take it as a way to get into deep learning, they can do so,&#8221; Huang said.</p>
<h2>Bringing Data to Your Drive</h2>
<p>Huang also outlined the strides NVIDIA is making with automakers, announcing that Swedish automaker Volvo has selected the<a href="https://www.nvidia.com/en-us/self-driving-cars/drive-platform/hardware/"> NVIDIA DRIVE AGX Xavier computer</a> for its vehicles, with production starting in the early 2020s.</p>
<p>DRIVE AGX Xavier — built around our Xavier SoC, the world’s most advanced — is a highly integrated AI car computer that enables Volvo to streamline development of self-driving capabilities while reducing total cost of development and support.</p>
<p>The initial production release will deliver Level 2+ automated driving features, going beyond traditional advanced driver assistance systems.The companies are working together to develop automated driving capabilities, uniquely integrating 360-degree surround perception and a driver monitoring system.</p>
<p>The NVIDIA-based computing platform will enable Volvo to implement new connectivity services, energy management technology, in-car personalization options, and autonomous drive technology.</p>
<p>It’s a vision that’s backed by a growing number of automotive companies, with Huang announcing Wednesday that, in addition to Volvo Cars, Volvo Trucks, tier one automotive components supplier Continental, and automotive technology companies Veoneer and Zenuity and have all adopted NVIDIA DRIVE AGX.</p>
<p>Jensen also showed the audience a video of how, this month, an autonomous NVIDIA test vehicle, nicknamed BB8, completed a jam-packed 80-kilometer, or 50 mile, loop, in Silicon Valley without the need for the safety driver to take control &#8212; even once.</p>
<p>Running on the <a href="https://www.nvidia.com/en-us/self-driving-cars/drive-platform/hardware/">NVIDIA DRIVE AGX Pegasus</a> AI supercomputer, the car handled highway entrance and exits and numerous lane changes entirely on its own.</p>
<h2>From Hospitals Serving Millions to Medicine Tailored Just for You</h2>
<p>AI is also driving breakthroughs in the healthcare, Huang explained, detailing how NVIDIA Clara will harness GPU computing for everything from medical scanning to robotic surgery.</p>
<p>He also announced a <a href="https://blogs.nvidia.com/blog/2018/10/10/kings-college-london-nvidia-clara">partnership with King’s College London</a> to bring AI tools to radiology, and deploy it to three hospitals serving 8 million patients in the U.K.</p>
<p>In addition, he announced NVIDIA Clara AGX — which brings the power of Xavier to medical devices — has been <a href="https://blogs.nvidia.com/blog/2018/10/10/oxford-nanopore-dna-sequencer-nvidia-agx">selected by Oxford Nanopore</a> to power its personal DNA sequencer MinION, which promises to driven down the cost and drive up the availability of medical care that’s tailored to a patient’s DNA.</p>
<h2>A New Computing Era</h2>
<p>Huang finished his talk by recapping the new NVIDIA platforms being rolled out — the Turing GPU architecture; the RAPIDS data science platform; and DRIVE AGX for autonomous machines of all kinds.</p>
<p>Then he left the audience with a stunning demo of a nameless hero being prepared for action by his robotic assistants — before he returns to catch his robotics bopping along to K.C. and the Sunshine Band and join in the fun before returning to stage with a quick caveat.</p>
<p>“And I forgot to tell you everything was done in real time,” Huang said. “That was not a movie.”</p>
<p>The post <a rel="nofollow" href="https://blogs.nvidia.com/blog/2018/10/10/gtc-europe-keynote-rapids-volvo-clara-ai/">NVIDIA Launches GPU-Acceleration Platform for Data Science, Volvo Selects NVIDIA DRIVE</a> appeared first on <a rel="nofollow" href="https://blogs.nvidia.com">The Official NVIDIA Blog</a>.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/nvidiablog?a=GrD5y6IruMI:IHC5GzVOmgs:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/nvidiablog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/nvidiablog?a=GrD5y6IruMI:IHC5GzVOmgs:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/nvidiablog?i=GrD5y6IruMI:IHC5GzVOmgs:V_sGLiPBpWU" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/nvidiablog/~4/GrD5y6IruMI" height="1" width="1" alt=""/> <p><b><a href="http://feedproxy.google.com/~r/nvidiablog/~3/GrD5y6IruMI/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2051</wp:post_id>
		<wp:post_date><![CDATA[2018-10-10 11:15:33]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-10 11:15:33]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[nvidia-launches-gpu-acceleration-platform-for-data-science-volvo-selects-nvidia-drive]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feeds.feedburner.com/nvidiablog]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feedproxy.google.com/~r/nvidiablog/~3/GrD5y6IruMI/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[nvidia-launches-gpu-acceleration-platform-for-data-science-volvo-selects-nvidia-drive__trashed]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Around the Valley in 80 Kilometers: NVIDIA Autonomous Test Vehicle Completes Fully Driverless Highway Loop</title>
		<link>https://fifthlevel.ai/archives/2052</link>
		<pubDate>Wed, 10 Oct 2018 10:31:58 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=40791</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The number of white knuckles on Silicon Valley highways may be on its way down.</p>
<p>This month, an NVIDIA autonomous test vehicle, nicknamed BB8, completed an 80-kilometer (approximately 50-mile) route near company headquarters in Santa Clara, Calif., without the safety driver ever taking control.</p>
<p>Running on the <a href="https://www.nvidia.com/en-us/self-driving-cars/drive-platform/hardware/">NVIDIA DRIVE AGX Pegasus</a> AI supercomputer, the car handled highway on- and off-ramps and numerous lane changes entirely on its own, marking a significant step toward a self-driving future.</p>
<p>With industry-leading compute performance, a single DRIVE AGX Pegasus platform ran both <a href="https://www.nvidia.com/en-us/self-driving-cars/drive-platform/software/">NVIDIA DRIVE AV software</a>, which handles autonomous driving functions like object detection and 360-degree surround perception, and the <a href="https://www.nvidia.com/en-us/self-driving-cars/drive-ix/">DRIVE IX intelligent experience software stack</a>, which processes voice commands and monitors the driver to ensure they are paying attention.</p>
<p>This revolutionary drive was no science experiment — it was completed using entirely shippable parts. The sensors, embedded DRIVE AGX Pegasus platform and software are all available to autonomous vehicle developers.</p>
<p>&#8220;This is not a demo, this is something you can get right now,&#8221; said NVIDIA CEO Jensen Huang at GTC Europe Wednesday.</p>
<figure id="attachment_40802" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-10-at-11.55.19-AM.png"><img class="size-large wp-image-40802" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-10-at-11.55.19-AM-672x369.png" alt="" width="672" height="369" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-10-at-11.55.19-AM-672x369.png 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-10-at-11.55.19-AM-400x220.png 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-10-at-11.55.19-AM-768x422.png 768w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-10-at-11.55.19-AM-819x450.png 819w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-10-at-11.55.19-AM-391x215.png 391w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-10-at-11.55.19-AM-182x100.png 182w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-10-at-11.55.19-AM-1280x703.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-10-at-11.55.19-AM.png 1584w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption class="wp-caption-text">An NVIDIA autonomous test vehicle completed an 80-kilometer route without any human intervention.</figcaption></figure>
<p>NVIDIA always conducts public road tests with two trained drivers in the vehicle. One supervises the environment from the driver’s seat and another in the passenger seat observes both the driver and the environment. In addition, this test drive deployed Drive IX, which uses infrared camera to monitor where the driver is looking at all times as well as attentiveness and fatigue.</p>
<h2><b>Two Software Stacks, One Platform</b></h2>
<p>In the past decade of development, self-driving test vehicles have relied on bulky PCs and servers wired up in the trunk to run the various deep learning and path planning algorithms for autonomous driving. However, to deploy production-level self-driving cars, autonomous driving hardware must have a significantly smaller footprint and consume much less energy.</p>
<p>At the size of a laptop computer, DRIVE AGX Pegasus is an incredibly energy-efficient hardware solution, capable of 320 trillion operations per second (TOPS) of performance. The supercomputer is able to run multiple deep neural networks at once for autonomous driving, including those for DRIVE AV and DRIVE IX.</p>
<p>This parallel processing capability means the vehicle is able to run deep neural networks for perception — like those found in DRIVE AV — while also running the algorithms for natural language processing to enable voice commands in DRIVE IX.</p>
<p>So, passengers may ask, “What’s the weather in San Francisco?” as their car operates in autonomous mode, with the same computer running both tasks simultaneously.</p>
<h2><b>Way Beyond Cruise Control</b></h2>
<p>With congested highways, frequent entrances and exits, and various construction sites, San Francisco Bay Area traffic is no Sunday drive. However, with a robust hardware and software suite, the NVIDIA BB8 vehicle handled infamous California traffic with ease.</p>
<p>By using deep neural networks to identify lanes, other vehicles and driveable space, the autonomous test car slowed when cars entered from the on-ramp into its lane, speeding back up to the pace of traffic once the car was at a safe distance.</p>
<p>BB8 also seamlessly executed lane changes, safely moving between lanes once it detected ample space.</p>
<p>The ability to navigate these complex and unpredictable traffic scenarios without intervention from a human driver is a major milestone in the journey to full autonomy, where roads will be safer, more efficient and open.</p>
<p>The post <a rel="nofollow" href="https://blogs.nvidia.com/blog/2018/10/10/self-driving-highway-loop/">Around the Valley in 80 Kilometers: NVIDIA Autonomous Test Vehicle Completes Fully Driverless Highway Loop</a> appeared first on <a rel="nofollow" href="https://blogs.nvidia.com">The Official NVIDIA Blog</a>.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/nvidiablog?a=WGhdVV1ItGA:chEw1uAmOpE:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/nvidiablog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/nvidiablog?a=WGhdVV1ItGA:chEw1uAmOpE:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/nvidiablog?i=WGhdVV1ItGA:chEw1uAmOpE:V_sGLiPBpWU" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/nvidiablog/~4/WGhdVV1ItGA" height="1" width="1" alt=""/> <p><b><a href="http://feedproxy.google.com/~r/nvidiablog/~3/WGhdVV1ItGA/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2052</wp:post_id>
		<wp:post_date><![CDATA[2018-10-10 10:31:58]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-10 10:31:58]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[around-the-valley-in-80-kilometers-nvidia-autonomous-test-vehicle-completes-fully-driverless-highway-loop]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feeds.feedburner.com/nvidiablog]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feedproxy.google.com/~r/nvidiablog/~3/WGhdVV1ItGA/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[around-the-valley-in-80-kilometers-nvidia-autonomous-test-vehicle-completes-fully-driverless-highway-loop__trashed]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Automakers, Suppliers, Build Autonomous Future on NVIDIA DRIVE AGX</title>
		<link>https://fifthlevel.ai/archives/2053</link>
		<pubDate>Wed, 10 Oct 2018 10:31:07 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=40794</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Autonomous driving is no longer a distant goal post, a vision of the future set decades ahead. At GTC Europe, three leading companies — Volvo Cars, Continental and Veoneer — disclosed their near-term production plans for the next generation of cars.</p>
<p>Whether developing fully autonomous, highly automated or driver assistance systems, automakers and suppliers are realizing there is a need for far greater compute inside the car. And that’s why they are building their future vehicles on <a href="https://www.nvidia.com/en-us/self-driving-cars/drive-platform/hardware/">NVIDIA DRIVE AGX</a>, an architecture designed for safety and a platform able to handle increasingly sophisticated AI software.</p>
<p><a href="https://nvidianews.nvidia.com/news/volvo-selects-nvidia-drive-for-production-cars"><b>Volvo Cars</b></a>, a premium brand synonymous with safety and innovation, announced that it’s developing its next-generation core computer using NVIDIA DRIVE AGX Xavier.</p>
<p>The future production vehicles from Volvo will feature Level 2+ assisted driving features, going beyond today’s traditional advanced driver assistance systems. NVIDIA and Volvo are working together to develop these capabilities, uniquely integrating 360-degree surround perception and a driver-monitoring system. The foundational technology built on DRIVE AGX Xavier is able to ultimately scale up to Level 4 highly automated driving capabilities.</p>
<p>“A successful launch of autonomous drive will require an enormous amount of computing power as well as constant advances in artificial intelligence,” said Håkan Samuelsson, president and chief executive of Volvo Cars.</p>
<p><b>Continental</b> also announced that its Assisted and Automated Driving Control Unit (ADCU) is powered by DRIVE AGX Xavier. Together with NVIDIA, the supplier is developing a single platform architecture that scales from Level 2+ premium driver assistance to Level 3 and Level 4 Traffic Jam Chauffeurs and Highway Pilots, up to a fully autonomous Level 5 robotaxi. This codesigned hardware and software platform is also being leveraged for a range of commercial vehicles.</p>
<figure id="attachment_40796" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/cube-is-already-testing-at-the-frankfurt-site-data.jpg"><img class="size-large wp-image-40796" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/cube-is-already-testing-at-the-frankfurt-site-data-672x447.jpg" alt="" width="672" height="447" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/cube-is-already-testing-at-the-frankfurt-site-data-672x447.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/cube-is-already-testing-at-the-frankfurt-site-data-400x266.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/cube-is-already-testing-at-the-frankfurt-site-data-768x511.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2018/10/cube-is-already-testing-at-the-frankfurt-site-data-676x450.jpg 676w, https://blogs.nvidia.com/wp-content/uploads/2018/10/cube-is-already-testing-at-the-frankfurt-site-data-323x215.jpg 323w, https://blogs.nvidia.com/wp-content/uploads/2018/10/cube-is-already-testing-at-the-frankfurt-site-data-150x100.jpg 150w, https://blogs.nvidia.com/wp-content/uploads/2018/10/cube-is-already-testing-at-the-frankfurt-site-data-1280x852.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption class="wp-caption-text">Continental&#8217;s Level 5 CUbE vehicle will run on the NVIDIA DRIVE AGX Pegasus supercomputer.</figcaption></figure>
<p>On display at GTC is the CUbE (<b>C</b>ontinental <b>U</b>rban Mo<b>b</b>ility <b>E</b>xperience). This mobility-as-a-service vehicle development platform will be powered by NVIDIA DRIVE AGX Pegasus, an AI supercomputer designed for driverless vehicles.</p>
<p><b>Veoneer</b>, an autonomous driving supplier spun out from tier 1 supplier Autoliv, has also selected NVIDIA DRIVE AGX Xavier to architect its Level 4 highly automated self-driving supercomputer, known as “Zeus.”</p>
<p>This sophisticated AI supercomputer runs the NVIDIA DRIVE OS operating system and AV startup Zenuity’s autonomous driving software stack and will be production-ready in 2021. Zeus will fuse data from cameras, radars and other sensors, interpret the situation and take required action.</p>
<figure id="attachment_40797" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/VeoneerZeus.png"><img class="wp-image-40797 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/VeoneerZeus-672x448.png" alt="" width="672" height="448" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/VeoneerZeus-672x448.png 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/VeoneerZeus-400x267.png 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/VeoneerZeus-768x512.png 768w, https://blogs.nvidia.com/wp-content/uploads/2018/10/VeoneerZeus-675x450.png 675w, https://blogs.nvidia.com/wp-content/uploads/2018/10/VeoneerZeus-323x215.png 323w, https://blogs.nvidia.com/wp-content/uploads/2018/10/VeoneerZeus-150x100.png 150w, https://blogs.nvidia.com/wp-content/uploads/2018/10/VeoneerZeus-1280x853.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption class="wp-caption-text">Introducing Zeus, an autonomous vehicle supercomputer developed with NVIDIA DRIVE AGX Xavier.</figcaption></figure>
<p>“We developed Zeus to provide safe mobility solutions, and it is an important step towards industrializing autonomous driving in 2021. Working closely with expert partners NVIDIA and Zenuity is key to innovate trusted solutions for future mobility,” said Jan Carlson, chairman, president and CEO of Veoneer.</p>
<h2><b>Safety in Compute Numbers</b></h2>
<p>The DRIVE AGX platform enables the diverse and redundant operations necessary for safe autonomous driving. NVIDIA DRIVE AGX Xavier achieves an unprecedented 30 trillion operations per second (TOPS) of performance, while DRIVE AGX Pegasus, designed for Level 5 robotaxis, delivers 320 TOPS.</p>
<h2><b>Staying Flexible</b></h2>
<p>High-performance compute isn’t the only hallmark of DRIVE AGX. As it’s an open platform, automakers, suppliers and other autonomous driving companies are developing their own software to run on it.</p>
<p>DRIVE AGX can also be updated over the air, meaning autonomous functions can constantly improve with time, without having to change the hardware in the vehicle. True solutions don’t just solve the problems of today, they also anticipate the needs of the tomorrow.</p>
<p>The post <a rel="nofollow" href="https://blogs.nvidia.com/blog/2018/10/10/automakers-suppliers-drive-agx/">Automakers, Suppliers, Build Autonomous Future on NVIDIA DRIVE AGX</a> appeared first on <a rel="nofollow" href="https://blogs.nvidia.com">The Official NVIDIA Blog</a>.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/nvidiablog?a=1VeQ8sYluEI:JWb_zMOWBrg:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/nvidiablog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/nvidiablog?a=1VeQ8sYluEI:JWb_zMOWBrg:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/nvidiablog?i=1VeQ8sYluEI:JWb_zMOWBrg:V_sGLiPBpWU" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/nvidiablog/~4/1VeQ8sYluEI" height="1" width="1" alt=""/> <p><b><a href="http://feedproxy.google.com/~r/nvidiablog/~3/1VeQ8sYluEI/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2053</wp:post_id>
		<wp:post_date><![CDATA[2018-10-10 10:31:07]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-10 10:31:07]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[automakers-suppliers-build-autonomous-future-on-nvidia-drive-agx]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feeds.feedburner.com/nvidiablog]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feedproxy.google.com/~r/nvidiablog/~3/1VeQ8sYluEI/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[automakers-suppliers-build-autonomous-future-on-nvidia-drive-agx__trashed]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Waymo clocks 10 million self-driven miles on public roads, double in 8 months</title>
		<link>https://fifthlevel.ai/archives/2058</link>
		<pubDate>Wed, 10 Oct 2018 13:00:35 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2403679</guid>
		<description></description>
		<content:encoded><![CDATA[<img width="578" height="312" src="https://venturebeat.com/wp-content/uploads/2018/03/1-gxj5pqurc0jnqxkunzsywa.jpeg?fit=578%2C312&amp;strip=all" class="attachment-single-feed size-single-feed wp-post-image" alt="" /><hr />Alphabet’s self-driving technology division, Waymo, revealed that its autonomous vehicles have now driven 10 million miles on public roads. This represents a 100 percent increase on the 5 million-mile milestone it reached back in February, and the company later revealed it had notched up 6 million miles in May, 7 million in June, and 8 million in J&hellip;<a href="https://venturebeat.com/2018/10/10/waymo-clocks-10-million-self-driven-miles-on-public-roads-double-in-8-months/" target="_blank">Read More</a> <p><b><a href="https://venturebeat.com/2018/10/10/waymo-clocks-10-million-self-driven-miles-on-public-roads-double-in-8-months/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2058</wp:post_id>
		<wp:post_date><![CDATA[2018-10-10 13:00:35]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-10 13:00:35]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[waymo-clocks-10-million-self-driven-miles-on-public-roads-double-in-8-months]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/10/10/waymo-clocks-10-million-self-driven-miles-on-public-roads-double-in-8-months/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>UK Autodrive demonstrates self-driving Range Rover in Coventry</title>
		<link>https://fifthlevel.ai/archives/2061</link>
		<pubDate>Wed, 10 Oct 2018 11:46:27 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14311</guid>
		<description></description>
		<content:encoded><![CDATA[<p>A Range Rover Sport has completed the first ever self-driving lap of one of the UK’s most challenging road layouts. A prototype self-driving Range Rover Sport handled the complex Coventry Ring Road, successfully changing lanes, merging with traffic and exiting junctions at the speed limit of 40mph.</p> <p>The trial is part of the £20 million ($26.2 million) government-funded project, UK Autodrive, which ends this month after a three-year programme. Jaguar Land Rover engineers have completed significant self-driving technology testing on closed tracks before heading onto public roads in Milton Keynes and Coventry.</p> <p>The Range Rover Sport chosen for its performance and existing features, such as Adaptive Cruise Control, has been modified to include additional navigation sensors, RADAR and LIDAR. Coupled with the UK Autodrive research, the vehicle can now autonomously handle roundabouts, traffic lights, pedestrians, cyclists and other vehicles on complicated roads. It can also park itself.</p> <p>&#8220;The Coventry Ring Road is known for its complicated slip roads and exits. It makes for very challenging conditions, especially when under pressure in the rush hour. Our self-driving car is not impacted by the same pressure, frustrations or fatigue that a driver may experience and so it’s capable of turning a potentially very stressful situation into a completely stress-free one,&#8221; said Mark Cund, Jaguar Land Rover Autonomous Vehicle Research Manager.</p> <p dir="ltr">Highly skilled Jaguar Land Rover engineers have also developed connected features as part of UK Autodrive. The safety-enhancing and emission reducing technology use the internet to connect vehicles to each other and to infrastructure such as traffic lights.</p>
<p dir="ltr">
<p dir="ltr">UK Autodrive has helped accelerate the development of Jaguar Land Rover’s future self-driving and connected technology. As well as strengthening the Midlands’ position as a hub of mobility innovation. Britain’s biggest car maker, headquartered in Coventry, is working on fully-and semi-automated vehicle technologies to offer the choice of an engaged or automated drive. The company’s vision is to make its self-driving vehicles the most capable in the widest range of terrain and weather conditions – always putting the customer in command with the option to resume control.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/uk-autodrive-demonstrates-self-driving-range-rover-coventry/">UK Autodrive demonstrates self-driving Range Rover in Coventry</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/uk-autodrive-demonstrates-self-driving-range-rover-coventry/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2061</wp:post_id>
		<wp:post_date><![CDATA[2018-10-10 11:46:27]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-10 11:46:27]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[uk-autodrive-demonstrates-self-driving-range-rover-in-coventry]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/uk-autodrive-demonstrates-self-driving-range-rover-coventry/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Where the next 10 million miles will take us</title>
		<link>https://fifthlevel.ai/archives/2063</link>
		<pubDate>Wed, 10 Oct 2018 13:01:26 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/de51bebb67d3</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>By: John Krafcik, CEO</em></p><p>Our self-driving vehicles just crossed 10 million miles driven on public roads.</p><p>When it comes to driving, experience is the best teacher, and that experience is even more valuable when it’s varied and challenging. These millions of miles were driven in <a href="https://waymo.com/ontheroad/">25 cities</a> across the United States: in sunny California, dusty Arizona, and snowy Michigan, and from the high-speed roads around Phoenix to the dense urban streets of San Francisco.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SWhPXY-DhXC9kVTW_qr2WQ.jpeg" /><figcaption><em>Waymo in San Francisco</em></figcaption></figure><p>Our progress on public roads is made possible by our deep investment in <a href="https://medium.com/waymo/simulation-how-one-flashing-yellow-light-turns-into-thousands-of-hours-of-experience-a7a1cb475565">simulation</a>. By the end of the month, we’ll cross 7 billion miles driven in our virtual world (that’s 10 million miles every single day). In simulation, we can recreate any encounter we have on the road and make situations even more challenging through “fuzzing.” We can test new skills, refine existing ones, and practice extremely rare encounters, constantly challenging, verifying, and validating our software. We can learn exponentially through this combination of driving on public roads and simulation.</p><p>Thanks to nearly 10 years of experience, and keeping safety at the core of everything we do, we’ve been able to put the world’s first fleet of fully self-driving vehicles on the road. Safety is baked into how we drive today: we stay out of other driver’s blind spots, give wide berth to pedestrians, and come to a full stop at 4-way stops. In Phoenix, Arizona over 400 <a href="https://medium.com/waymo/waymos-early-rider-program-one-year-in-3a788f995a9c">early riders</a> use our app and ride in our cars, allowing them to get around town without the stress of driving and with the peace of mind that they’ll arrive safely.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FROAwXEqDk7k%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DROAwXEqDk7k&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FROAwXEqDk7k%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/e144d13edf2c46807c18c9951b982fc6/href">https://medium.com/media/e144d13edf2c46807c18c9951b982fc6/href</a></iframe><p>While we’ve made great strides thanks to these 10 million miles, the next 10 million will focus on turning our advanced technology into a service that people will use and love. To best serve our riders and make it possible for more people to benefit from this technology, we need to be safe and also capable, comfortable, and convenient.</p><p><strong>More capability</strong></p><p>We want to help as many people benefit from this technology as possible by operating in more places. Today, our vehicles are fully self-driving, around the clock, in a territory within the Metro Phoenix area. Now we’re working to master even more driving capabilities so our vehicles can drive even more places. Our engineers and scientists are applying advanced artificial intelligence and new in-house designed sensing systems to help us navigate complex weather conditions like heavy rain and snow, which are difficult even for human drivers.</p><p><strong>More comfort</strong></p><p>Our driving should feel natural to our riders and others on the road. Today, our cars are programmed to be cautious and courteous above all, because that’s the safest thing to do. We’re working on striking the balance between this and being assertive as we master maneuvers that are tough for everyone on the road. For example, merging lanes in fast-moving traffic requires a driver to be both assertive enough to complete the maneuver without causing others to brake and smooth enough to feel pleasant to our passengers.</p><p><strong>More convenience</strong></p><p>Self-driving technology is most useful if it gets you where you want to go, as quickly as possible. Today, our cars are designed to take the safest route, even if that means adding a few minutes to your trip. They won’t block your neighbor’s driveway and will choose the safest place to pull over, even if it means having to walk a few extra steps to a destination. We value our riders’ time, and with even more experience and feedback from people in our cars, we’re working on ways to make our routes, pick-ups, and drop-offs even more efficient. (That’s especially important in the Phoenix heat and suburbs with large parking lots!)</p><p>Over the next 10 million miles, our journey will take even more riders to even more places, in cars that are safe, in addition to being more capable, comfortable, and convenient. Building the world’s most experienced driver is a mission we’ll pursue for millions of miles to come, from 10 to 100 million and beyond. We hope you’ll come along for the ride!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=de51bebb67d3" width="1" height="1"><hr><p><a href="https://medium.com/waymo/where-the-next-10-million-miles-will-take-us-de51bebb67d3">Where the next 10 million miles will take us</a> was originally published in <a href="https://medium.com/waymo">Waymo</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><b><a href="https://medium.com/waymo/where-the-next-10-million-miles-will-take-us-de51bebb67d3?source=rss----7075a35566d9---4" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2063</wp:post_id>
		<wp:post_date><![CDATA[2018-10-10 13:01:26]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-10 13:01:26]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[where-the-next-10-million-miles-will-take-us]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/waymo]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/waymo/where-the-next-10-million-miles-will-take-us-de51bebb67d3?source=rss----7075a35566d9---4]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>DRIVE AGX Makes Self-Driving Dreams a Reality</title>
		<link>https://fifthlevel.ai/archives/2118</link>
		<pubDate>Thu, 11 Oct 2018 06:07:34 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=40782</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The result of more than 18,000 hardware engineer years and 10,000 software engineer years in development, the <a href="https://www.nvidia.com/en-us/self-driving-cars/drive-platform/hardware/">NVIDIA DRIVE</a> AGX platform delivers a true autonomous driving solution for production-level vehicles.</p>
<p>With today’s announcement of the availability of the <a href="https://developer.nvidia.com/drive/hardware">NVIDIA DRIVE Hyperion</a> sensor suite, which enables manufacturers to validate their technology in the vehicle, the DRIVE platform makes it possible for companies to deploy self-driving cars at scale. At <a href="https://www.nvidia.com/en-eu/gtc/">GTC Europe</a> Wednesday, NVIDIA CEO Jensen Huang outlined the end-to-end hardware and software solution for safe self-driving, demonstrating its capability with an <a href="https://blogs.nvidia.com/blog/2018/10/10/self-driving-highway-loop/">80-kilometer highway loop</a> driven in an autonomous test vehicle with no human intervention.</p>
<p>“This is not a demo, this is something you can get right now,” Huang said. “The entire computer is production ready and automotive grade.”</p>
<p>The journey to a production-level platform began more than a decade ago, with the vision that future autonomous machines would demand an entirely new type of processor that could handle diverse and redundant compute workloads. From this vision, the Xavier system-on-a-chip (SoC) was born.</p>
<p>The solution has scaled to an entire high-performance compute platform for full autonomous driving that can withstand the rigors of operating inside a vehicle, as well as solutions to test and validate various levels of self-driving technology.</p>
<h2><b>DRIVE AGX by the Numbers</b></h2>
<p>The DRIVE AGX platform is a true engineering feat offering the performance capabilities for production-level autonomous driving. Those tens of thousands of engineering years of effort have resulted in a DRIVE AGX Pegasus compute platform the size of a laptop that delivers the performance of more than 60 laptop computers. DRIVE AGX Pegasus development platform can scale up to 320 TOPS of peak performance.</p>
<p>High compute performance translates to safety when it comes to autonomous driving. This capability enables diverse and redundant processes, meaning the system can continue to operate in the event of malfunction or failure is detected. The industry agrees, with more companies running autonomous vehicle prototypes using DRIVE AGX than any other platform.</p>
<p>“We don’t rely on any one algorithm, sensor or computing block,” Huang said. “As a result our progress has been incredible.”</p>
<p>A key component to this functional diversity is software. <a href="https://blogs.nvidia.com/blog/2018/09/12/nvidia-drive-agx-developer-kit-autonomous-driving/">DRIVE Software</a> is an extensible, constantly improving suite of self-driving and user experience functionalities powered by AI, accelerated computer vision and image processing. At its core is DRIVE OS, which delivers a safe, secure, real-time hypervisor and other core capabilities for autonomous driving applications.</p>
<p>Layered upon that, sensor processing from radar, camera and lidar sensors enable the car to build a holistic world model, with DriveWorks applications, such as deep neural networks for autonomous vehicle perception and data recording.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/NVIDIA-autonomous-DRIVE-SOFTWARE_V2.jpg"><img class="aligncenter size-large wp-image-40830" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/NVIDIA-autonomous-DRIVE-SOFTWARE_V2-511x500.jpg" alt="" width="511" height="500" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/NVIDIA-autonomous-DRIVE-SOFTWARE_V2-511x500.jpg 511w, https://blogs.nvidia.com/wp-content/uploads/2018/10/NVIDIA-autonomous-DRIVE-SOFTWARE_V2-400x392.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/NVIDIA-autonomous-DRIVE-SOFTWARE_V2-768x752.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2018/10/NVIDIA-autonomous-DRIVE-SOFTWARE_V2-460x450.jpg 460w, https://blogs.nvidia.com/wp-content/uploads/2018/10/NVIDIA-autonomous-DRIVE-SOFTWARE_V2-220x215.jpg 220w, https://blogs.nvidia.com/wp-content/uploads/2018/10/NVIDIA-autonomous-DRIVE-SOFTWARE_V2-102x100.jpg 102w, https://blogs.nvidia.com/wp-content/uploads/2018/10/NVIDIA-autonomous-DRIVE-SOFTWARE_V2-1280x1253.jpg 1280w" sizes="(max-width: 511px) 100vw, 511px" /></a></p>
<p>In addition to self-driving capabilities, DRIVE Software enables in-vehicle functionalities, such as DRIVE IX for driver monitoring and user experience and DRIVE AR for visualization of the car’s perception of its environment.</p>
<h2><b>Streamlined Evaluation</b></h2>
<p>With hundreds of companies worldwide developing technologies for autonomous driving, solutions must be flexible and customizable. NVIDIA DRIVE AGX developer kits let manufacturers streamline the develop of autonomous vehicle systems.</p>
<p>The recently announced <a href="https://blogs.nvidia.com/blog/2018/09/12/nvidia-drive-agx-developer-kit-autonomous-driving/">DRIVE AGX Xavier</a> and DRIVE AGX Pegasus Developer Kits — which include the AI car computer and vehicle harness to connect the platform to the car, international power supply, camera sensor and other accessories — let companies test their self-driving applications.</p>
<p>For validation in the car, NVIDIA DRIVE Hyperion Development includes a DRIVE AGX Pegasus development platform along with sensors for autonomous driving (seven cameras, eight radars and optional lidar), sensors for driver monitoring, sensors for localization and other accessories. DRIVE Hyperion allows developers to experience and evaluate NVIDIA’s DRIVE Software suite consisting of DRIVE AV, DRIVE IX and DRIVE AR.</p>
<p>Manufacturers and developers can take advantage of the DRIVE software APIs to integrate their own software and test it in a real vehicle. DRIVE Hyperion is a complete sensor and compute platform setup that can be retrofitted onto a test vehicle.</p>
<p>With these powerful tools for development and validation, manufacturers now have everything at their fingertips to deploy self-driving for safer, more efficient transportation for all.</p>
<p>The post <a rel="nofollow" href="https://blogs.nvidia.com/blog/2018/10/10/drive-hyperion-development-platform/">DRIVE AGX Makes Self-Driving Dreams a Reality</a> appeared first on <a rel="nofollow" href="https://blogs.nvidia.com">The Official NVIDIA Blog</a>.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/nvidiablog?a=h81Q6rJtCEs:WpfqEYHI60A:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/nvidiablog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/nvidiablog?a=h81Q6rJtCEs:WpfqEYHI60A:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/nvidiablog?i=h81Q6rJtCEs:WpfqEYHI60A:V_sGLiPBpWU" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/nvidiablog/~4/h81Q6rJtCEs" height="1" width="1" alt=""/> <p><b><a href="http://feedproxy.google.com/~r/nvidiablog/~3/h81Q6rJtCEs/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2118</wp:post_id>
		<wp:post_date><![CDATA[2018-10-11 06:07:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-11 06:07:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[drive-agx-makes-self-driving-dreams-a-reality]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feeds.feedburner.com/nvidiablog]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feedproxy.google.com/~r/nvidiablog/~3/h81Q6rJtCEs/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[drive-agx-makes-self-driving-dreams-a-reality__trashed]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Veoneer launches new Autonomous Driving Supercomputer</title>
		<link>https://fifthlevel.ai/archives/2437</link>
		<pubDate>Wed, 10 Oct 2018 11:55:56 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14314</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Veoneer, Inc., a pure-play company focused on Advanced Driving Assistance Systems (ADAS) and Automated Driving (AD), has developed the “Zeus” supercomputer, designed to meet the requirements for level 4 Autonomous Driving with Zenuity’s Autonomous Driving software stack, based on the NVIDIA DRIVE AGX Xavier running NVIDIA DRIVE OS.</p> <p>The supercomputer Zeus is an ADAS/AD ECU, a “brain” that fuses data from cameras, radars and other sensors, interprets the situation and takes required action. Zeus has been designed to meet the requirements of level 4, High Automation, as defined by SAE (Society of Automotive Engineers), meaning the automated driving system manages all dynamic driving tasks and, under most circumstances, no human interaction is required.</p> <p>Veoneer and Zenuity, the joint venture between Veoneer and Volvo Cars, have developed the Zeus board. Veoneer is responsible for hardware and basic software, and Zenuity for the Autonomous Driving feature software development and vehicle integration. Zeus is based on the scalable architecture of NVIDIA DRIVE AGX Xavier and runs the NVIDIA DRIVE OS operating system.</p> <p>“We developed Zeus to provide safe mobility solutions, and it is an important step towards industrializing autonomous driving in 2021. Working closely with expert partners NVIDIA and Zenuity is key to innovate trusted solutions for future mobility,” says Jan Carlson, Chairman, President and CEO of Veoneer.</p> <p>The supercomputer is built on the NVIDIA Xavier system-on-a-chip, which is architected for safety by integrating six different processors that accelerate diverse and redundant algorithms, including deep learning artificial intelligence software. The platform provides the computational horsepower to process data from up to 27 sensors.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/veoneer-launches-new-autonomous-driving-supercomputer/">Veoneer launches new Autonomous Driving Supercomputer</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/veoneer-launches-new-autonomous-driving-supercomputer/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2437</wp:post_id>
		<wp:post_date><![CDATA[2018-10-10 11:55:56]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-10 11:55:56]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[veoneer-launches-new-autonomous-driving-supercomputer]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/veoneer-launches-new-autonomous-driving-supercomputer/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Metawave demonstrates 300 metre radar on Infineon’s platform</title>
		<link>https://fifthlevel.ai/archives/2438</link>
		<pubDate>Tue, 09 Oct 2018 15:07:13 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14305</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Metawave Corporation announced the first ever industry demonstration of advanced radar that is able to detect automobiles and their speed at 300 metres, and pedestrians and bicycles as far as 180 metres.</p> <p>Integrated with Infineon’s 77GHz radar chipset comprising of the RXS8160 MMIC and AURIX microcontroller, along with NVIDIA’s AI Processing Engine, Metawave’s development testing platform more than doubles today’s existing automotive sensors, which can only detect unidentifiable, often blurry objects at a much shorter 100 metre range.</p> <p>“We look forward to working with Infineon to develop an advanced platform that gives automotive industry giants the ability to deliver a faster and safer road to autonomous driving,” said Tim Curley, Metawave VP of Strategic Alliances. “We’re making great strides in demonstrating what smart, advanced radar is capable of achieving using Infineon chipsets – from showing high-resolution automotive 77GHz radar with object tracking, to AI deployment to learn what specifically is ahead of the car, to proving how our platform detects humans and cars at a very long range.”</p> <p>Advanced, smart radar plays a significant role in making autonomous driving safer, especially in challenging weather and operating conditions such as dense fog, heavy storms and dirty roads. Unlike cameras and LiDAR, radar can detect objects at a distance through difficult weather conditions. Today’s existing radar is limited in its ability to see high-resolution, making it impossible to determine and learn to classify through AI what is in front of the automobile, especially at 300 meters.</p> <p>Warlord, Metawave’s smart radar platform, uses <i>one</i> antenna and pushes complexity to analog. With Warlord, the antenna itself shapes and steers the beam, recognises objects quickly in the analog space and leverages AI to learn as the radar sees.</p> <p>“We are pleased to take our partnership with Metawave to the road,” said Ritesh Tyagi, Head of the Infineon Silicon Valley Automotive Innovation Centre. “This demonstration shows how Infineon’s radar chipset and Metawave’s sophisticated antenna technology can be combined to deliver a cutting-edge sensor platform that will advance the safety and future of autonomous driving.”</p> <p>Three sensors are fundamental components of the perception system for self-driving cars today: camera, LiDAR and radar. The camera is the highest resolution sensor but cannot see objects beyond 70 metres. LiDAR extends the range to about 180 metres with a fairly high resolution imaging capability. Radar operates at a lower frequency and sees long ranges sooner than any other sensor. Today&#8217;s radar lacks resolution and cannot differentiate objects. These systems require multiple antennas, which are heavy and expensive, and need to analyse every signal in the digital space, which takes time.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/metawave-demonstrates-300-metre-radar-infineons-platform/">Metawave demonstrates 300 metre radar on Infineon&#8217;s platform</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/metawave-demonstrates-300-metre-radar-infineons-platform/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2438</wp:post_id>
		<wp:post_date><![CDATA[2018-10-09 15:07:13]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-09 15:07:13]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[metawave-demonstrates-300-metre-radar-on-infineons-platform]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/metawave-demonstrates-300-metre-radar-infineons-platform/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Hyundai Cradle invests in Perceptive Automata</title>
		<link>https://fifthlevel.ai/archives/2439</link>
		<pubDate>Tue, 09 Oct 2018 13:32:43 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14301</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Hyundai Cradle, Hyundai Motor Company&#8217;s corporate venturing and open innovation business, announced that it is investing in Perceptive Automata to develop artificial intelligence software for self-driving cars and automated systems.</p> <p>Perceptive Automata, a startup based in <span class="xn-location">Somerville, Mass.</span> with an office in Silicon Valley, has developed software that gives autonomous vehicles the ability to understand the state-of-mind of people, including pedestrians, cyclists and other motorists. The predictive technology enables automated vehicles to make rapid judgments about the intentions and awareness of people on the street. This gives machines unprecedented human-like intuition.</p> <p>Perceptive Automata&#8217;s core technology takes sensor data from vehicles that show interactions with people. This rich data is used to train deep learning models to interpret human behavior the way people do. The end result is sophisticated AI software that can be integrated into autonomous driving systems. With the software installed, autonomous vehicles can anticipate what pedestrians, cyclists and motorists might do next.</p> <p>&#8220;We are ecstatic to have an investor on board like Hyundai that understands the importance of the problem we are solving for self-driving cars and next-generation driver assistance systems,&#8221; said <span class="xn-person">Sid Misra</span>, co-founder and CEO of Perceptive Automata. &#8220;Hyundai is one of the biggest automakers in the world and having them back our technology is incredibly validating.&#8221;</p> <p>Perceptive Automata&#8217;s software is particularly useful if a pedestrian begins to cross the street but sees the approaching autonomous car and decides to stop and &#8216;wave&#8217; it on. In this situation, an autonomous vehicle without the software would stop and wait, even though the pedestrian has no intention to cross. Perceptive Automata&#8217;s software can read the pedestrian&#8217;s intent and pass this information to the autonomous system&#8217;s decision-making module.</p> <p>&#8220;One of the biggest hurdles facing autonomous vehicles is the inability to interpret the critical visual cues about human behaviour that human drivers can effortlessly process,&#8221; said <span class="xn-person">John Suh</span>, vice president of Hyundai Cradle. &#8220;Perceptive Automata is giving the AV industry the tools to deploy autonomous vehicles that understand more like humans, creating a safer and smoother driving experience.&#8221;</p> <p>This year, Hyundai has expanded its investment into artificial intelligence technologies that can improve Hyundai&#8217;s core automotive business, as well as adjacent areas in robotics and human machine interactions. Hyundai has also been actively investing from its AI Alliance Fund, co-founded in late 2017, with SK Telecom and Hanwha Asset Management.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/hyundai-cradle-invests-perceptive-automata/">Hyundai Cradle invests in Perceptive Automata</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/hyundai-cradle-invests-perceptive-automata/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2439</wp:post_id>
		<wp:post_date><![CDATA[2018-10-09 13:32:43]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-09 13:32:43]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[hyundai-cradle-invests-in-perceptive-automata]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/hyundai-cradle-invests-perceptive-automata/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>US DOT lays down guidance document for autonomous vehicles</title>
		<link>https://fifthlevel.ai/archives/2440</link>
		<pubDate>Mon, 08 Oct 2018 10:33:24 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14285</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The U.S. Department of Transportation (USDOT) released new Federal guidance for automated vehicles, advancing its commitment to supporting the safe integration of automation into the broad multimodal surface transportation system.</p> <p>‘Preparing for the Future of Transportation: Automated Vehicles 3.0’ (AV 3.0) builds upon, but does not replace, voluntary guidance provided in ‘Automated Driving Systems 2.0: A Vision for Safety.’</p> <p>“The safe integration of automated vehicle technology into our transportation system will increase productivity, facilitate freight movement and create new types of jobs,” said Secretary Elaine L. Chao.</p> <p>AV 3.0 incorporates the results of extensive stakeholder engagement to provide updated voluntary guidance and policy considerations for a range of industry sectors, including: manufacturers and technology developers, infrastructure owners and operators, commercial motor carriers, bus transit, and State and local governments.</p> <p>AV 3.0 supports the safe development of automated vehicle technologies by:</p> <ul>
<li>Providing new multi-modal safety guidance</li>
<li>Reducing policy uncertainty and clarifying roles</li>
<li>Outlining a process for working with USDOT as technology evolves</li>
</ul> <p>Specifically, the new AV 3.0 guidance provides several updates to the Department’s initiatives relating to automated vehicles, by:</p> <ul>
<li>Stating that the Department will interpret and, consistent with all applicable notice and comment requirements, adapt the definitions of “driver” or “operator” as appropriate to recognise that such terms do not refer exclusively to a human, but may include an automated system.</li>
<li>Identifying and supporting the development of automation-related voluntary standards  developed through organisations and associations, which can be an effective non-regulatory means to advance the integration of automation technologies.</li>
<li>Affirming that the Department is continuing its work to preserve the ability for transportation safety applications to function in the 5.9 GHz spectrum.</li>
</ul> <p>AV 3.0 also announces and discusses several upcoming rulemakings and other actions being taken in the near future by the Department’s operating administrations, including:</p> <ol>
<li>The National Highway Traffic Safety Administration (NHTSA) will request public comment on a proposal to streamline and modernise the procedures it will follow when processing and deciding exemption petitions.</li>
<li>The Federal Motor Carrier Safety Administration (FMCSA) will initiate an Advance Notice of Proposed Rulemaking to address automated vehicles, particularly to identify regulatory gaps, including in the areas of inspection, repair, and maintenance for ADS.</li>
<li>The Federal Highway Administration (FHWA) announces plans to update the 2009 Manual on Uniform Traffic Control Devices (MUTCD), taking into consideration new connected and automated vehicle technologies.</li>
<li>The Federal Railroad Administration (FRA) is initiating research to develop and demonstrate a concept of operations, including system requirements, for the use of automated and connected vehicles to improve safety of highway-rail crossings.</li>
<li>The Maritime Administration (MARAD) and FMCSA are evaluating the regulatory and economic feasibility of using automated truck queueing as a technology solution to truck staging, access, and parking issues at ports.</li>
<li>The Pipelines and Hazardous Materials Administration (PHMSA) is researching the ability to enable the digital transmission of information to first responders before they arrive at an incident that involves hazardous materials.</li>
<li>The Federal Transit Administration (FTA) has published a five-year research plan on automating bus transit.</li>
</ol>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/us-dot-lays-guidance-document-autonomous-vehicles/">US DOT lays down guidance document for autonomous vehicles</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/us-dot-lays-guidance-document-autonomous-vehicles/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2440</wp:post_id>
		<wp:post_date><![CDATA[2018-10-08 10:33:24]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-08 10:33:24]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[us-dot-lays-down-guidance-document-for-autonomous-vehicles]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/us-dot-lays-guidance-document-autonomous-vehicles/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>RoboSense receives over $45 million funding for LiDAR development</title>
		<link>https://fifthlevel.ai/archives/2129</link>
		<pubDate>Thu, 11 Oct 2018 08:32:52 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14326</guid>
		<description></description>
		<content:encoded><![CDATA[<p>RoboSense, an autonomous driving LiDAR perception solution provider, announced the completion of <span class="xn-location">China&#8217;s</span> largest-ever single round of financing for a LiDAR company, a combined investment of over $45 million (<span class="xn-money">RMB 300 million</span>).</p> <p>The strategic funding is provided by technology and industry leaders, including Cainiao Smart Logistics Network Ltd. (&#8220;Cainiao&#8221;), the logistics arm of the Alibaba Group; SAIC Motor Group, the largest publicly-traded auto manufacturer in <span class="xn-location">China&#8217;s</span> A-Share; and BAIC Group (Beijing Automotive Industry Holding Co.) electric vehicle company.</p> <p>With an over 50% market share of all LiDAR sold in <span class="xn-location">Asia</span>, RoboSense is the market leader in the region. From previous financing rounds, RoboSense already has secured backing from auto manufacturing, financial brokerage, and artificial intelligence (AI) companies, including Haitong Securities, Fosun Group, Oriental Fortune Capital, Kinzon Capital, and Guangdong Investment Pte Ltd.</p> <p>The funding will be used to increase RoboSense&#8217;s market share and the R&amp;D of autonomous vehicle technologies, including its solid-state LiDAR, AI sensing algorithms, and other advanced technologies, as well as accelerating product development, long-term manufacturing and market penetration.</p> <p>&#8220;The rapid development of autonomous driving has ignited a huge demand for LiDAR,&#8221; said <span class="xn-person">Mark Qiu</span>, co-founder of RoboSense. &#8220;RoboSense is embracing this market demand through partnerships with multiple industry leaders. It is our great pleasure to be endorsed and funded by industry giants from many different fields. This round of funding is not only for capital assistance, but also for strategic resources. We are looking forward to continuously working with our partners to lead the large-scale commercialisation era of the autonomous driving industry.&#8221;</p> <p>In the past two years, RoboSense has had explosive growth. In <span class="xn-chron">April 2017</span>, the company completed mass production of its 16-beam automotive LiDAR. In <span class="xn-chron">September 2017</span>, the company mass-produced its 32-beam LiDAR, released a LiDAR-based autonomous driving environmental sensing AI system, and provided a software and hardware combined LiDAR environment sensing solution.</p> <p>In <span class="xn-chron">October 2017</span>, RoboSense launched its breakthrough product – the MEMS solid-state LiDAR, publicly exhibited for the first time at CES 2018 in <span class="xn-chron">January 2018</span>. Four months after CES 2018, RoboSense partnered with the Cainiao Network to launch the world&#8217;s first MEMS LiDAR autonomous logistics vehicle – the G Plus.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/robosense-receives-45-million-funding-lidar-development/">RoboSense receives over $45 million funding for LiDAR development</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/robosense-receives-45-million-funding-lidar-development/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2129</wp:post_id>
		<wp:post_date><![CDATA[2018-10-11 08:32:52]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-11 08:32:52]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[robosense-receives-over-45-million-funding-for-lidar-development]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/robosense-receives-45-million-funding-lidar-development/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Bosch launches IoT software solutions on Huawei Cloud</title>
		<link>https://fifthlevel.ai/archives/2130</link>
		<pubDate>Thu, 11 Oct 2018 07:55:16 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14323</guid>
		<description></description>
		<content:encoded><![CDATA[<p>At Huawei Connect 2018, Bosch, a global supplier of technology and services, and Huawei, a global provider of information and communications technology (ICT) infrastructure and smart devices, announced a partnership to accelerate the development of the internet of things (IoT) in China. The collaborative agreement will see the two partners make Bosch’s IoT Suite software services available in China on Huawei Cloud.</p> <p>“The demand for IoT solutions in China is rising. The partnership between Bosch and Huawei Cloud marks a decisive step for Bosch in one of the fastest growing IoT markets in Asia,” said Dr. Stefan Ferber, CEO of Bosch Software Innovations, a wholly-owned subsidiary of Bosch.</p> <p>The company expects China’s market for IoT platforms to grow by close to 70% over the coming years. “We are pleased to have the opportunity to collaborate with Huawei Cloud to offer cloud-based IoT services providing various functions needed to connect devices, users, and businesses. I am confident that by joining forces our companies will advance the development of the internet of things in China,” Ferber continued.</p> <p>The Bosch software platform connects web-enabled objects to facilitate data sharing across a multitude of digital services and business models. The first service made available to Chinese consumers via Huawei Cloud will be the Bosch IoT Remote Manager – a service for managing and controlling gateways, sensors, and devices. Additional services of the Bosch IoT Suite will follow in 2019.</p> <p>With offices in Shanghai and Nanjing, Bosch Software Innovations has been active in China since 2012 and has successfully implemented IoT projects ranging from Industry 4.0 to connected transportation. A leading Chinese automaker has chosen to deploy the Bosch IoT Suite on Huawei Cloud for updating its vehicles’ firmware over the air (FOTA). The solution is expected to be rolled out to millions of connected cars in China over the coming years.</p> <p>“Bosch Software Innovations is one of the global leaders in IoT, providing cutting-edge IoT solutions and services,” said Mr. Zheng Yelai, the vice president of Huawei and president of Huawei Cloud BU. “Huawei Cloud is a fast growing global cloud provider, and has committed to cultivating this fertile environment for its partners. With the technical excellence of Bosch IoT Suite, and the reliable infrastructure services by Huawei Cloud, we will provide more intelligent IoT solutions to smart cities, enterprises, families, and individuals.”</p> <p>Most significantly, for consumers in China, the world’s largest automotive market, the Bosch IoT Suite enables vital services such as the Vehicle Management Solution, which is available locally on Huawei Cloud. This solution connects vehicles throughout their service life, providing the technological foundation for cloud-based services such as predictive diagnostics and over-the-air software updates. The software provides a secure communication interface between the vehicle, the cloud, and the services. Data management enables vehicle manufacturers and fleet managers to organize vehicle data, analyze it, and keep the vehicle software continuously updated.</p> <p>Following this announcement, Bosch and Huawei intend to develop an integrated end-to-end IoT offering. Huawei is developing IoT hardware gateways that will be pre-configured with Bosch IoT Gateway software and managed through the Bosch IoT Remote Manager to run on Huawei Cloud. This close integration will provide customers with a more complete IoT solution that is easier to deploy and manage.</p> <p>Bosch and Huawei share a similar vision for the IoT, based on open source and industry standards and a commitment to building strong ecosystems in the domains of connected vehicles, manufacturing, homes, cities, and agriculture. Both companies are also members of the Eclipse Foundation and serve in leadership roles within the Industrial Internet Consortium and the OSGi Alliance.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/connected-car/bosch-launches-iot-software-solutions-huawei-cloud/">Bosch launches IoT software solutions on Huawei Cloud</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/connected-car/bosch-launches-iot-software-solutions-huawei-cloud/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2130</wp:post_id>
		<wp:post_date><![CDATA[2018-10-11 07:55:16]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-11 07:55:16]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[bosch-launches-iot-software-solutions-on-huawei-cloud]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/connected-car/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/connected-car/bosch-launches-iot-software-solutions-huawei-cloud/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Automakers Extend AI Use Beyond Autonomous Vehicles into Back Office</title>
		<link>https://fifthlevel.ai/archives/2139</link>
		<pubDate>Thu, 11 Oct 2018 06:08:51 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=40783</guid>
		<description></description>
		<content:encoded><![CDATA[<p>For companies developing autonomous vehicles, AI may be taking the wheel, but it’s also increasingly being used in the back office.</p>
<p>Automakers, suppliers and startups developing self-driving cars are implementing deep learning algorithms to operate vehicles without a human driver. With high-performance compute solutions from NVIDIA, vehicle manufacturers are also using AI in their data centers to enhance day-to-day operations.</p>
<p>From predicting inventory demand to training algorithms to make an unprotected left turn, automakers and <a href="https://www.nvidia.com/en-us/self-driving-cars/drive-platform/">NVIDIA DRIVE</a> ecosystem partners are turning to high-performance compute solutions to solve the problems of today and tomorrow.</p>
<h2><b>Meeting Customer Needs with an AI for Detail</b></h2>
<p>When it comes to selling cars, time is money. Extra inventory sitting on lots can cost automakers and dealers thousands of dollars, while cars with initial quality issues can lead to costly repairs and disgruntled customers.</p>
<p>With the ability to learn from past patterns, AI can help ease both these bottlenecks in the sales process. Using <a href="https://www.nvidia.com/en-us/data-center/dgx-1/">NVIDIA GPU-accelerated computing solutions</a> to run predictive deep learning algorithms, Volkswagen can project demand for specific models.</p>
<p>The result is fewer vehicles sitting idle at dealers and higher customer satisfaction. Volkswagen is implementing a similar AI process in its on-demand concierge service to predict and efficiently meet customer needs at time of service.</p>
<p>The automaker is also leveraging NVIDIA AI infrastructure to ease customer pain points throughout the vehicle life cycle. By ingesting traffic data to build an AI model that predicts the speed of traffic and road occupancy,  automakers can provide drivers with congestion updates and adjust the engine to operate most efficiently for upcoming road conditions.</p>
<p>New vehicles that aren’t up to quality standards can take a significant chunk out of an automaker’s bottom line. In the past five years, carmakers have set aside an average 2.7 percent of sales revenue every time they sold a vehicle, totalling $50 billion in warranty funds annually.</p>
<p>At a panel on datacenter strategies at <a href="https://www.nvidia.com/en-eu/gtc/">GTC Europe</a>, BMW said it’s addressing this challenge by using NVIDIA-powered deep learning algorithms to identify and address quality issues before cars get shipped — a more efficient process than visual inspection and traditional monitoring activities.</p>
<h2><b>Efficient Training for AI Drivers</b></h2>
<p>While data center solutions for AI are easing auto industry inefficiencies, companies are also using them to prepare for a new automotive future.</p>
<p>Autonomous Intelligent Driving (AID), a subsidiary of Audi developing self-driving technology for VW, Audi and Porsche, as well as BMW, are relying on <a href="https://www.nvidia.com/en-us/self-driving-cars/data-center/">NVIDIA GPU-accelerated computing solutions</a> to train deep learning algorithms for autonomous driving.</p>
<p>Zenuity, a joint venture between Volvo Cars and supplier Veoneer, is leveraging <a href="https://www.nvidia.com/en-us/self-driving-cars/data-center/">NVIDIA DGX</a> for self-driving AI training. With the high-performance datacenter, the autonomous driving software supplier is able to handle the exponentially increasing volume of sensory data to train their models, <a href="https://www.forbes.com/sites/nvidia/2018/06/28/making-self-driving-cars-a-reality-sooner/">reduce data bottlenecks</a> and speed up development cycles.</p>
<p>The high-performance data center solution enables companies to use massive amounts of data to train AI at unprecedented speeds. Engineers can use camera and sensor data collected from vehicles to teach AI algorithms the rules of the road and how to react to various traffic scenarios.</p>
<p>This streamlined solution brings safe self-driving to roads sooner, mitigating the $500 billion lost in damages due to traffic accidents each year.</p>
<p>These uses are just a few examples of how AI infrastructures can transform the industry, from new driving technologies to more efficient operations. With the level of compute enabled by NVIDIA data center solutions, the possibilities to work smarter and faster are endless.</p>
<p>The post <a rel="nofollow" href="https://blogs.nvidia.com/blog/2018/10/10/data-center-ai-infrastructure-for-automakers/">Automakers Extend AI Use Beyond Autonomous Vehicles into Back Office</a> appeared first on <a rel="nofollow" href="https://blogs.nvidia.com">The Official NVIDIA Blog</a>.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/nvidiablog?a=k2PdyjpjWSM:K1FRypUnIkw:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/nvidiablog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/nvidiablog?a=k2PdyjpjWSM:K1FRypUnIkw:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/nvidiablog?i=k2PdyjpjWSM:K1FRypUnIkw:V_sGLiPBpWU" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/nvidiablog/~4/k2PdyjpjWSM" height="1" width="1" alt=""/> <p><b><a href="http://feedproxy.google.com/~r/nvidiablog/~3/k2PdyjpjWSM/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2139</wp:post_id>
		<wp:post_date><![CDATA[2018-10-11 06:08:51]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-11 06:08:51]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[automakers-extend-ai-use-beyond-autonomous-vehicles-into-back-office]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feeds.feedburner.com/nvidiablog]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feedproxy.google.com/~r/nvidiablog/~3/k2PdyjpjWSM/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
			<wp:meta_value><![CDATA[automakers-extend-ai-use-beyond-autonomous-vehicles-into-back-office__trashed]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Porsche Aims To Keep Steering Wheel, Pedals, Manual &#039;Box Forever</title>
		<link>https://fifthlevel.ai/archives/2163</link>
		<pubDate>Thu, 11 Oct 2018 09:16:52 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://www.motor1.com/news/269300/porsche-keeping-pedals-steering-wheel/?utm_source=RSS&#038;utm_medium=referral&#038;utm_campaign=RSS-category-</guid>
		<description></description>
		<content:encoded><![CDATA[Thank you for that, Porsche. <p><b><a href="https://www.motor1.com/news/269300/porsche-keeping-pedals-steering-wheel/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2163</wp:post_id>
		<wp:post_date><![CDATA[2018-10-11 09:16:52]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-11 09:16:52]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[porsche-aims-to-keep-steering-wheel-pedals-manual-box-forever]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/rss/news/category/autonomous-cars/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.motor1.com/news/269300/porsche-keeping-pedals-steering-wheel/?utm_source=RSS&utm_medium=referral&utm_campaign=RSS-category-]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Hyundai Mobis and Tata Elxsi collaborate to develop scene generator tool</title>
		<link>https://fifthlevel.ai/archives/2174</link>
		<pubDate>Thu, 11 Oct 2018 16:08:30 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14335</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Mobis Technical Centre, India, a wholly owned subsidiary of Hyundai Mobis, South Korea, announced their collaboration with Tata Elxsi for the development of Synthetic Scene Generator Tool.</p> <p>HMTCI and Tata Elxsi are working on developing a tool that can replicate every real-world scenario an automobile could encounter, which could run into millions of possibilities.</p> <p>This tool would help accelerate the ongoing research and development support HMTCI is providing to their OEMs in Autonomous Driving.</p> <p>Tata Elxsi’s is bringing together deep expertise in digital technologies including Artificial Intelligence, Extended Reality, and Gaming to develop this advanced simulation tool that will accelerate the realization of driverless cars.</p> <p>Shaju S, Head of Automotive Division of Tata Elxsi said, “Our leadership position in automotive engineering services and Autonomous driving technologies, coupled with digital capabilities will be a valuable addition for Hyundai Mobis. We are delighted to be partnering with Hyundai Mobis in realising their vision for the future of autonomous driving.”</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/hyundai-mobis-tata-elxsi-collaborate-develop-scene-generator-tool/">Hyundai Mobis and Tata Elxsi collaborate to develop scene generator tool</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/hyundai-mobis-tata-elxsi-collaborate-develop-scene-generator-tool/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2174</wp:post_id>
		<wp:post_date><![CDATA[2018-10-11 16:08:30]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-11 16:08:30]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[hyundai-mobis-and-tata-elxsi-collaborate-to-develop-scene-generator-tool]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/hyundai-mobis-tata-elxsi-collaborate-develop-scene-generator-tool/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Why I Ride with Waymo: Samantha</title>
		<link>https://fifthlevel.ai/archives/2200</link>
		<pubDate>Fri, 12 Oct 2018 13:00:13 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/5f194b25fe4f</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>Editor’s Note: As part of our “Why I Ride with Waymo” series, we’ve been checking in with members of our early rider program to learn more about their experience using Waymo. We previously profiled </em><a href="https://medium.com/waymo/why-i-ride-with-waymo-lilla-6136f1bb279c"><em>Lilla</em></a><em>, and next up is Samantha, who tells us how Waymo has become a part of her family’s everyday life.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4AShurXxOD41DzZqME5QXg.png" /></figure><p><strong>Tell us a bit about you and your family!</strong></p><p>I’m Samantha from Tempe, Arizona. I met and married my husband, T’Shaka in November 2000, not too long after moving to the Phoenix area from Michigan, and we have two kids, Kyla, who’s 17, and T’Shaka II (TJ), who’s 12. We’re all members of the early rider program and use Waymo to get around every day.</p><p>For the past 11 years, I’ve worked for Downtown Phoenix Inc. as part of a team originally working to revitalize downtown. Now, we’re focused on cultivating and helping downtown continue to grow. I lead our Operations Department, which oversees programs in safety, hospitality, transportation, parking, placemaking, and homeless outreach.</p><p><strong>What’s something you and your family like about riding with Waymo?</strong></p><p>Driving stresses me out, so I’m happier riding with Waymo. It’s much more peaceful when you don’t have to be the driver. My kids, especially our 17 year old, like the freedom it provides. If she wants to go somewhere, she calls Waymo instead of asking for a ride. TJ likes the technology and ingenuity of the car and how cool it is to ride in one.</p><p><strong>What has been your family’s favorite ride so far?</strong></p><p>We started out the program driving with a Waymo employee at the wheel, so my husband loved his <a href="https://www.youtube.com/watch?v=I5ASOrmASVo">first ride</a> with no one in the driver’s seat. He wasn’t nervous at all, and it was a great experience. His only complaint was that the ride was too short!</p><p><strong>We’ve heard from riders that the Waymo driver has its own unique personality, the same way human drivers do. Have you noticed anything specific about our driver’s personality?</strong></p><p>Waymo is the most mindful driver. Safety is number one, and that is so appreciated.</p><p><strong>Has riding in our cars changed your opinion on self-driving technology?</strong></p><p>Yes, in a way. I was a fan of self-driving vehicles (or the idea of them) before we became early riders. Now having seen how capable the software is, I look forward to when even more people can ride with Waymo so they don’t have to drive at all.</p><p>For me, humans cause accidents. I can’t wait for safer roads for all modes of transportation — vehicles, pedestrians, bicyclists, scooters. I also want to see what impact self-driving technology has on traffic, the environment, and any other efficiencies it lends to our communities.</p><p><strong>Anything else you want to share about your experience with Waymo?</strong></p><p>This has truly been a memorable experience for my entire family. One day, our kids will get to say, “I remember when I was a teenager, we got to ride in some of the first self driving vehicles!” We feel like in some small way we made a contribution to this new way of life.</p><p>—</p><p>Our early rider program <a href="https://waymo.com/apply/">continues to take applications</a>; if you live in the area, drop us a line.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5f194b25fe4f" width="1" height="1"><hr><p><a href="https://medium.com/waymo/why-i-ride-with-waymo-samantha-5f194b25fe4f">Why I Ride with Waymo: Samantha</a> was originally published in <a href="https://medium.com/waymo">Waymo</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><b><a href="https://medium.com/waymo/why-i-ride-with-waymo-samantha-5f194b25fe4f?source=rss----7075a35566d9---4" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2200</wp:post_id>
		<wp:post_date><![CDATA[2018-10-12 13:00:13]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-12 13:00:13]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[why-i-ride-with-waymo-samantha]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/waymo]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/waymo/why-i-ride-with-waymo-samantha-5f194b25fe4f?source=rss----7075a35566d9---4]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Arizona and Intel team up on Institute for Automated Mobility to develop driverless vehicle technology</title>
		<link>https://fifthlevel.ai/archives/2383</link>
		<pubDate>Fri, 12 Oct 2018 01:30:32 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2404295</guid>
		<description></description>
		<content:encoded><![CDATA[<img width="578" height="367" src="https://venturebeat.com/wp-content/uploads/2018/01/apollocar.jpeg?fit=578%2C367&amp;strip=all" class="attachment-single-feed size-single-feed wp-post-image" alt="" /><hr />Arizona Governor Doug Ducey today announced that he has signed an executive order creating the Institute for Automated Mobility (IAM), a brain trust of enterprises, government agencies, and universities that will collaborate on autonomous vehicle testing in Arizona. IAM will comprise physical centers designed for &#8220;complex research&#8221; and&hellip;<a href="https://venturebeat.com/2018/10/11/arizonas-institute-for-automated-mobility-will-research-and-develop-autonomous-vehicle-technologies/" target="_blank">Read More</a> <p><b><a href="https://venturebeat.com/2018/10/11/arizonas-institute-for-automated-mobility-will-research-and-develop-autonomous-vehicle-technologies/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2383</wp:post_id>
		<wp:post_date><![CDATA[2018-10-12 01:30:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-12 01:30:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[arizona-and-intel-team-up-on-institute-for-automated-mobility-to-develop-driverless-vehicle-technology]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/10/11/arizonas-institute-for-automated-mobility-will-research-and-develop-autonomous-vehicle-technologies/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Huawei and Audi announce joint innovation in L4 Autonomous Driving</title>
		<link>https://fifthlevel.ai/archives/2436</link>
		<pubDate>Fri, 12 Oct 2018 16:21:22 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14352</guid>
		<description></description>
		<content:encoded><![CDATA[<p>At Huawei Connect 2018, Huawei and Audi showed their future plans of a cooperation in the field of intelligent connected vehicles (ICV), featuring the new Audi Q7 as a demonstration of this state-of-the-art technology.</p> <p>Executive Vice President of Audi China R&amp;D Saad Metz commented on the German automobile manufacturer’s plans saying, “We evaluate a joint development of highly automated driving functions and future oriented vehicle-2-infrastructure communications. Audi proved in many events over the world to be one of the technology leaders in the area of highly automated drive. We are looking forward to intensify the partnership with Huawei in the future, because we are convinced that a closer cooperation between our two companies will bring substantial benefits for both sides.”</p> <p>William Xu, Director of the Board and Chief Strategy Marketing Officer of Huawei, agreed. “As cars get smarter, we take advantage of our leading ICT technologies with Audi – one of the world’s most successful premium car brands – to lead automatic driving into the fast lane. Very soon, consumers will enjoy more secure, comfortable, convenient, and intelligent self-driving services.”</p> <p>Huawei’s Mobile Data Center (MDC) – is integrated into the Audi Q7 for urban automatic driving environments. The prototype was showcased to the public during Huawei Connect 2018.</p> <p>Earlier this year, Huawei and Audi signed a memorandum of understanding on strategic cooperation in Berlin to jointly develop intelligent connected vehicles. This time, their collaboration showed further progress in overall strategy between the two parties.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/huawei-audi-announce-joint-innovation-l4-autonomous-driving/">Huawei and Audi announce joint innovation in L4 Autonomous Driving</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/huawei-audi-announce-joint-innovation-l4-autonomous-driving/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2436</wp:post_id>
		<wp:post_date><![CDATA[2018-10-12 16:21:22]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-12 16:21:22]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[huawei-and-audi-announce-joint-innovation-in-l4-autonomous-driving]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/huawei-audi-announce-joint-innovation-l4-autonomous-driving/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Ford exploring technology that could eliminate the need for stopping at junctions</title>
		<link>https://fifthlevel.ai/archives/2441</link>
		<pubDate>Fri, 12 Oct 2018 08:19:05 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14345</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Ford is trialling a new way in which connected car technology could set us on that journey – and that takes its lead from how humans negotiate their way through busy crowds, by slowing down or speeding up to avoid collisions, without coming to a standstill.</p> <p>Intersection Priority Management (IPM) – being demonstrated on the streets of Milton Keynes, U.K., as part of the government-funded UK Autodrive programme – aims to keep drivers driving and bring an end to unnecessary stops at junctions, both easing traffic flow and increasing safety and efficiency.</p> <p>“We know that intersections and traffic lights can be a real bugbear for many drivers,” said Christian Ress, supervisor, Driver Assist Technologies, Ford Research and Advanced Engineering. “With the connected car technology we have been demonstrating this week, we envisage a world where vehicles are more aware of each other and their environment, enabling intelligent cooperation and collaboration on the roads – and around junctions.”</p> <p>Every year, the average driver spends two days waiting at traffic lights. And not only can junctions be frustrating – they are also the cause of up to 60% of road traffic accidents. As well as saving time, avoiding stopping at junctions could also save fuel, as drivers avoid braking and accelerating away from the lights.</p> <p>IPM uses vehicle-to-vehicle (V2V) communications to coordinate with other vehicles in the vicinity and suggests optimum speeds that will allow cars to safely pass by each other at intersections without coming to a halt.</p> <p>For the trial, test cars have been equipped with V2V communication systems that broadcast the vehicles’ location, direction of travel and speed. The onboard IPM systems are able to identify an upcoming junction and the trajectory of other vehicles approaching it. It will then suggest an optimum speed for each vehicle as they approach the junction that will allow them pass through safely.</p> <p>The vehicles in the trial have people behind the wheel, but it is envisaged that autonomous vehicles could also benefit from the technology. Automating how vehicles negotiate junctions with each other in this way that may mean that, one day, vehicles could pass through safely and efficiently without the need for traffic lights or road signs.</p> <p>While today’s autonomous vehicles operate independently using the sensor technologies and map data on board, V2V and vehicle-to-everything (V2X) communications technologies could benefit the driverless cars of the future.</p> <p>IPM builds upon other connected-car technologies developed by Ford and its project partners as part of UK Autodrive, a £20 million ($26.2 million) programme taking self-driving and connected car technologies from the test track to the streets.</p> <p>Among the technologies showcased during the two-year programme are Intersection Collision Warning, which alerts drivers of potential accidents when approaching an intersection, and Green Light Optimal Speed Advisory (GLOSA), which helps cars to synchronise with nearby traffic lights to help them avoid getting stuck on red.</p> <p>Other features demonstrated include Collaborative Parking – which builds a crowd-sourced map of a car park’s available spaces – and Emergency Vehicle Warning, where drivers are advised of the location and distance of an approaching emergency vehicle.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/connected-car/ford-exploring-technology-eliminate-need-stopping-junctions/">Ford exploring technology that could eliminate the need for stopping at junctions</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/connected-car/ford-exploring-technology-eliminate-need-stopping-junctions/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2441</wp:post_id>
		<wp:post_date><![CDATA[2018-10-12 08:19:05]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-12 08:19:05]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[ford-exploring-technology-that-could-eliminate-the-need-for-stopping-at-junctions]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/connected-car/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/connected-car/ford-exploring-technology-eliminate-need-stopping-junctions/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Intel partners with Rolls-Royce to develop autonomous cargo ships</title>
		<link>https://fifthlevel.ai/archives/2634</link>
		<pubDate>Mon, 15 Oct 2018 13:00:23 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2405188</guid>
		<description></description>
		<content:encoded><![CDATA[<img width="578" height="325" src="https://venturebeat.com/wp-content/uploads/2018/10/IAM-illustration2018.jpg?fit=578%2C325&amp;strip=all" class="attachment-single-feed size-single-feed wp-post-image" alt="IBM and Rolls Royce have announced an autonomous shipping platform." /><hr />Rolls-Royce today announced that it would use Intel chips as it develops a global system for autonomous ships that carry cargo on the high seas. The partnership follows the announcement last year of an ambitious timetable to have fully autonomous shipping fleets deployed by 2025. While based in the U.K., Rolls-Royce is developing the shipping techn&hellip;<a href="https://venturebeat.com/2018/10/15/intel-partners-with-rolls-royce-to-develop-autonomous-cargo-ships/" target="_blank">Read More</a> <p><b><a href="https://venturebeat.com/2018/10/15/intel-partners-with-rolls-royce-to-develop-autonomous-cargo-ships/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2634</wp:post_id>
		<wp:post_date><![CDATA[2018-10-15 13:00:23]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-15 13:00:23]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[intel-partners-with-rolls-royce-to-develop-autonomous-cargo-ships]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/10/15/intel-partners-with-rolls-royce-to-develop-autonomous-cargo-ships/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Waymo’s autonomies vehicles complete testing 10 million miles on public roads</title>
		<link>https://fifthlevel.ai/archives/2639</link>
		<pubDate>Mon, 15 Oct 2018 13:01:34 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14355</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Google backed Waymo&#8217;s self-driving vehicles just crossed the milestone of driving 10 million miles on public roads.  These millions of miles were driven in 25 cities across the United States: California, Arizona, Michigan, also from the high-speed roads around Phoenix to the dense urban streets of San Francisco.</p> <p>Waymo&#8217;s progress on public roads is made possible by their deep investment in simulation. By the end of the month, the company will cross 7 billion miles driven in virtual world (that’s 10 million miles every single day). In simulation, one can recreate any encounter they have on the road and make situations even more challenging through “fuzzing.” Waymo can test new skills, refine existing ones, and practice extremely rare encounters, constantly challenging, verifying, and validating their software. The company can learn exponentially through this combination of driving on public roads and simulation.</p> <p>Thanks to nearly 10 years of experience, and keeping safety at the core of everything, Waymo has been able to put the world’s first fleet of fully self-driving vehicles on the road. Safety is baked into how the autonomous cars drive today: these cars stay out of other driver’s blind spots, give wide berth to pedestrians, and come to a full stop at 4-way stops.</p> <p>In Phoenix, Arizona over 400 early riders use Waymo&#8217;s app and ride in their cars, allowing them to get around town without the stress of driving and with the peace of mind that they’ll arrive safely.</p> <p>The next 10 million will focus on turning Waymo&#8217;s advanced technology into a service that people will use and love. To best serve riders and make it possible for more people to benefit from this technology, they need to be safe and also capable, comfortable, and convenient.</p> <p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/waymos-autonomies-vehicles-complete-testing-10-million-miles-public-roads/">Waymo&#8217;s autonomies vehicles complete testing 10 million miles on public roads</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/waymos-autonomies-vehicles-complete-testing-10-million-miles-public-roads/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2639</wp:post_id>
		<wp:post_date><![CDATA[2018-10-15 13:01:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-15 13:01:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[waymos-autonomies-vehicles-complete-testing-10-million-miles-on-public-roads]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/waymos-autonomies-vehicles-complete-testing-10-million-miles-public-roads/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Riding the (autonomous) rails: Why does the future of mobility start in the past?</title>
		<link>https://fifthlevel.ai/archives/2656</link>
		<pubDate>Mon, 15 Oct 2018 14:09:40 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://360.here.com/riding-the-autonomous-rails-why-does-the-future-of-mobility-start-in-the-past</guid>
		<description></description>
		<content:encoded><![CDATA[<div class="hs-featured-image-wrapper"> <a href="https://360.here.com/riding-the-autonomous-rails-why-does-the-future-of-mobility-start-in-the-past" title="" class="hs-featured-image-link"> <img src="https://360.here.com/hubfs/HEREblog_autonomousrails_hero1.jpg?t=1539611440753" alt="HEREblog_autonomousrails_hero1" class="hs-featured-image" style="width:auto !important; max-width:50%; float:left; margin:0 15px 15px 0;"> </a> </div> <h3>Our mobility ecosystem is often framed in data and told through efficiency. What does this 1980s comedy case study tell us about the future of mobility and what we can’t leave behind? &nbsp;</h3> <p>Last year, France announced plans to develop the first driverless high-speed trains for its national rail system. According to a <span><a href="https://www.francetvinfo.fr/economie/transports/sncf/sncf-le-train-sans-conducteur-bientot-sur-les-rails-en-france_2236131.html">FranceInfo</a></span> report, the semi-autonomous TGV trains are expected to launch next year for prototype testing and could be shuttling passengers from Paris to Marseille by 2023. Between <span><a href="http://www.seabubbles.fr/en/">Seabubbles</a></span> and Hyperloops, the future of mobility seems like a race from automated to autonomous to absolutely baffling, all playing out in real time.</p> <img src="https://track.hubspot.com/__ptq.gif?a=2174253&amp;k=14&amp;r=https%3A%2F%2F360.here.com%2Friding-the-autonomous-rails-why-does-the-future-of-mobility-start-in-the-past&amp;bu=https%253A%252F%252F360.here.com&amp;bvt=rss" alt="" width="1" height="1" style="min-height:1px!important;width:1px!important;border-width:0!important;margin-top:0!important;margin-bottom:0!important;margin-right:0!important;margin-left:0!important;padding-top:0!important;padding-bottom:0!important;padding-right:0!important;padding-left:0!important; "> <p><b><a href="https://360.here.com/riding-the-autonomous-rails-why-does-the-future-of-mobility-start-in-the-past" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2656</wp:post_id>
		<wp:post_date><![CDATA[2018-10-15 14:09:40]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-15 14:09:40]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[riding-the-autonomous-rails-why-does-the-future-of-mobility-start-in-the-past]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://360.here.com/rss.xml]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://360.here.com/riding-the-autonomous-rails-why-does-the-future-of-mobility-start-in-the-past]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Self-Driving Ships</title>
		<link>https://fifthlevel.ai/archives/2689</link>
		<pubDate>Tue, 16 Oct 2018 17:44:53 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/777ae0a81ae2</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/920/1*T5lj7R5AtMzDMIcEXCLKrg.jpeg" /></figure><p><em>The Verge</em> reports on a partnership between Intel and Rolls-Royce to <a href="https://www.theverge.com/2018/10/15/17979252/self-driving-autonomous-ships-drones-intel-rolls-royce-partnership">build “self-driving” ships</a>. The article blends discussion of three different scenarios:</p><ol><li>autonomous long-haul shipping</li><li>remote-control operation</li><li>pilot assistance for docking and similar scenarios</li></ol><p>I have almost no knowledge of shipping or boats or the ocean or even water. I do know how to swim.</p><p>Nonetheless, I speculate that #3 seems the most useful.</p><p>The gains achieved by removing a human crew from a cargo ship seem minimal. In the context of a massive shipping vessel stuffed with rectangular containers, the cost of the human crew just doesn’t seem that significant.</p><p>But in the context of the close quarters of a harbor or port, I can imagine that there might be substantial performance gains from automation or pilot assistance.</p><p>Again, knowing not much about the actual constraints of maritime shipping, I could imagine harbors as bottlenecks, where ships get queued up in lines, waiting for relatively scarce tugboats and harbor pilots. Furthermore, ships do not turn on a dime, and so presumably need to maintain substantial buffer distances.</p><p>Autonomous shipping in close quarters might improve both the latency of docking (by allowing ships to skip the line) and the throughput (by allowing ships to shrink buffer distances).</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=777ae0a81ae2" width="1" height="1"><hr><p><a href="https://medium.com/self-driving-cars/self-driving-ships-777ae0a81ae2">Self-Driving Ships</a> was originally published in <a href="https://medium.com/self-driving-cars">Self-Driving Cars</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><b><a href="https://medium.com/self-driving-cars/self-driving-ships-777ae0a81ae2?source=rss----bdc6e635b3c0---4" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2689</wp:post_id>
		<wp:post_date><![CDATA[2018-10-16 17:44:53]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-16 17:44:53]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[self-driving-ships]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driving-cars]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driving-cars/self-driving-ships-777ae0a81ae2?source=rss----bdc6e635b3c0---4]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>ZF Develops 3D Interior Observation System for Advanced Occupant and Interior Sensing</title>
		<link>https://fifthlevel.ai/archives/2983</link>
		<pubDate>Wed, 17 Oct 2018 07:56:42 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14383</guid>
		<description></description>
		<content:encoded><![CDATA[<p>ZF is developing a three-dimensional Interior Observation System (IOS) capable of detecting and classifying vehicle occupants, determining their size, location and position, and whether or not they are in active control of the vehicle via the steering wheel or monitoring the automated driving systems, all key data to assist with advanced safety and automated driving functions.</p> <p>While ZF is well-known in the production of forward facing object recognition cameras that sense the external vehicle environment, the company has also been involved in the development of interior observation cameras for more than ten years. Due to the trend toward advanced safety and vehicle automation, interest in interior camera technology has intensified and ZF is now developing a 3D camera interior observation system supporting a broad variety of potential safety, comfort and AD applications.</p> <p>A prime example is occupant position sensing. Seat-mounted sensors are already used to determine whether airbag and seat belt pretensioners should be deployed and at what force, and a 3D IOS camera can augment this information regarding the size and real-time position and posture of occupants, including out-of-position detection such as reclined seats – all information that can help tailor occupant energy management prior or in the event of a crash.</p> <p>Just knowing if an occupant or other object is present in a seating position can help determine if and with which adaptive occupant safety functions to deploy in that position. The camera can complement existing seatbelt buckle sensors with visual verification if the occupant is properly belted, and give guidance to do so with a reminder or other action if this is not the case.</p> <p>Occupant sensing can also be extremely valuable in situations where small children may be left in the car – leading to emergency situations particularly in extreme temperatures – the IOS system can be calibrated to determine the presence of a child and activate emergency actions such as an automatic call to the vehicle owners mobile phone, lowering electric windows and opening a moonroof to reduce interior temperatures, activation of the horn and emergency flashers so others nearby can assist, or a call to emergency providers such as OEM emergency service providers or local police and first responders to assist in rescuing the child.</p> <p>As vehicles become increasingly equipped with automated functions, intelligently networked interior sensors can determine if the driver’s hands are on the steering wheel and are in active control of the vehicle, and if the driver’s head is facing the road and monitoring the vehicle while making proper use of the seatbelt system. The system can indicate that the driver is in an automated mode and when a potential emergency situation is detected can trigger alerts.</p> <p>“The bottom line is that visual data from the interior of the vehicle can be highly valuable from many standpoints,” said Norbert Kagerer, senior vice president engineering for the passive safety systems division. “The development of our 3D interior observation system leads to enhanced safety, convenience and helps support the evolution toward automated vehicles.</p> <p>The IOS system is expected to be production ready by late 2021.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/zf-develops-3d-interior-observation-system-advanced-occupant-interior-sensing/">ZF Develops 3D Interior Observation System for Advanced Occupant and Interior Sensing</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/zf-develops-3d-interior-observation-system-advanced-occupant-interior-sensing/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2983</wp:post_id>
		<wp:post_date><![CDATA[2018-10-17 07:56:42]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-17 07:56:42]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[zf-develops-3d-interior-observation-system-for-advanced-occupant-and-interior-sensing]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/zf-develops-3d-interior-observation-system-advanced-occupant-interior-sensing/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Cognata raises $18.5 million to grow its autonomous vehicle simulation platform</title>
		<link>https://fifthlevel.ai/archives/3002</link>
		<pubDate>Wed, 17 Oct 2018 10:04:23 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2406155</guid>
		<description></description>
		<content:encoded><![CDATA[<img width="578" height="322" src="https://venturebeat.com/wp-content/uploads/2018/10/Cognata-Homepage.jpg?fit=578%2C322&amp;strip=all" class="attachment-single-feed size-single-feed wp-post-image" alt="Cognata Homepage" /><hr />Cognata raises $18.5 million to grow autonomous vehicle simulation platform<a href="https://venturebeat.com/2018/10/17/cognata-raises-18-5-million-to-grow-its-autonomous-vehicle-simulation-platform/" target="_blank">Read More</a> <p><b><a href="https://venturebeat.com/2018/10/17/cognata-raises-18-5-million-to-grow-its-autonomous-vehicle-simulation-platform/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3002</wp:post_id>
		<wp:post_date><![CDATA[2018-10-17 10:04:23]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-17 10:04:23]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[cognata-raises-18-5-million-to-grow-its-autonomous-vehicle-simulation-platform]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/10/17/cognata-raises-18-5-million-to-grow-its-autonomous-vehicle-simulation-platform/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>ARM processor for Macs coming in 2020 or 2021, Apple car in 2023 says Ming-Chi Kuo</title>
		<link>https://fifthlevel.ai/archives/3085</link>
		<pubDate>Wed, 17 Oct 2018 17:05:00 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/10/17/arm-processor-for-macs-coming-in-2020-or-2021-apple-car-in-2023-says-ming-chi-kuo</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/28125-43197-19126-19082-tsmc-top-l-l.jpg" alt="Article Image" border="0" /> <br><br> The symbiotic relationship between Apple and TSMC is expected to continue for at least five years, with chips from the foundry expected to pop up in Macs no later than 2021, and in Apple's long-awaited car effort between 2023 and 2025, if Ming-Chi Kuo is correct. <p><b><a href="https://appleinsider.com/articles/18/10/17/arm-processor-for-macs-coming-in-2020-or-2021-apple-car-in-2023-says-ming-chi-kuo" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3085</wp:post_id>
		<wp:post_date><![CDATA[2018-10-17 17:05:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-17 17:05:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[arm-processor-for-macs-coming-in-2020-or-2021-apple-car-in-2023-says-ming-chi-kuo]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/10/17/arm-processor-for-macs-coming-in-2020-or-2021-apple-car-in-2023-says-ming-chi-kuo]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>How 20 Self-Driving Demos Wowed GTC Europe with Diversity and Innovation</title>
		<link>https://fifthlevel.ai/archives/3097</link>
		<pubDate>Wed, 17 Oct 2018 19:09:34 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=40848</guid>
		<description></description>
		<content:encoded><![CDATA[<p>From race cars to school buses to street sweepers, an autonomous vehicle for every need is on display at GTC Europe.</p>
<p>At the region’s premiere AI conference this week in Munich, NVIDIA DRIVE ecosystem partners demonstrated concepts and prototypes of an autonomous future. With more than 20 vehicles of all shapes and sizes at the event, it’s clear autonomy is spreading to virtually everything on wheels.</p>
<p>With a range of companies leveraging NVIDIA technology to make autonomous driving a reality, the possibilities for new forms of mobility are endless. Here’s a look at how a few NVIDIA partners are envisioning the new automotive era.</p>
<h2><b>Haulin’ MaaS</b></h2>
<p>When there’s no need for a human driver, vehicle designs can provide more space for passengers. Additionally, driverless vehicles can operate 24 hours a day, seven days a week without needing a break. This combination of roomier cars and increased availability facilitates more shared services for convenient and efficient transportation, a vision known as mobility-as-a-service (MaaS).</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-continental-cube.jpg"><img class="aligncenter size-large wp-image-40859" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-continental-cube-672x448.jpg" alt="Continental CUbE at GTC Europe" width="672" height="448" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-continental-cube-672x448.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-continental-cube-400x267.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-continental-cube-768x512.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-continental-cube-675x450.jpg 675w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-continental-cube-322x215.jpg 322w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-continental-cube-150x100.jpg 150w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-continental-cube-1280x854.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>For urban transit, global supplier <b>Continental</b> is developing CUbE, the Continental Urban moBility Experience. The autonomous shuttle, which will be powered by the NVIDIA DRIVE AGX Pegasus supercomputer, aims to increase efficiency in urban areas by enabling more shared rides and reducing space-consuming parking structures.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-vw-sedric.jpg"><img class="aligncenter size-large wp-image-40861" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-vw-sedric-672x448.jpg" alt="VW Sedric at GTC Europe" width="672" height="448" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-vw-sedric-672x448.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-vw-sedric-400x267.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-vw-sedric-768x512.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-vw-sedric-675x450.jpg 675w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-vw-sedric-323x215.jpg 323w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-vw-sedric-150x100.jpg 150w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-vw-sedric-1280x853.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>To provide a customized experience, <b>Volkswagen</b> has developed the Sedric driverless vehicle concept. The shuttle comes in various flavors, from an office-on-wheels to a roving dancefloor to a futuristic school bus — all serving to meet various travel needs in an autonomous age.</p>
<h2><b>A Clean Sweep for AVs</b></h2>
<p>Removing the driver’s seat doesn’t only create more room for passengers, it also makes it easier to design vehicles for logistics and other labor-intensive tasks.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-einride.jpg"><img class="aligncenter wp-image-40865 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-einride-672x448.jpg" alt="Einride at GTC Europe" width="672" height="448" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-einride-672x448.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-einride-400x267.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-einride-768x512.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-einride-675x450.jpg 675w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-einride-323x215.jpg 323w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-einride-150x100.jpg 150w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-einride-1280x853.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p><b>Einride</b> is a Swedish startup developing self-driving vehicles for hauling cargo. Its first vehicle, the T-pod, is tailor-made to carry heavy loads of cargo pallets. Its sibling, the T-log, is designed to transport lumber, easing logistics bottlenecks in the timber industry. Both robotrucks will take advantage of the high-performance compute of the NVIDIA DRIVE AGX platform to operate without a human driver.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-enway.jpg"><img class="aligncenter wp-image-40866 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-enway-672x448.jpg" alt="Enway at GTC Europe" width="672" height="448" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-enway-672x448.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-enway-400x267.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-enway-768x512.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-enway-675x450.jpg 675w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-enway-323x215.jpg 323w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-enway-150x100.jpg 150w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-enway-1280x853.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>Crowded conferences can create a mess in the neighborhood. Luckily, <b>Enway</b> brought along its autonomous street sweeper. Using deep neural networks, the Enway street sweeper can plan a path ahead and classify objects as trash, leaving clean pavement in its wake.</p>
<h2><b>Sleek Self-Driving</b></h2>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-audi-elaine.jpg"><img class="aligncenter wp-image-40864 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-audi-elaine-672x448.jpg" alt="Audi Elaine at GTC Europe" width="672" height="448" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-audi-elaine-672x448.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-audi-elaine-400x267.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-audi-elaine-768x512.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-audi-elaine-675x450.jpg 675w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-audi-elaine-323x215.jpg 323w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-audi-elaine-150x100.jpg 150w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-audi-elaine-1280x853.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>The <b>Audi</b> Elaine is a concept car for Level 4 autonomous driving, which doesn’t require any human intervention in defined conditions. The sporty vehicle would run a more advanced version of Audi’s zFas assisted driving platform, powered by NVIDIA.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-porsche-mission-e.jpg"><img class="aligncenter size-large wp-image-40867" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-porsche-mission-e-672x448.jpg" alt="Porsche Mission E at GTC Europe" width="672" height="448" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-porsche-mission-e-672x448.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-porsche-mission-e-400x267.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-porsche-mission-e-768x512.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-porsche-mission-e-675x450.jpg 675w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-porsche-mission-e-323x215.jpg 323w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-porsche-mission-e-150x100.jpg 150w, https://blogs.nvidia.com/wp-content/uploads/2018/10/12-gtc-eu-porsche-mission-e-1280x853.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>Luxury carmaker Porsche is leveraging NVIDIA technology to create an even more premium experience in the vehicle, such as the <b>Porsche</b> Mission E. The automaker’s first all-electric vehicle, the Mission E is a sleek leader into the future of electromobility.</p>
<p>Commuting, logistics, city services and even joy riding are being transformed by autonomous driving. <a href="https://www.nvidia.com/en-us/self-driving-cars/partners/">Learn more</a> about how NVIDIA partners are moving the world forward with self-driving technology.</p>
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="https://blogs.nvidia.com/blog/2018/10/17/self-driving-demos-gtc-europe/">How 20 Self-Driving Demos Wowed GTC Europe with Diversity and Innovation</a> appeared first on <a rel="nofollow" href="https://blogs.nvidia.com">The Official NVIDIA Blog</a>.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/nvidiablog?a=j8Hynq4Z8Gg:AhmThQXk0FY:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/nvidiablog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/nvidiablog?a=j8Hynq4Z8Gg:AhmThQXk0FY:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/nvidiablog?i=j8Hynq4Z8Gg:AhmThQXk0FY:V_sGLiPBpWU" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/nvidiablog/~4/j8Hynq4Z8Gg" height="1" width="1" alt=""/> <p><b><a href="http://feedproxy.google.com/~r/nvidiablog/~3/j8Hynq4Z8Gg/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3097</wp:post_id>
		<wp:post_date><![CDATA[2018-10-17 19:09:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-17 19:09:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[how-20-self-driving-demos-wowed-gtc-europe-with-diversity-and-innovation]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feeds.feedburner.com/nvidiablog]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feedproxy.google.com/~r/nvidiablog/~3/j8Hynq4Z8Gg/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>17 Startups Driving the Automotive Future at GTC Israel</title>
		<link>https://fifthlevel.ai/archives/3118</link>
		<pubDate>Thu, 18 Oct 2018 09:00:32 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=40918</guid>
		<description></description>
		<content:encoded><![CDATA[<p>At <a href="https://www.nvidia.com/en-il/gtc/">GTC Israel</a> this week, 17 automotive startups showcased the latest developments in autonomous driving, connectivity and mobility services.</p>
<p>The young companies are developing on <a href="https://www.nvidia.com/en-gb/self-driving-cars/drive-platform/">NVIDIA DRIVE</a>, taking advantage of Israel’s fast-paced, flexible startup culture to speed the deployment of safe self-driving on public roads.</p>
<p>With the help of high-performance, energy-efficient compute from the <a href="https://developer.nvidia.com/drive">NVIDIA DRIVE platform</a>, these companies are moving the industry forward with breakthrough technologies.</p>
<p>From emergency response to simulation testing to thermal sensors, GTC exhibitors are taking novel approaches self-driving systems. Here’s a look at a few of the startups helping drive the future of the automotive industry in Israel.</p>
<h2><b>Sensing Innovation</b></h2>
<p>Most of the industry agrees that self-driving cars require a trio of sensor types: camera, radar and lidar. However, for truly advanced pedestrian and object detection, Israeli startup <b>AdaSky</b> contends that thermal sensors must be included in the mix.</p>
<p><iframe width="500" height="375" src="https://www.youtube.com/embed/me_6pc-UmMY?feature=oembed" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
<p>The company’s Viper sensor can measure heat from objects surrounding the car, using algorithms running on the NVIDIA DRIVE platform to classify them as humans, vehicles or other objects. The state-of-the-art sensor can be easily incorporated into any autonomous driving or advanced driver assistance system for an added layer of safety.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/FACE_web_page_image.jpg"><img class="alignright wp-image-40919 size-medium" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/FACE_web_page_image-400x267.jpg" alt="" width="400" height="267" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/FACE_web_page_image-400x267.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/FACE_web_page_image-672x448.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/FACE_web_page_image-675x450.jpg 675w, https://blogs.nvidia.com/wp-content/uploads/2018/10/FACE_web_page_image-322x215.jpg 322w, https://blogs.nvidia.com/wp-content/uploads/2018/10/FACE_web_page_image-150x100.jpg 150w, https://blogs.nvidia.com/wp-content/uploads/2018/10/FACE_web_page_image.jpg 700w" sizes="(max-width: 400px) 100vw, 400px" /></a></p>
<p>Sensors inside the car can be just as important as those outside. Before Level 5 driverless vehicles hit the road, drivers must still pay attention and be able to take back control. That’s why <b>Jungo Connectivity </b>has integrated its AI software into the <a href="https://www.nvidia.com/en-gb/self-driving-cars/drive-ix/">NVIDIA DRIVE IX </a>intelligent experience platform for advanced driver monitoring. The company’s algorithms can help detect whether a driver’s attention is on the road while they are driving or advanced driver assistance systems are operating.</p>
<p><b>Guardian Optical Technologies </b>is also developing in-cabin sensing technology, making it more affordable and more efficient for manufacturers to implement driver and passenger detection on a single sensor. Backed by NVIDIA GPU compute, one sensor mounted on the ceiling of the vehicle can track the body positions of the driver and passengers in the car to enable safety and convenience features.</p>
<h2><b>Pioneering Software</b></h2>
<p>Autonomous vehicles must be tested and validated before operating on public roads. Software startup <b>Cognata</b> builds detailed simulation environments, allowing manufacturers to perform these tests in any condition, over and over again, before the rubber ever meets the road.</p>
<figure id="attachment_40921" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/Cognata-live-system-3.jpg"><img class="wp-image-40921 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/Cognata-live-system-3-672x378.jpg" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/Cognata-live-system-3-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Cognata-live-system-3-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Cognata-live-system-3-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Cognata-live-system-3-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Cognata-live-system-3-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Cognata-live-system-3-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Cognata-live-system-3-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption class="wp-caption-text">Cognata simulates driving environments for autonomous vehicle testing and validation.</figcaption></figure>
<p>Cognata, which won the NVIDIA Inception startup award at last year’s GTC Israel, is integrating its world-building simulation algorithms into the <a href="https://blogs.nvidia.com/blog/2018/09/12/drive-constellation-open-simulation/">open NVIDIA DRIVE Constellation platform</a>.</p>
<figure id="attachment_40922" style="width: 623px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/Haifa-tactile-map.png"><img class="wp-image-40922 size-full" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/Haifa-tactile-map.png" alt="" width="623" height="370" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/Haifa-tactile-map.png 623w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Haifa-tactile-map-400x238.png 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Haifa-tactile-map-362x215.png 362w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Haifa-tactile-map-168x100.png 168w" sizes="(max-width: 623px) 100vw, 623px" /></a><figcaption class="wp-caption-text">A Tactile Mobility map marking the varying road gradients around Haifa, Israel.</figcaption></figure>
<p>For safe autonomous driving, cars can’t just see the world around them, they must also be able to feel it. With software from <b>Tactile Mobility</b>, vehicles can interpret information such as road grade and tire grip from data collected by sensors on the vehicle. Using the NVIDIA DRIVE platform, the vehicle can fuse this data for a comprehensive view of its surroundings.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-17-at-1.19.52-PM.png"><img class="aligncenter size-large wp-image-40923" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-17-at-1.19.52-PM-672x346.png" alt="" width="672" height="346" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-17-at-1.19.52-PM-672x346.png 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-17-at-1.19.52-PM-400x206.png 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-17-at-1.19.52-PM-768x395.png 768w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-17-at-1.19.52-PM-842x433.png 842w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-17-at-1.19.52-PM-406x209.png 406w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-17-at-1.19.52-PM-188x97.png 188w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-17-at-1.19.52-PM-1280x658.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>In addition to startups, students showcased innovative technology at this year’s GTC Israel. Researchers at <b>Israel Technion University</b> demonstrated their Formula 1 autonomous vehicle project on the event floor. Fresh off a driverless run on public roads in Israel, the car uses high-performance compute from NVIDIA DRIVE to run autonomous driving software.</p>
<p>Learn more about NVIDIA DRIVE partners in Israel and watch <a href="https://www.ustream.tv/gpu-technology-conference">this year’s keynote</a>.</p>
<p>The post <a rel="nofollow" href="https://blogs.nvidia.com/blog/2018/10/18/startups-automotive-gtc-israel/">17 Startups Driving the Automotive Future at GTC Israel</a> appeared first on <a rel="nofollow" href="https://blogs.nvidia.com">The Official NVIDIA Blog</a>.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/nvidiablog?a=Owocd8b1ZfM:laZhHZmjzZk:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/nvidiablog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/nvidiablog?a=Owocd8b1ZfM:laZhHZmjzZk:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/nvidiablog?i=Owocd8b1ZfM:laZhHZmjzZk:V_sGLiPBpWU" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/nvidiablog/~4/Owocd8b1ZfM" height="1" width="1" alt=""/> <p><b><a href="http://feedproxy.google.com/~r/nvidiablog/~3/Owocd8b1ZfM/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3118</wp:post_id>
		<wp:post_date><![CDATA[2018-10-18 09:00:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-18 09:00:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[17-startups-driving-the-automotive-future-at-gtc-israel]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feeds.feedburner.com/nvidiablog]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feedproxy.google.com/~r/nvidiablog/~3/Owocd8b1ZfM/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Rethinking Maps for Self-Driving</title>
		<link>https://fifthlevel.ai/archives/3270</link>
		<pubDate>Mon, 15 Oct 2018 19:32:43 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/a147c24758d6</guid>
		<description></description>
		<content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*n2vl1FcI1NPTeJ7PwCSC5A.png" /></figure><p><em>By Kumar Chellapilla, Director of Engineering, </em><a href="http://lyft.com/level5"><em>Lyft Level 5</em></a></p><p>Maps are a key component to building self-driving technology. Unlike regular <a href="https://en.wikipedia.org/wiki/Comparison_of_web_map_services">web map services</a> which are in wide use today for turn by turn navigation, the specialized needs of autonomous vehicles (AVs) require a new class of high definition (HD) maps. These HD maps need to represent the world at an unprecedented centimeter resolution, which is one to two orders of magnitude greater than the roughly meter level resolution that web map services offer today.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/705/0*Z_snChhawfkaywJa" /></figure><p>AVs demand such a high resolution because they need to routinely execute complex maneuvers such as nudging into a bike lane to take a turn and safely passing bicyclists. For example, <a href="https://safety.fhwa.dot.gov/ped_bike/univcourse/swless19.cfm">marked bike lanes in the United States</a> are typically 4 feet (1.2 meters) wide, but can be as narrow as 2 feet (0.6 meters) for unmarked lanes. The lane markings themselves are 4 inches (10 cm) wide. Centimeter level accurate maps are a must for an AV to be able to confidently reason about its position within a lane, assess distance from the curb and confidently take action.</p><p>In this blog post, I’ll share how we at Level 5 are thinking about HD maps for self-driving and how the various pieces of information in the HD map are produced and organized for use by the AV. This treatment is meant for a general audience who is interested in learning about HD maps and how they power self-driving. Future posts will go into deeper technical aspects of HD maps.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/573/0*M79hXnDlhzYfoKtC" /></figure><h3>What is the map?</h3><p>HD maps are not new. Several mapping and self-driving companies have already started to produce and consume HD maps. However, it is still early days in terms of how these maps are being built, the richness of information they contain, and how accurate they are. Companies are iterating quickly on making these HD maps better and as such there is little standardization between various providers and consumers.</p><p>At Level 5, the HD maps we are building are purpose driven. Our goal is to accelerate the development of self driving vehicles that serve ridesharing scenarios. The overall Level 5 mission is to become the first ridesharing network to operate self-driving vehicles at scale. Scale here implies hundreds of thousands of self-driving vehicles. Given this, we view the HD map as a specialized component in the autonomy stack.</p><p>What belongs in an HD map and what doesn’t is decided based on a set of principles. One can ask, why does this matter? If you build HD maps for one use case, can’t they be easily used for other purposes? The short answer is No, while a somewhat longer answer is Maybe. Let’s consider two examples. HD maps can be built either for use by AVs in a ride-sharing network or for use with augmented reality (AR) applications on mobile phones. The former would focus on road elements and high demand routes with low driver supply, while the latter would focus on street side, store fronts and interiors, public landmarks, etc. There is little overlap between the two use cases. Having a clear set of principles helps us build the right HD mapping technology. Tight integration between the map build technology and autonomy technology helps accelerate both by creating and leveraging shared components. For this reason, most HD maps built for autonomy use today are vertically integrated with the corresponding autonomy software stack.</p><h3>HD Map Principles</h3><p>There are four main principles that capture how we define the map and go about building it.</p><p><strong>Mapping as pre-computation:</strong> From the point of view of self-driving technology, the mapping operation includes everything we can do to pre-compute things before the AV starts driving. In some cases, this pre-computation can result in completely solving sub-parts of the autonomy problem. For example, perception and localization of static objects in the world such as roads, intersections, street signs, etc. can be solved offline and in a highly accurate manner. Human operators can curate pre-computed data to ensure high quality. In other cases, the problem cannot be completely solved ahead of time, but one can pre-compute partial, approximate, or intermediate results that can make real-time autonomy work easier. We use the latter to build various <em>map prior</em> pieces that represent Bayesian prior probabilities about dynamic parts of the world, such as observed speed profiles and unmarked parking spaces. Pre-computed results include both spatial and temporal aspects of the world and are indexed for efficient retrieval.</p><p><strong>Mapping to improve safety:</strong> The use of maps for navigation is well understood. An AV that aspires to drive safely not only needs to perform expert navigation, but also adopt pragmatic best practices that reduce risk during driving. So, Level 5 HD maps are designed not only to contain speed limit information for each lane segment, but also speed profiles derived from actual human drivers on the Lyft network that meet our high bar for safety.</p><p><strong>Map as a unique sensor:</strong> At runtime, the map is viewed by the autonomy system as a sensor with special perception and prediction capabilities. When compared with other sensors such as cameras and lidar, the map has no range limitations. It can sense things way beyond the 100–200m range that is typical of today’s AV sensors. It is also immune to runtime occlusion from dynamic objects like other vehicles. Viewing the map as yet another sensor allows us to design efficient map access patterns and integrate map data more naturally into the autonomy stack (e.g. sensor-fusion components).</p><p><strong>Map as global shared state:</strong> AVs can achieve higher safety and efficiency levels by working together in AV fleets. You can think of the pre-computed data described above as social memory of the whole AV fleet that is accessible to every AV in the fleet as part of the map. This memory is very large and changes slowly. In addition, at runtime, you can view the AV fleet as sensing the world together in a distributed manner from various angles and points of view. The map then becomes a shared data structure that lives both in the cloud and also docked in each of the AVs. AVs use the map to both read and write to this social memory. The latter is how we think of sharing real-time information between the AVs in the fleet.</p><h3>Mapping Vehicles</h3><p>We build HD maps using a fleet of cars containing state of the art sensors such as HD cameras, lidar, radar, GPS, and inertial measurement units (IMUs). Early on we chose to keep the sensor configuration and hardware build to be the same as that used by our self-driving cars. This constraint makes it easy for us to ensure that the HD maps we build work correctly with the subsequent autonomy software stack which supports various self-driving functions such as localization, perception, prediction, and planning. In our experience, sharing cars interchangeably between mapping and autonomy feature development gets us better utilization of our autonomy fleet while keeping fleet management overhead low. Similarly, having a single hardware SKU makes it simpler for us to manage hardware development. Our map build process also uses the same data collect logs as autonomy feature development. This enables us to re-use any and all miles collected by the car to build better maps. Map build data collection runs don’t need the self-driving car to be engaged in autonomy mode aka the car is driving by itself. However, we do keep many of the autonomy services running, especially localization and perception, in passive mode as their outputs help greatly in subsequent map build processing. For example, perception output can help with removal of dynamic objects that aren’t part of the HD map and localization match errors can quickly help us detect which parts of the map are stale.</p><h3>A Layered Map</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*novXPga1nTb5aI1g9_-ReQ.png" /></figure><p>The information contained in an HD map is represented as layers. Organizing the information in layers makes it easy to independently design, build, test, and release new information. These layers are perfectly aligned with each other and indexed in a manner that allows for efficient parallel lookups of information both for the current location of the AV and also local neighborhood. We think of the basic road network data offered by web map services as being the bottom most layer. Each subsequent layer adds additional details to the map. At Lyft Level 5, our HD maps contain several layers. Four noteworthy HD layers are: the geometric map, the semantic map, map priors, and real-time knowledge.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xmWnrK02fY-8jHKR" /><figcaption>Geometric map</figcaption></figure><h4>Geometric Map Layer</h4><p>The geometric map layer contains 3D information of the world. This information is organized in very high detail to support precise calculations. Raw sensor data from lidar, various cameras, GPS, and IMUs is processed using simultaneous localization and mapping (SLAM) algorithms to first build a 3D view of the region explored by the mapping data collect run. The outputs of the SLAM algorithm are an aligned dense 3D point cloud and a very precise trajectory that the mapping vehicle took. The vehicle trajectory is shown in pink. Each of the 3D points is colored using the colors observed for that 3D point in the corresponding camera images. The 3D point cloud is post-processed to produce derived map objects that are stored in the geometric map. Two important derived objects are the voxelized geometric maps and a ground map. The voxelized geometric map is produced by segmenting the point cloud into voxels that are as small as 5cm x 5cm x 5cm. During real-time operation, the geometric map is the most efficient way to access point cloud information. It offers a good trade-off between accuracy and speed. Segmentation algorithms identify 3D points in the point cloud for building a model of the ground, defined as the driveable surface part of the map. These ground points are used to build a parametric model of the ground in small sections. The ground map is key for aligning the subsequent layers of the map, such as the semantic map.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*oAfCfvFoCvEw8A0g" /><figcaption>Semantic map</figcaption></figure><h4><strong>Semantic Map Layer</strong></h4><p>The semantic map layer builds on the geometric map layer by adding semantic objects. Semantic objects include various traffic 2D and 3D objects such as lane boundaries, intersections, crosswalks, parking spots, stop signs, traffic lights, etc. that are used for driving safely. These objects contain rich metadata associated with them such as speed limits and turn restrictions for lanes. While the 3D point cloud might contain all of the pixels and voxels that represent a traffic light, it is in the semantic map layer that a clean 3D object identifying the 3D location and bounding box for the traffic light and its various components are stored. We use a combination of heuristics, computer vision, and point classification algorithms to generate hypotheses for these semantic objects and their metadata. The output of these algorithms isn’t accurate enough for us to produce a high fidelity map. Human operators post-process these hypotheses via rich visualization and annotation tools to both validate the quality and fix any misses. For example, to identify traffic lights, we first run a traffic light detector on the camera images. Visual SLAM is used to process multiple camera images to get a coarse location of the traffic light in 3D. Lidar points in the local neighborhood of this location are matched and processed to produce the bounding box and orientation of the traffic light and its sub-components. We also employ heuristics for solving simpler problems. One area where we’ve found heuristics to be useful is in the generation of lane hypotheses, yield relationships, and connectivity graphs at intersections. There is a lot of structure in how these are setup for roads, especially since there are local laws that ensure consistency. Feedback from the human curation and quality assurance steps is used to keep these up to date.</p><p>The geometric and semantic map layers provide information about the static and physical parts of the world that are important to the self-driving vehicle. They are built at a very high fidelity and there is very little ambiguity about what the ground truth is. At Level 5, we view the map as a component that not only captures our understanding of the physical and static parts of the world, but also dynamic and behavioral aspects of the environment. The map priors layer and real-time knowledge layer represent this information. Information in these layers is computed not only from logs from the AV fleet, but also from the Lyft ridesharing network comprising millions of Lyft drivers. This scale is necessary to achieve high coverage of the map priors and ensure freshness of the real-time information.</p><h4><strong>Map priors layer</strong></h4><p>The map priors layer contains derived information about dynamic elements and also human driving behavior. Information here can pertain to both semantic and geometric parts of the map. For example, derived information such as the order in which traffic lights at an intersection cycle through their various states e.g. (red, protected-left, green, yellow, red) or (red, green, protected-left, yellow, red) and the amount of time spent in each state are encoded in the map priors layer. Time and day of week dimensions are used as keys to support multiple settings. These priors are approximate and serve as hints to the onboard autonomy systems. Another example is parking priors in the map. These parking priors are used by the prediction and planning systems to determine object velocities and make appropriate decisions. Parking priors are represented as polygonal regions on the lanes with metadata that capture the probability of encountering a parked vehicle at that location in the lane. When the AV encounters a stationary vehicle in a map region with high parking prior, then it will more aggressively explore plans that route the AV around the vehicle and demote plans that queue up the AV behind the vehicle. Similarly, knowing where people normally park allows perception systems to be more cautious to car doors opening and detected pedestrians as they might be getting in and out of cars. Unlike information in the geometric and semantic layers of the map, the information in the map priors layer is designed to be approximate and act as hints. Autonomy algorithms commonly consume these priors in models as inputs or features and combined with other real-time information.</p><h4><strong>Real-time knowledge layer</strong></h4><p>The real-time layer is the top most layer in the map and is designed to be read/write capable. This is the only layer in the map designed to be updated while the map is in use by the AV serving a ride. It contains real-time traffic information such as observed speeds, congestion, newly discovered construction zones, etc. The real-time layer is designed to support gathering and sharing of real-time global information between a whole fleet of AVs.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/984/0*z3_x_mzMiUVe4Ajt" /><figcaption>Real-time knowledge</figcaption></figure><p>Each of the above map layers is built independently. Derived layers may rely on intermediate outputs from previous layers. For example, the semantic layer uses the ground map generated by the geometric layer to identify z-positions of the lane polygons. In the final step, alignment algorithms are used to stitch together all layers of the map before it is released as one consistent component to the self-driving vehicle.</p><p>At Level 5, we are excited about building HD maps that make self-driving cars a reality. If these HD maps topics interest you and you are passionate about building maps and autonomy components for self-driving cars, check out our <a href="https://www.lyft.com/careers">careers page</a>. We’re hiring!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a147c24758d6" width="1" height="1"> <p><b><a href="https://medium.com/@LyftLevel5/https-medium-com-lyftlevel5-rethinking-maps-for-self-driving-a147c24758d6?source=rss-d6f431a02b8c------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3270</wp:post_id>
		<wp:post_date><![CDATA[2018-10-15 19:32:43]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-15 19:32:43]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[rethinking-maps-for-self-driving]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@LyftLevel5/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@LyftLevel5/https-medium-com-lyftlevel5-rethinking-maps-for-self-driving-a147c24758d6?source=rss-d6f431a02b8c------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Samsung expands Exynos and ISOCELL brands to Include Automotive-Grade solutions</title>
		<link>https://fifthlevel.ai/archives/3524</link>
		<pubDate>Tue, 16 Oct 2018 16:04:17 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14375</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Samsung Electronics Co., Ltd., announced two new automotive solution brands, Samsung Exynos Auto and Samsung ISOCELL Auto, to bring cutting-edge application processor and image sensor technology solutions to the road.</p> <p>“Samsung&#8217;s new automotive brand solutions, Exynos Auto and ISOCELL Auto, bring Samsung&#8217;s market-proven technologies to automotive applications with enhanced features and durability required by the market,” said Kenny Han, vice president of Samsung’s Device Solutions Division. “With fast telecommunication, accurate sensing and powerful processing capabilities, Samsung’s Auto-branded solutions will enable new driving experiences to next-generation smart vehicles.”</p> <p>Samsung’s Exynos processors have been recognised for their performance and stability since its brand launched in 2011. On top of the powerful yet efficient qualities of Exynos’ mobile versions, Exynos Auto meets industry requirements for harsher environments, allowing automotive manufacturers to develop cutting-edge applications such as infotainment, advanced driving assistance systems (ADAS) and telematics.</p> <p>Processors provided by Exynos Auto are sorted into three sub-categories: Exynos Auto V series for advanced in-vehicle infotainment (IVI) systems; Exynos Auto A series for ADAS; and Exynos Auto T series for telematics solutions.</p> <p>Samsung’s ISOCELL image sensors have brought innovative imaging technologies to mobile products since 2017 and the new automotive lineup will help bring the power of sight to cars. Built on the company’s innovative pixel isolation technology, ISOCELL, the sensors provide greater visibility of the road and surroundings even in low-light environments, while enabling more precise identification of objects. This allows, for example, vehicles to perceive road conditions or potential hazards even when driving through tunnels or other high-contrast environments.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/samsung-expands-exynos-isocell-brands-include-automotive-grade-solutions/">Samsung expands Exynos and ISOCELL brands to Include Automotive-Grade solutions</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/samsung-expands-exynos-isocell-brands-include-automotive-grade-solutions/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3524</wp:post_id>
		<wp:post_date><![CDATA[2018-10-16 16:04:17]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-16 16:04:17]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[samsung-expands-exynos-and-isocell-brands-to-include-automotive-grade-solutions]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/samsung-expands-exynos-isocell-brands-include-automotive-grade-solutions/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Startups Pack GTC Israel as Smallest of Them All Sweeps Top Inception Award</title>
		<link>https://fifthlevel.ai/archives/3225</link>
		<pubDate>Thu, 18 Oct 2018 17:34:50 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=40935</guid>
		<description></description>
		<content:encoded><![CDATA[<p>NVIDIA’s third <a href="https://www.nvidia.com/en-us/gtc/">GPU Technology Conference</a> in just over a month spilled across Tel Aviv’s convention center this week, in a packed show featuring a live demo of an AI-infused apple-picking drone, a student-built autonomous Formula One car and a two-person company that ran away with the title of Israel’s hottest startup.</p>
<p>The second annual <a href="https://www.nvidia.com/en-il/gtc/">GTC Israel</a> show drew 2,000 attendees, up 75 percent from last year, on the back of recent sellout crowds in at GTCs in <a href="https://www.nvidia.com/ja-jp/gtc/">Tokyo</a> and <a href="https://www.nvidia.com/en-eu/gtc/">Munich</a>. It was wall to wall with the companies that have won Israel the moniker of “startup nation.” Indeed, there are more than 4,000 tech startups in a country of 8.5 million, or 40x the density of startups in the U.S.</p>
<p>None flew higher at the show than <a href="https://www.nvidia.com/en-eu/gtc/agenda/ai-startups/">Inception award</a> winner <a href="http://www.thewhollysee.com/">TheWhollySee</a><b>, </b>a winkingly named shop with just two full-time employees that creates high-fidelity image datasets for training and certifying the AI brains of autonomous vehicles.</p>
<p>Its founder, Dan Yanson, said the first thing he’d do with the prize — $100,000 in cash plus an <a href="https://blogs.nvidia.com/blog/2018/10/18/israel-startups-nvidia-dgx-station/">NVIDIA DGX Station</a> personal AI supercomputer — is to bring his two part-timers fully on board.</p>
<p>“My first reaction? I’m just overwhelmed,” said Yanson, who studied in Sweden and Russia before completing his Ph.D. from the University of Glasgow. “We’re really a baby company — it’s a small team, we haven’t raised a lot of money yet and the competition was extremely strong. The prize money is a great boost, but it’s the DGX Station that will be a springboard to accelerate us, both in terms of our technology and our business.”</p>
<p>Yanson competed against seven other startups — in fields that included healthcare, agriculture, retail and esports — in a back-to-back series of five-minute presentations and then Q&amp;A with a four-person panel. He briskly described the company’s ability to infuse imagery into the foreground of scenes to more rapidly train neural networks for self-driving cars</p>
<p>The Inception awards, which drew a crowd of more than 300 sitting largely nightclub style in a soaring black-walled space, capped off a day that had started with a blistering keynote about NVIDIA’s mission to accelerate computing by NVIDIA Chief Scientist Bill Dally.</p>
<p>Dally announced that NVIDIA has just named a long-time Google Brain researcher, Gal Chechik, to the newly created position of Israel Research Head. Chechik’s mission: to build a world-class team focused on research into deep learning for smarter perception — including combining vision with language and knowledge, learning to generalize more broadly, and understanding complex data.</p>
<p>Along with some 450 individuals who received training from the <a href="https://www.nvidia.com/en-us/deep-learning-ai/education/">Deep Learning Institute</a> and 50+ talks by AI experts, the show included a teeming exhibition hall.</p>
<p>Among its hottest draws was a large netted structure where Tevel Aerobotics showed off its autonomous drone which can gingerly pick, thin and prune fruit trees, relieving the labor crunch in the agriculture sector, while helping farmers’ margins.</p>
<p>And a team of undergrads from Israel’s top-ranked Technion University showed off their side project developing an AI-powered mini-Formula One car, which they’re in the process of converting from gas-powered to all electric.</p>
<p>Other finalists in the Inception awards included:</p>
<p><a href="https://www.blink.gg/"><b>Blink</b></a><b> (esports) </b>— Aiming at the rapidly growing esports market, the company focuses on what it estimates as 600 million gamers and gaming enthusiasts who want to share their favorite moments online. Its platform focuses on the social side of esports by automatically detecting great gaming moments, saving them and making them easily shareable online.</p>
<p><a href="https://ibex-ai.com/"><b>IBEX Medical Analytics</b></a><b> (Healthcare) </b>— This two-year-old startup applies AI and big data to support pathologists in diagnosing types of cancer. Its work is focused on developing products that improve clinical decision making, streamline laboratory workflows and enable predictive, personalized cancer treatments.</p>
<p><a href="https://www.jungo.com/"><b>Jungo Connectivity</b></a><b> (Automotive)</b> — This Cisco spinoff offers in-car AI software focused on no-driver monitoring and cabin sensing, enabling vehicles to make better decisions and protect their driver and passengers. Its CoDriver SDK provides deep learning, machine learning and computer vision algorithms to OEMs and tier-1 suppliers, enabling them to create next-gen driver and occupant monitoring systems.</p>
<p><a href="https://tevel-tech.com/"><b>Tevel Aerobotics</b></a><b> (Agriculture) </b>— This two-year-old startup addresses the shortage of labor in the agricultural sector, by developing a fleet of autonomous airborne drones for picking, thinning and pruning fruit trees, enhancing productivity and saving costs. It promotes its solution as cost effective, flexible, easy to operate and enabling fruit trees to grow higher, thus maximizing yields and improving farmers’ margins.</p>
<p><a href="http://www.tracxpoint.com/"><b>TRACXPOiNT</b></a><b> (Retail) </b>— Aiming to bring the convenience of online shopping to retail, the company has created an AI-infused, self-checkout shopping cart. Using visual detection with deep learning capabilities, its AIC cart recognizes customers, transfers their shopping list to its monitor, automatically finds and recognizes products, while offering coupons, and provides automatic checkout.</p>
<p><a href="http://www.voiceitt.com/"><b>Voiceitt</b></a><b> (Healthcare) </b>— Its proprietary technology enables individuals with non-standard speech — such as stroke victims or those with muscle-related disabilities — to have their vocal expressions translated into clear speech, in real time. Its technology can be integrated with smart assistants, such as Alexa, to provide new levels of independence for those otherwise unable to carry out many simple actions.</p>
<p><a href="http://waycaretech.com/"><b>WayCare</b></a><b> (Smart Cities) </b>— In a world of ever-worsening traffic conditions, the company is focusing on providing municipalities with the ability to harness in-vehicle information and traffic data to optimize roadways. It uses CCTVs, traffic accident information, telematics, traffic detectors, weather forecasts and other data sources to extract, compile and analyze data for improving traffic flows.</p>
<p>The post <a rel="nofollow" href="https://blogs.nvidia.com/blog/2018/10/18/gtc-israel-startups-inception-award-winner/">Startups Pack GTC Israel as Smallest of Them All Sweeps Top Inception Award</a> appeared first on <a rel="nofollow" href="https://blogs.nvidia.com">The Official NVIDIA Blog</a>.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/nvidiablog?a=UQwM0n4ZQVE:BkyiD9dKkGQ:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/nvidiablog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/nvidiablog?a=UQwM0n4ZQVE:BkyiD9dKkGQ:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/nvidiablog?i=UQwM0n4ZQVE:BkyiD9dKkGQ:V_sGLiPBpWU" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/nvidiablog/~4/UQwM0n4ZQVE" height="1" width="1" alt=""/> <p><b><a href="http://feedproxy.google.com/~r/nvidiablog/~3/UQwM0n4ZQVE/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3225</wp:post_id>
		<wp:post_date><![CDATA[2018-10-18 17:34:50]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-18 17:34:50]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[startups-pack-gtc-israel-as-smallest-of-them-all-sweeps-top-inception-award]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feeds.feedburner.com/nvidiablog]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feedproxy.google.com/~r/nvidiablog/~3/UQwM0n4ZQVE/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Metawave names Karen C. Francis to Board of Directors</title>
		<link>https://fifthlevel.ai/archives/3227</link>
		<pubDate>Thu, 18 Oct 2018 18:40:10 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14404</guid>
		<description></description>
		<content:encoded><![CDATA[Metawave Corporation announced the appointment of Karen C. Francis to its board of directors. Bringing years of experience to both automotive and technology industries, Francis joins Denso’s Tony Cannestra, Metawave’s Maha Achourand Conny Marx, and Motus Ventures’ Jim Disanto.

“I am proud to be a part of Metawave’s board as the company solves big needs in two rapidly growing industries – autonomous driving and ubiquitous wireless communications,” said Francis. “The need for a more intelligent radar as a critical sensor for self driving cars is paramount to the growth in that market. And, ensuring network efficiencies and ubiquity around the world for faster wireless communications is clearly needed as the world moves to 5G. Metawave is poised to lead in bringing needed, intelligent technology platforms to market.”

Francis brings decades of experience across public and private companies. Most recently, she served as chairman and CEO of AcademixDirect, a Silicon Valley-based education technology firm, and chairman and CEO of Publicis and Hal Riney. She has also held senior leadership positions at Ford Motor Company as vice president and president/CEO of ConsumerConnect, Ford’s corporate venture capital group and at General Motors as the general manager of Oldsmobile where she was responsible for sales, marketing, strategy, and product development.

She currently serves on a number of boards including Telenav, a connected car embedded navigation company, and Nauto, an AI-powered autonomous vehicle technology company. Francis is a well-known thought leader on the future of autonomy, innovation, and talent development.

She has been named a National Association of Corporate Directors Board Leadership Fellow, holds a Bachelor of Arts degree in economics from Dartmouth College, and holds a Master of Business Administration from Harvard Business School.

“As a key member of our board, Karen will help us heighten our presence in both autonomous driving and communications by bringing together key partners, customers and investors as we continue to build our product and grow our business quickly,” said Maha Achour, Metawave co-founder and CEO.

The post <a href="http://www.newmobility.global/autonomous/metawave-names-karen-c-francis-board-directors/" rel="nofollow">Metawave names Karen C. Francis to Board of Directors</a> appeared first on <a href="http://www.newmobility.global" rel="nofollow">New Mobility</a>.

<b><a href="http://www.newmobility.global/autonomous/metawave-names-karen-c-francis-board-directors/" target="_blank" rel="noopener">Read the original article</a></b>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3227</wp:post_id>
		<wp:post_date><![CDATA[2018-10-18 18:40:10]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-18 18:40:10]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[metawave-names-karen-c-francis-to-board-of-directors]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/metawave-names-karen-c-francis-board-directors/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_advads_ad_settings]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{s:11:"disable_ads";i:0;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[company]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_company]]></wp:meta_key>
			<wp:meta_value><![CDATA[field_5bbaffdedc218]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Considering autonomy on the ground and in the air</title>
		<link>https://fifthlevel.ai/archives/3229</link>
		<pubDate>Thu, 18 Oct 2018 12:55:19 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://360.here.com/considering-autonomy-on-the-ground-and-in-the-air</guid>
		<description></description>
		<content:encoded><![CDATA[<div class="hs-featured-image-wrapper"> <a href="https://360.here.com/considering-autonomy-on-the-ground-and-in-the-air" title="" class="hs-featured-image-link"> <img src="https://360.here.com/hubfs/HEREblog_tech2_hero1.jpg?t=1539867112294" alt="HEREblog_tech2_hero1" class="hs-featured-image" style="width:auto !important; max-width:50%; float:left; margin:0 15px 15px 0;"> </a> </div> <h3>Be it self-driving cars, or self-flying drones, autonomous innovation is here to stay in the market. Smart companies are keeping pace with the regulations that oversee them.</h3> <p>If you attended any of the <span><a href="/techcrunch-2018-coverage-a-new-age-of-convenience-is-upon-us">tech conferences</a></span> this summer, one could not turn the corner without running into the topic of autonomy. But hot on the heels of any conversation regarding self-driving, self-flying, and self-optimizing systems was the issue of security and safety.</p> <img src="https://track.hubspot.com/__ptq.gif?a=2174253&amp;k=14&amp;r=https%3A%2F%2F360.here.com%2Fconsidering-autonomy-on-the-ground-and-in-the-air&amp;bu=https%253A%252F%252F360.here.com&amp;bvt=rss" alt="" width="1" height="1" style="min-height:1px!important;width:1px!important;border-width:0!important;margin-top:0!important;margin-bottom:0!important;margin-right:0!important;margin-left:0!important;padding-top:0!important;padding-bottom:0!important;padding-right:0!important;padding-left:0!important; "> <p><b><a href="https://360.here.com/considering-autonomy-on-the-ground-and-in-the-air" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3229</wp:post_id>
		<wp:post_date><![CDATA[2018-10-18 12:55:19]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-18 12:55:19]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[considering-autonomy-on-the-ground-and-in-the-air]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://360.here.com/rss.xml]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://360.here.com/considering-autonomy-on-the-ground-and-in-the-air]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Momenta receives a new round of funding at a valuation over $1 billion</title>
		<link>https://fifthlevel.ai/archives/3276</link>
		<pubDate>Fri, 19 Oct 2018 08:40:32 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14408</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Momenta, a <span class="xn-location">China</span>-based autonomous driving company, announced that it has secured a new round of funding from industry-leading strategic investors and government funds at a valuation north of <span class="xn-money">$1 billion</span>, making Momenta the first autonomous driving unicorn company in China.</p> <p>Existing investor NIO Capital and Pagoda Investment increased their investments. Strategic investors, including <span class="xn-money">Tencent</span>, government funds from <span class="xn-location">Shanghai</span> and Suzhou, China Merchants Group, and CCB International participated in this round.</p> <p>Earlier, Momenta announced the close of a <span class="xn-money">$46</span> million round of B-1 funding led by NIO Capital, a B-2 round led by Cathay Capital, an A round led by Shunwei Capital, as well as angel investment from Blue Lake Capital, Sinovation Ventures and Zhen Fund. Since its inception, Momenta has attracted investment totaling more than <span class="xn-money">$200 million</span>.</p> <p>&#8220;This round of funding has strategic importance to the company<span id="spanHghlt9a0b">,</span>&#8221; Momenta CEO <span class="xn-person">Xudong Cao</span> said, &#8220;Momenta will continue to work closely with its strategic partners in automotive, logistics, big data and other related areas.&#8221;</p> <p>Founded in <span class="xn-chron">September 2016</span>, Momenta is building intelligent systems for autonomous driving, aiming to provide solutions for Tier 1 suppliers and OEMs. Cao said, &#8220;In the past two years, Momenta has undergone three phases of development: Phase 1 &#8211; Establish big data and computing platforms; Phase 2 &#8211; Develop deep-learning algorithms as foundation for Perception, HD Semantic Mapping, and Planning &amp; Control; and Phase 3 &#8211; Develop multi-level autonomous driving solutions for mass production for highway and urban scenarios.&#8221;</p> <p>This year, Momenta also formed strategic partnership with the government of Suzhou, where the company will deploy a large-scale test fleet to accelerate its Level 4 autonomous driving development and support the government in building smart transportation systems. Momenta has expanded its business to international OEMs and Tier 1 suppliers.</p> <p>Momenta has world-class deep learning experts, including the authors of the most advanced image recognition frameworks, Faster R-CNN and ResNet, and winners of ImageNet 2015, ImageNet 2017, MS COCO Challenge and many other competitions. The team has grown significantly within the past two years with 80% of the Momenta team being researchers and developers.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/momenta-receives-new-round-funding-valuation-1-billion/">Momenta receives a new round of funding at a valuation over $1 billion</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/momenta-receives-new-round-funding-valuation-1-billion/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3276</wp:post_id>
		<wp:post_date><![CDATA[2018-10-19 08:40:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-19 08:40:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[momenta-receives-a-new-round-of-funding-at-a-valuation-over-1-billion]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/momenta-receives-new-round-funding-valuation-1-billion/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Western Digital launches new 3D NAND UFS Embedded Flash Drive</title>
		<link>https://fifthlevel.ai/archives/3277</link>
		<pubDate>Fri, 19 Oct 2018 08:15:35 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14414</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Addressing the demand of advanced automotive systems such as Advanced Driver-Assistance Systems (ADAS) and autonomous vehicles, Western Digital Corp. introduced the first 3D TLC NAND automotive embedded UFS flash drive (EFD), expanding its portfolio of high-quality, high-endurance automotive storage solutions.</p> <p>The Western Digital iNAND AT EU312 EFD is based on the established UFS 2.1 interface and delivers high capacities and up to 2.5 times the performance of the company’s previous e.MMC-based products, while maintaining the rigorous quality and reliability levels required for automotive-grade devices.</p> <p>Connected vehicles require fast and reliable data storage in increasingly higher capacities to support the vast amounts of data being generated, analysed and accessed by digital cluster, infotainment, 3D map and navigation, telematics, ADAS applications, augmented reality and other advanced automotive systems. The Western Digital iNAND AT EU312 EFD is designed to operate in V2X environments, both in vehicle and infrastructure systems, that are constantly generating and streaming data for real-time and offline data analytics.</p> <p>“As modern vehicles add more data-rich capabilities, such as machine vision, 3D mapping, multi-camera and multi-sensor-based systems and AI-driven databases, the need for bigger, faster, more reliable data storage has skyrocketed,” said Oded Sagee, senior director, product marketing, Western Digital. “Leveraging more than 16 years of automotive design experience, including advanced 3D TLC NAND technology built with Western Digital’s own NAND flash controllers, firmware and assembly and test, the new Western Digital iNAND AT EU312 EFD provides automotive OEMs and tier-one manufacturers the scalability in storage capacity, performance and reliability that will be the foundation of future mobility experiences.”</p> <p><b>Western Digital iNAND AT EU312 UFS EFD Features:</b></p> <ul>
<li><span class="bwuline">High Capacity</span>: Available in capacities ranging from 16GB to 256GB based on 3D NAND technology</li>
<li><span class="bwuline">Performance</span>: Delivers write speeds up to 550 MB/s and read speeds up to 800 MB/s</li>
<li><span class="bwuline">Automotive Quality and Reliability</span>: Features advanced memory management firmware and hardware, including robust error correction code (ECC), and is compliant with the JEDEC47, ISO26262, and AEC-Q100 Grade 3 and Grade 2 standards</li>
<li>Expanded Smart Features for OEMs: Western Digital provides a suite of automotive smart features, including enhanced power failure protection, a comprehensive memory health status monitor, enhanced SLC LUN, and OEM configurable boot partitioning</li>
</ul> <p>The Western Digital iNAND AT EU312 UFS EFD is the latest addition to Western Digital’s suite of automotive-grade storage solutions optimised to address the significant volume and criticality of data generated at the edge by connected automotive applications, including its iNAND e.MMC EFDs and Automotive SD cards. Western Digital is currently sampling storage solutions to OEMs in capacities up to 256GB.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/western-digital-launches-new-3d-nand-ufs-embedded-flash-drive/">Western Digital launches new 3D NAND UFS Embedded Flash Drive</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/western-digital-launches-new-3d-nand-ufs-embedded-flash-drive/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3277</wp:post_id>
		<wp:post_date><![CDATA[2018-10-19 08:15:35]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-19 08:15:35]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[western-digital-launches-new-3d-nand-ufs-embedded-flash-drive]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/western-digital-launches-new-3d-nand-ufs-embedded-flash-drive/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Drive.ai’s self-driving car service opens to all in Arlington, Texas</title>
		<link>https://fifthlevel.ai/archives/3313</link>
		<pubDate>Fri, 19 Oct 2018 12:00:21 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2406956</guid>
		<description></description>
		<content:encoded><![CDATA[<img width="578" height="385" src="https://venturebeat.com/wp-content/uploads/2018/10/Arlington_3.jpg?fit=578%2C385&amp;strip=all" class="attachment-single-feed size-single-feed wp-post-image" alt="Drive.ai Arlington" /><hr />Drive.ai is broadening deployment of its self-driving car service to every resident, employee, and visitor in Arlington, Texas.<a href="https://venturebeat.com/2018/10/19/drive-ais-self-driving-car-service-opens-to-all-in-arlington-texas/" target="_blank">Read More</a> <p><b><a href="https://venturebeat.com/2018/10/19/drive-ais-self-driving-car-service-opens-to-all-in-arlington-texas/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3313</wp:post_id>
		<wp:post_date><![CDATA[2018-10-19 12:00:21]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-19 12:00:21]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[drive-ais-self-driving-car-service-opens-to-all-in-arlington-texas]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/10/19/drive-ais-self-driving-car-service-opens-to-all-in-arlington-texas/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Telsla drops ‘full self-driving capability’ option for new vehicles</title>
		<link>https://fifthlevel.ai/archives/3356</link>
		<pubDate>Fri, 19 Oct 2018 15:25:06 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2407048</guid>
		<description></description>
		<content:encoded><![CDATA[<img width="578" height="325" src="https://venturebeat.com/wp-content/uploads/2018/06/35341487661_8168521a50_k.jpg?fit=578%2C325&amp;strip=all" class="attachment-single-feed size-single-feed wp-post-image" alt="Tesla Model X" /><hr />Tesla has removed the controversial 'Full Self-Driving Capability' option for new customers. It appears to be a temporary move.<a href="https://venturebeat.com/2018/10/19/telsla-drops-full-self-driving-capability-option-for-new-vehicles/" target="_blank">Read More</a> <p><b><a href="https://venturebeat.com/2018/10/19/telsla-drops-full-self-driving-capability-option-for-new-vehicles/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3356</wp:post_id>
		<wp:post_date><![CDATA[2018-10-19 15:25:06]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-19 15:25:06]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[telsla-drops-full-self-driving-capability-option-for-new-vehicles]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/10/19/telsla-drops-full-self-driving-capability-option-for-new-vehicles/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_advads_ad_settings]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{s:11:"disable_ads";i:0;}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>ZF acquires stake in engineering services provider ASAP</title>
		<link>https://fifthlevel.ai/archives/3357</link>
		<pubDate>Fri, 19 Oct 2018 10:14:17 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14420</guid>
		<description></description>
		<content:encoded><![CDATA[<p>ZF Friedrichshafen AG has taken over 35% of the shares of the engineering services provider ASAP Holding GmbH, located in Gaimersheim, Bavaria (near Ingolstadt).</p> <p>With this move, the company is pursuing its strategy of securing additional resources for autonomous driving and e-mobility in order to better meet the ever growing demands from customers in these areas.</p> <p>“As an established development partner, ASAP Group has extensive expertise in the areas of autonomous driving, e-mobility, connected cars and vehicle software,” says Torsten Gollewski, general manager of Zukunft Ventures GmbH and head of ZF’s Advanced Development department. “This participation allows us access to human resources from an outstanding engineering services provider with extensive industry experience and special expertise in testing and validation. We look forward to working with the team at ASAP in order to develop forward-looking mobility solutions for series production.”</p> <p>Michael Neisen, chairman of the Executive Board at ASAP, said: “Autonomous driving and e-mobility are the dominant trends in the automotive industry which we focused on intensively in recent years. With ZF Friedrichshafen AG, one of the world’s largest automotive suppliers, we have a partner at our side with whom we will further advance these future technologies.”</p> <p>By establishing strategic partnerships and equity stakes under the umbrella of Zukunft Ventures GmbH, ZF has significantly expanded its development capabilities in the area of autonomous driving in recent years.</p> <p>The now agreed share is also in line with ZF’s strategy of significantly strengthening the company’s electromobility and autonomous driving expertise in the areas of software and validation.</p> <p>ZF’s Chief Executive Officer Wolf-Henning Scheider recently announced at the IAA Commercial Vehicles trade show that ZF will be investing some €12 billion ($13.92 billion) in these two areas alone over the next five years.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/zf-acquires-stake-engineering-services-provider-asap/">ZF acquires stake in engineering services provider ASAP</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/zf-acquires-stake-engineering-services-provider-asap/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3357</wp:post_id>
		<wp:post_date><![CDATA[2018-10-19 10:14:17]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-19 10:14:17]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[zf-acquires-stake-in-engineering-services-provider-asap]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/zf-acquires-stake-engineering-services-provider-asap/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Volkswagen launches connected services in India</title>
		<link>https://fifthlevel.ai/archives/3358</link>
		<pubDate>Fri, 19 Oct 2018 11:39:36 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14423</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Accelerating digital transformation across the world, Volkswagen launched Volkswagen Connect, an intelligent connected vehicle assistance system followed by a brand new Lapiz Blue body color across its popular carlines Polo, Ameo and Vento.</p> <p>Reiterating its ‘value-for-money’ proposition, Volkswagen also introduced dynamic new features across its popular carlines which further enhances style, safety and customer convenience.</p> <p>Speaking on the occasion, Mr. Steffen Knapp, Director, Volkswagen Passenger Cars said, “Volkswagen is transforming into a mobility provider with the aim to offer fully connected vehicle fleets. In India, with Volkswagen Connect and the recently launched ‘Digital Workplace’ experience we aim to completely digitalise the customer experience right from pre to post-purchase experiences. With Volkswagen Connect, customers are empowered to manage a host of services such as driving behavior analysis, trip tracking, fuel cost monitoring, and other convenient features available on their fingertips.”</p> <p>Volkswagen Connect is an intelligent, connected vehicle assistant that offers a seamless connected car experience to Indian drivers. The interactive ‘Connect’ app enables users to connect their car to their smart phone through a “Plug and Play” data dongle fitted to the on-board diagnostics (OBD) port of the car. Once installed and connected with a smartphone via Bluetooth, customers can experience advanced connected features such as trip tracking, fuel cost monitoring, driving behavior and much more.</p> <p>The Connect app is compatible with both Android and iOS platforms and is available to download at the Google Play and App Store respectively.</p> <p>The key highlights of the Connect App are:</p> <p>1.       <strong>Trip tracking</strong>: Users can track every trip made by the car when the phone is connected to the car</p>
<p>2.       <strong>Fuel cost monitor:</strong> Users can keep a tab on their monthly or trip wise fuel costs</p>
<p>3.       <strong>Driving behavior</strong>: The driving behavior monitor enables users to monitor their driving styles. Individual scores are given for acceleration, braking, engine speed, engine temperature etc. for every trip which enables users to improve their driving efficiency by altering their driving style</p>
<p>4.       <strong>Location sharing:</strong> This feature enables users to share their last parking locations with their loved ones even after moving away from that location</p>
<p>5.       <strong>SOS call:</strong> Customers now need not memorise or store the customer care or RSA number. The app can directly connect a call to the helplines from the user’s phone</p>
<p>6.      <strong>Service appointment:</strong> The app can automatically inform the dealership when the customers car is due for service. With this intimation, the dealer workshop can contact the customer proactively and offer an appointment at a convenient time.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/connected-car/volkswagen-launches-connected-services-india/">Volkswagen launches connected services in India</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/connected-car/volkswagen-launches-connected-services-india/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3358</wp:post_id>
		<wp:post_date><![CDATA[2018-10-19 11:39:36]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-19 11:39:36]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[volkswagen-launches-connected-services-in-india]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/connected-car/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/connected-car/volkswagen-launches-connected-services-india/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Ford to use Wind River’s OTA update technology</title>
		<link>https://fifthlevel.ai/archives/3359</link>
		<pubDate>Fri, 19 Oct 2018 10:07:25 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14417</guid>
		<description></description>
		<content:encoded><![CDATA[<p class="p-xs">Wind River, a company delivering IoT software for safety-critical domains including automotive, announced that Ford Motor Company will be using Wind River over-the-air (OTA) update technology.</p>
<p class="p-xs">
<p class="p-xs">To deliver advanced connected car capabilities, Wind River Edge Sync technology can provide differential updates that allow the ability to minimise data update size, transmission time, and memory usage for updating vehicle software over the air. By creating the most minimal differential update possible during OTA updates, this approach offers the potential for cost reductions as well as a better experience for consumers.</p>
<p class="p-xs">
<p class="p-xs">With OTA updates, carmakers can rapidly deploy secure, cost-efficient updates and new features to connected vehicles, eliminating software recalls and the associated costs.</p>
<p class="p-xs">
<p class="p-xs">“Complexity is dramatically increasing as new capabilities are expected from connected, highly intelligent cars, and software is the key to enabling this transformation within the industry,” said Marques McCammon, Vice President of Automotive at Wind River. “Automakers must keep pace with innovation while also ensuring that they deliver safe vehicles. Therefore, the ability to continuously and remotely update, resolve issues and introduce new features for improved customer satisfaction will be more critical than ever. By delivering new innovations to companies like Ford, we’re helping to improve efficiencies and experiences for carmakers and their customers.”</p>
<p class="p-xs">
<p class="p-xs">Edge Sync is a software framework for remote OTA updates and software lifecycle management that allows for rapid, safe, and secure updates to software and firmware throughout the vehicle lifecycle, from initial product development through to the end of life of the vehicle. Edge Sync enables rapid, cost-effective solutions for software-related updates, corrections to security vulnerabilities, and feature-related recalls, as well as helps automakers introduce new and exciting value-add features to vehicles that are already in the hands of customers.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/connected-car/ford-use-wind-rivers-ota-update-technology/">Ford to use Wind River&#8217;s OTA update technology</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/connected-car/ford-use-wind-rivers-ota-update-technology/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3359</wp:post_id>
		<wp:post_date><![CDATA[2018-10-19 10:07:25]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-19 10:07:25]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[ford-to-use-wind-rivers-ota-update-technology]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/connected-car/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/connected-car/ford-use-wind-rivers-ota-update-technology/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>What’s the Best Deep Learning Framework?</title>
		<link>https://fifthlevel.ai/archives/3360</link>
		<pubDate>Fri, 19 Oct 2018 15:00:34 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=40904</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Think of a deep learning framework as a grocery store.</p>
<p>Rather than laboring in individual backyard farms, most people shop at markets when they want to whip up a meal.</p>
<p>Just as they don’t pick lettuce and uproot carrots when they have a hankering for salad, developers don’t want to start from scratch every time they build a <a href="https://developer.nvidia.com/discover/artificial-neural-network" target="_blank" rel="noopener">deep learning neural network</a>.</p>
<p>Deep learning models are large and complex, so instead of writing out every function from the ground up, programmers rely on <a href="https://developer.nvidia.com/deep-learning-frameworks" target="_blank" rel="noopener">frameworks</a> and software libraries to build neural networks efficiently. The top deep learning frameworks provide highly optimized, GPU-enabled code that are specific to deep neural network computations.</p>
<p>Different grocers specialize in unique sets of inventory. You might cover a basic recipe at your local supermarket. But you might prefer Whole Foods for that certain kind of mushroom. Or a farmer’s market for organic greens. Or a big-box store when cooking for a crowd.</p>
<p>Similarly, though a developer can build most types of networks (like <a href="https://blogs.nvidia.com/blog/2018/09/05/whats-the-difference-between-a-cnn-and-an-rnn/" target="_blank" rel="noopener">CNNs or RNNs</a>) in any deep learning framework, each framework is best-suited to a different set of neural network architectures.</p>
<p>Developers can access frameworks through the command line, script interfaces in programming languages like Python or C/C++, and user interfaces like <a href="https://developer.nvidia.com/digits" target="_blank" rel="noopener">NVIDIA DIGITS</a>. This unlocks the power of GPU speedups without developers needing to write complex code.</p>
<p><a href="https://ngc.nvidia.com/" target="_blank" rel="noopener">NVIDIA GPU Cloud</a> provides instant access to all of the frameworks mentioned below, and many more, delivering optimal GPU-accelerated performance on demand.</p>
<div style="width: 50%; padding: 0 10px 0 0; float: left;">
<h2><b>Ranked by GitHub </b><a href="https://www.kdnuggets.com/2018/04/top-16-open-source-deep-learning-libraries.html" target="_blank" rel="nofollow noopener"><b>popularity</b></a><b>: </b></h2>
<ol>
<li style="font-weight: 400;"><a href="https://blogs.nvidia.com/blog/2018/10/19/best-deep-learning-frameworks/#1">TensorFlow</a></li>
<li style="font-weight: 400;"><a href="https://blogs.nvidia.com/blog/2018/10/19/best-deep-learning-frameworks/#2">Keras</a></li>
<li style="font-weight: 400;"><a href="https://blogs.nvidia.com/blog/2018/10/19/best-deep-learning-frameworks/#3">Caffe</a></li>
<li style="font-weight: 400;"><a href="https://blogs.nvidia.com/blog/2018/10/19/best-deep-learning-frameworks/#4">Microsoft Cognitive Toolkit</a></li>
<li><a href="https://blogs.nvidia.com/blog/2018/10/19/best-deep-learning-frameworks/#5">PyTorch</a></li>
</ol>
</div>
<div style="width: 50%; padding: 0 10px 0 0; float: right;">
<h2><b>Ranked by beginner-friendliness: </b></h2>
<ol>
<li style="font-weight: 400;"><a href="https://blogs.nvidia.com/blog/2018/10/19/best-deep-learning-frameworks/#2">Keras</a></li>
<li style="font-weight: 400;"><a href="https://blogs.nvidia.com/blog/2018/10/19/best-deep-learning-frameworks/#5">PyTorch</a></li>
<li style="font-weight: 400;"><a href="https://blogs.nvidia.com/blog/2018/10/19/best-deep-learning-frameworks/#1">TensorFlow</a></li>
<li style="font-weight: 400;"><a href="https://blogs.nvidia.com/blog/2018/10/19/best-deep-learning-frameworks/#3">Caffe</a></li>
<li style="font-weight: 400;"><a href="https://blogs.nvidia.com/blog/2018/10/19/best-deep-learning-frameworks/#4">Microsoft Cognitive Toolkit</a></li>
</ol>
</div>
<figure id="attachment_40910" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/10-dl-frameworks-672x378.png"><img class="size-full wp-image-40910" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/10-dl-frameworks-672x378.png" alt="deep learning frameworks" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/10-dl-frameworks-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/10-dl-frameworks-672x378-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/10-dl-frameworks-672x378-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2018/10/10-dl-frameworks-672x378-178x100.png 178w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption class="wp-caption-text">NVIDIA GPU Cloud gives developers instant access to these and other top deep learning frameworks.</figcaption></figure>
<p><a name="1"></a></p>
<h2><strong><a href="https://www.tensorflow.org/" target="blank" rel="nofollow noopener">TensorFlow</a></strong></h2>
<p><i>The most popular deep learning framework</i></p>
<p><b>Popularity ranking? </b>1st<br />
<b>Beginner friendly ranking? </b>3rd<br />
<b>Programming language? </b>C++, Python</p>
<p><b>Best for? </b>TensorFlow is the world’s most widely used deep learning framework. Because of its popularity, the framework has extensive documentation and wide community support for developers. It works primarily with static graphs, making it good for training fixed-sized neural networks like <a href="https://blogs.nvidia.com/blog/2018/09/05/whats-the-difference-between-a-cnn-and-an-rnn/" target="_blank" rel="noopener">CNNs</a>. It has strong performance on multi-GPU applications and is one of the strongest frameworks for production due to its scalability.<br />
<a name="2"></a></p>
<p><b>One example of what you can do with TensorFlow: </b><a href="https://blogs.nvidia.com/blog/2018/06/06/deep-learning-ultrasound-cancer-diagnosis/" target="_blank" rel="noopener">AI and deep learning for cancer diagnosis</a></p>
<h2><strong><a href="https://keras.io/" target="_blank" rel="nofollow noopener">Keras</a></strong></h2>
<p><i>The most beginner-friendly deep learning framework</i></p>
<p><b>Popularity ranking? </b>2nd<br />
<b>Beginner friendly ranking? </b>1st<br />
<b>Programming language? </b>Python</p>
<p><b>Best for? </b>Keras is an interface that can run on top of multiple frameworks like TensorFlow, Theano and Microsoft Cognitive Toolkit. It uses a high-level Python API, making it a go-to source for beginners looking to build their first deep learning models. Keras uses object-oriented design, resulting in a cleaner interface. However, since it’s a general purpose interface, its performance levels are lower than other frameworks.<br />
<a name="3"></a></p>
<p><b>One example of what you can do with Keras: </b><a href="https://blogs.nvidia.com/blog/2017/10/24/how-ai-could-help-people-dodge-monster-storms/" target="_blank" rel="noopener">How AI could help people dodge hurricanes</a></p>
<h2><strong><a href="http://caffe.berkeleyvision.org/" target="_blank" rel="nofollow noopener">Caffe</a></strong></h2>
<p><i>The no-coding-required deep learning framework</i></p>
<p><b>Popularity ranking? </b>3rd<br />
<b>Beginner friendly ranking? </b>4th<br />
<b>Programming language? </b>C++, Python, MATLAB</p>
<p><b>Best for? </b>Caffe is a deep learning framework that works well for the production and deployment of neural networks. Created at UC Berkeley, it’s good for feed-forward neural networks like CNNs, but it doesn’t provide as much support for large or recurrent networks. Caffe is speedy for learning and inference, and it can process <a href="https://www.nvidia.com/en-gb/data-center/gpu-accelerated-applications/caffe/" target="_blank" rel="noopener">more than 1200 images per second</a> on a cluster of four Tesla P100 GPUs. Developers often can simply define their network architecture and training parameters, then call up the scripts they need in the command line without having to write any code.</p>
<p><b>Spinoffs: </b><a href="https://github.com/NVIDIA/caffe" target="_blank" rel="noopener">NVCaffe</a> is an NVIDIA-maintained fork of Caffe, specially tuned for multi-GPU use. And <a href="https://ngc.nvidia.com/registry/nvidia-caffe2" target="_blank" rel="noopener">Caffe2</a>, developed by Facebook, is a spinoff framework that can express both convolutional and recurrent neural networks and supports GPUs.<br />
<a name="4"></a></p>
<p><b>One example of what you can do with Caffe: </b><a href="https://blogs.nvidia.com/blog/2017/10/30/detecting-lung-cancer/ " target="_blank" rel="noopener">How AI could spot lung cancer sooner</a></p>
<h2><strong><a href="https://docs.microsoft.com/en-us/cognitive-toolkit/" target="_blank" rel="nofollow noopener">Microsoft Cognitive Toolkit</a></strong></h2>
<p><i>The speedy deep learning framework</i></p>
<p><b>Popularity ranking? </b>4th<br />
<b>Beginner friendly ranking? </b>5th<br />
<b>Programming language? </b>Python, C++</p>
<p><b>Best for? </b>Originally called CNTK, this deep learning framework from Microsoft powers its own AI models, like Cortana. Since it was originally built for speech recognition, this framework works particularly well on RNNs, but also supports other kinds of neural networks. Microsoft Cognitive Toolkit is faster than other frameworks for training AI models, an attractive feature for developers.<br />
<a name="5"></a><br />
<b>One example of what you can do with Microsoft Cognitive Toolkit: </b><a href="https://blogs.nvidia.com/blog/2015/12/10/microsoft-gpus-image-recognition/" target="_blank" rel="noopener">Microsoft uses GPUs to build record-breaking image recognition system</a></p>
<h2><strong><a href="https://pytorch.org/" target="_blank" rel="nofollow noopener">PyTorch</a></strong></h2>
<p><i>The researcher’s deep learning framework</i></p>
<p><b>Popularity ranking? </b>5th<br />
<b>Beginner friendly ranking? </b>2nd<br />
<b>Programming language? </b>Python</p>
<p><b>Best for? </b>First there was Torch, a popular deep learning framework based on the programming language Lua. Then, Facebook introduced PyTorch, which takes the features developers liked from Torch and implements them in Python. This framework allows users to take advantage of Python features like loops, and it makes debugging easy. PyTorch is an especially attractive option for researchers because it’s well-suited to dynamic graph networks like <a href="https://blogs.nvidia.com/blog/2018/09/05/whats-the-difference-between-a-cnn-and-an-rnn/" target="_blank" rel="noopener">RNNs</a>, where the size of the input and output data is variable. It’s often used to build natural language processing models.</p>
<p><b>One example of what you can do with PyTorch: </b><a href="https://blogs.nvidia.com/blog/2018/06/29/ai-driven-3d-cell-model-magic/" target="_blank" rel="noopener">AI-driven 3D cell modeling</a></p>
<h2><strong>Mixing and Matching Top Frameworks</strong></h2>
<p>Depending on the application at hand, developers may want to build and train a deep learning model using one framework, then deploy it for inference using a different one. The <a href="https://onnx.ai/" target="_blank" rel="noopener">Open Neural Network Exchange</a>, known as ONNX, is a format for deep learning models that allows developers to move their models between frameworks. ONNX currently supports Caffe2, Microsoft Cognitive Toolkit, PyTorch and other frameworks.</p>
<p>Those looking for a user-friendly interface to build a deep neural network can explore DIGITS, a web application that allows developers to work with the Caffe, Torch or TensorFlow frameworks without writing any code. Users need only to provide the datasets and define the network architecture.</p>
<p>For additional resources and install information for deep learning frameworks, explore the <a href="https://developer.nvidia.com/deep-learning-frameworks" target="_blank" rel="noopener">NVIDIA Developer site</a>.</p>
<p>The post <a rel="nofollow" href="https://blogs.nvidia.com/blog/2018/10/19/best-deep-learning-frameworks/">What’s the Best Deep Learning Framework?</a> appeared first on <a rel="nofollow" href="https://blogs.nvidia.com">The Official NVIDIA Blog</a>.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/nvidiablog?a=cUZyxM7s6ms:Kpl16TIQP_Y:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/nvidiablog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/nvidiablog?a=cUZyxM7s6ms:Kpl16TIQP_Y:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/nvidiablog?i=cUZyxM7s6ms:Kpl16TIQP_Y:V_sGLiPBpWU" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/nvidiablog/~4/cUZyxM7s6ms" height="1" width="1" alt=""/> <p><b><a href="http://feedproxy.google.com/~r/nvidiablog/~3/cUZyxM7s6ms/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3360</wp:post_id>
		<wp:post_date><![CDATA[2018-10-19 15:00:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-19 15:00:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[whats-the-best-deep-learning-framework]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feeds.feedburner.com/nvidiablog]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feedproxy.google.com/~r/nvidiablog/~3/cUZyxM7s6ms/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Drive.ai’s On-Demand Ride-Hailing Service in Arlington, TX is Ready To Drive You</title>
		<link>https://fifthlevel.ai/archives/3370</link>
		<pubDate>Fri, 19 Oct 2018 13:25:47 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/274f4af74f80</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>A Look at Our Second Self-Driving Service on Public Roads, and What’s to Come</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ePG4utEbSMmJjmyXiJqSuw.jpeg" /></figure><p><strong>All aboard! We’re live in Arlington</strong></p><p>Less than two months ago, we announced our second self-driving program on public roads: an on-demand ride-hailing service in the City of Arlington, Texas. Today, that program is live. Drive.ai is deploying a fleet of self-driving vehicles in Arlington’s downtown district, available for use by the city’s 400,000+ residents as well as visitors to the area.</p><p>Our goal — as a company and with this program in Arlington — is to use self-driving technology to create truly impactful mobility solutions. We have been able to scale quickly due to our advanced deep-learning based technology, our people-centric approach, and by working closely with local governments and partners to identify and solve for real transit needs within their communities. This program in Arlington is our second self-driving service to go live in four months, following a successful launch in Frisco, TX in July.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*c7hfnXfazowm8_LEpeYrXw.jpeg" /></figure><p><strong>Tackling transportation issues one ride at a time</strong></p><p>Arlington is currently the 50th most populous city in the country, and growing rapidly. Home to the Dallas Cowboys and Texas Rangers, Arlington also welcomes about 14 million visitors each year and brings 100,000 people through Arlington’s Entertainment District on Cowboys and Rangers game days. The City of Arlington chose Drive.ai to solve their unique mobility needs that entail providing on-demand transportation within a specific area.</p><p>Our self-driving vehicles will run on-demand between fixed pick-up and drop-off locations. Rides can be hailed by using one of the kiosks located throughout the service area, or by downloading the Drive.ai app. We’re operating along multiple routes to service football games, baseball games, and conventions, as well as Arlington’s Entertainment District and nearby offices, restaurants and amenities. Now people can use our self-driving service to catch a game downtown or go grab lunch with colleagues — at no cost.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bSiXL1VsfpZVmFR0J_im-A.jpeg" /></figure><p><strong>Building off our blueprint</strong></p><p>At Drive.ai, we believe three key elements will be the foundation of advancing self-driving programs and scaling them in new cities and use-cases. First, we leverage deep-learning technology which allows us to scale quickly. We can adapt to a range of geographies and vehicles, meaning we’re nimble and can rapidly ramp up with new partners. Second, we have a people-centric approach which prioritizes a safe, smooth experience for riders, pedestrians, and other drivers. An example of this can be seen in our innovative <a href="https://www.wired.com/story/driveai-self-driving-design-frisco-texas/">external communication panels</a>. Finally, we are committed to working closely with regional and local governments to deploy self-driving programs that are safe and scalable.</p><p>With this blueprint for impactful transportation services, we will continue expanding and implementing self-driving programs that are tailored to the transportation needs of different communities and environments.</p><p>Looking ahead, we foresee the demand for convenient transit solutions proliferating substantially, both across rapidly growing urban centers, and areas that cannot be served by traditional public transit options. We remain convinced that self-driving vehicles will be a key part of city infrastructure, and that, by increasing mobility, Drive.ai can make significant contributions to benefit these connected communities.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DbYFK9WDV9yv41v6nQjM7w.jpeg" /></figure><p><strong>So, what now?</strong></p><p>Drive.ai is rapidly growing. We will continue to focus on these three core elements while seeking out new cities and partners with which we can deploy additional self-driving programs. We are committed to bringing tech-forward mobility solutions to more people. We started in Frisco, TX, and now we’re excited to begin service in Arlington, TX — and we will continue to launch new programs. We remain committed to building smarter, safer self-driving technology and working closely with partners to change the way the world moves, for the better.</p><p>If you’re interested in bringing self-driving vehicles to your community, let us know. Reach out to us at partnerships@drive.ai, and follow us at <a href="https://twitter.com/driveai_?lang=en">@driveai_</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=274f4af74f80" width="1" height="1"> ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3370</wp:post_id>
		<wp:post_date><![CDATA[2018-10-19 13:25:47]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-19 13:25:47]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[drive-ais-on-demand-ride-hailing-service-in-arlington-tx-is-ready-to-drive-you]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@drive.ai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@drive.ai/drive-ais-on-demand-ride-hailing-service-in-arlington-tx-is-ready-to-drive-you-274f4af74f80?source=rss-37851f177ff8------2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple snaps up massive manufacturing space in Milpitas, California</title>
		<link>https://fifthlevel.ai/archives/3420</link>
		<pubDate>Sat, 20 Oct 2018 22:37:59 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/10/20/apple-snaps-up-massive-manufacturing-space-in-milpitas-california</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/28170-43348-milpitas-space-l.jpg" alt="Article Image" border="0" /> <br><br> Apple has reportedly secured a 10-year lease for a Milpitas warehouse measuring almost 314,000 square feet, raising questions about the company's intentions. <p><b><a href="https://appleinsider.com/articles/18/10/20/apple-snaps-up-massive-manufacturing-space-in-milpitas-california" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3420</wp:post_id>
		<wp:post_date><![CDATA[2018-10-20 22:37:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-20 22:37:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-snaps-up-massive-manufacturing-space-in-milpitas-california]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/10/20/apple-snaps-up-massive-manufacturing-space-in-milpitas-california]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Addison Lee aims to deploy self-driving cars in London by 2021</title>
		<link>https://fifthlevel.ai/archives/3526</link>
		<pubDate>Mon, 22 Oct 2018 05:00:23 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.theguardian.com/technology/2018/oct/22/self-driving-cars-london-addison-lee-oxbotica-huge-leap</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Tech pioneer Oxbotica to start mapping public roads as it calls deal with hire firm ‘huge leap’</p><p>Self-driving car services could be on the streets of London within three years under a partnership between the private hire firm Addison Lee and the British driverless car pioneers Oxbotica.</p><p>The companies have signed a deal to develop and deploy <a href="https://www.theguardian.com/technology/self-driving-cars">autonomous vehicles</a> in the city by 2021.</p> <a href="https://www.theguardian.com/technology/2018/oct/22/self-driving-cars-london-addison-lee-oxbotica-huge-leap">Continue reading...</a> <p><b><a href="https://www.theguardian.com/technology/2018/oct/22/self-driving-cars-london-addison-lee-oxbotica-huge-leap" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3526</wp:post_id>
		<wp:post_date><![CDATA[2018-10-22 05:00:23]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-22 05:00:23]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[addison-lee-aims-to-deploy-self-driving-cars-in-london-by-2021]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/self-driving-cars/rss]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/2018/oct/22/self-driving-cars-london-addison-lee-oxbotica-huge-leap]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Addison Lee Group, Oxbotica join forces in strategic alliance to make self-driving services a reality in London</title>
		<link>https://fifthlevel.ai/archives/3532</link>
		<pubDate>Mon, 22 Oct 2018 08:42:27 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14434</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Addison Lee Group, the global ground transportation business, and Oxbotica, the British leader in self-driving vehicle software, have agreed a wide-ranging strategic alliance that accelerates the implementation of autonomous vehicles to London’s streets.</p>
<p>Under the agreement, the two companies will collaborate on the development, deployment and operation of autonomous vehicles with a view to providing customers self-driving services in London by 2021.</p>
<p>The long-term aim is to take greater share of an expanding car services market for connected autonomous vehicle technology, forecasted to be worth £28 billion in the UK by 2035. Addison Lee Group aims to use its trusted brand to offer affordable, quality, ride-shared services to passengers currently underserved by existing driven transport modes, as well as explore opportunities to provide corporate shuttles, airport and campus-based services.</p>
<p>Addison Lee Group and Oxbotica, both British success stories, will pool expertise, technology and on-the-ground resource to explore self-driving car services that are safe and environmentally friendly, and which still deliver the exceptional customer experiences Addison Lee Group is known for.</p>
<p>The companies will work together to create detailed, digital maps of more than 250,000 miles of public roads in and around the capital. These maps will record the position of every kerb, road sign, landmark and traffic light in preparation for the deployment of autonomous cars.</p>
<p>Mobility Opportunity</p>
<p>With private car ownership declining at the same time city populations are expanding, consumers are increasing their use of car services. Added to that, the global transport services market is experiencing significant growth – specifically for the premium segment, which shows strong growth of up to 21 per cent by 2030.</p>
<p>This provides an opportunity for focused investment in future-looking technologies that, alongside traditional driven vehicles, will help meet this need. By leveraging the strengths of each partner, the alliance will open new opportunities to reach consumers in new markets and segments starting in London, following into New York and other international markets.</p>
<p>Graeme Smith, CEO of Oxbotica, said: “This represents a huge leap towards bringing autonomous vehicles into mainstream use on the streets of London, and eventually in cities across the United Kingdom and beyond.</p>
<p>“Our partnership with Addison Lee Group represents another milestone for the commercial deployment of our integrated autonomous vehicle and fleet management software systems in complex urban transport conditions. Together, we are taking a major step in delivering the future of mobility.”</p>
<p>Andy Boland, CEO of Addison Lee Group, said: “Urban transport will change beyond recognition in the next 10 years with the introduction of self-driving services, and we intend to be at the very forefront of this change by acting now.</p>
<p>“Autonomous technology holds the key to many of the challenges we face in transport. By providing ride-sharing services, we can help address congestion, free space used for parking and improve urban air quality through zero-emission vehicles. We are proud to be partnering with a British technology pioneer and leader in autonomous vehicle technology, Oxbotica, and together we will continue our British success story in how we revolutionise the way people get around cities.”</p>
<p>Addison Lee Group recently set up and led the MERGE Greenwich consortium, a government-funded project investigating how autonomous vehicle ride-sharing could be introduced to complement existing public transport services. Using the London Borough of Greenwich as a model, the project found that by 2025, self-driving, ride-shared services could assist significantly with addressing the capital’s transport challenges and make it easier and more accessible for citizens to move around. In parallel, Oxbotica is leading the DRIVEN consortium, and has already launched a fleet of vehicles currently running autonomously in public trials in London and Oxford.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/addison-lee-group-oxbotica-join-forces-strategic-alliance-make-self-driving-services-reality-london/">Addison Lee Group, Oxbotica join forces in strategic alliance to make self-driving services a reality in London</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/addison-lee-group-oxbotica-join-forces-strategic-alliance-make-self-driving-services-reality-london/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3532</wp:post_id>
		<wp:post_date><![CDATA[2018-10-22 08:42:27]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-22 08:42:27]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[addison-lee-group-oxbotica-join-forces-in-strategic-alliance-to-make-self-driving-services-a-reality-in-london]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/addison-lee-group-oxbotica-join-forces-strategic-alliance-make-self-driving-services-reality-london/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Tata Motors’ European Technical Centre shows off its latest Mobility Technology in Coventry</title>
		<link>https://fifthlevel.ai/archives/3548</link>
		<pubDate>Mon, 22 Oct 2018 14:49:35 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14440</guid>
		<description></description>
		<content:encoded><![CDATA[<p>A three-year project into the trials of connected and autonomous vehicle technology has drawn to a close, with the Coventry-based, Tata Motors European Technical Centre (TMETC) revealing its commitment to shaping the future of mobility.</p> <p>As the UK centre of excellence in design and engineering for the Tata Motors Passenger Vehicle business in India, TMETC plans to focus its learning and future developments on controlled road environments.</p> <p>TMETC participated in the UK Autodrive project, co-funded by Innovate UK, the UK’s innovation agency, which brought together 15 partners including OEMs, cutting edge engineering businesses, academia and progressive councils to explore the impact of ACES (Autonomous, Connected, Electric and Shared) technology, in a safe and controlled setting.</p> <p>As part of the project, TMETC has successfully completed test trials on the new generation SUV, the Tata Hexa. The HEXA lends spacious interior and provides ample room to accommodate the necessary driving hardware, visitors and engineers in comfort.</p> <p>According to Mr. Rajendra Petkar, Chief Technology Officer, Tata Motors, “At Engineering Research Centre (ERC) we have been actively undertaking R&amp;D work on advanced driver assistance systems (ADAS) as well as full vehicle autonomy in order to be future ready. Road congestion, air pollution and road safety, are acute concerns in India. We are likely to embrace connected, electric and shared technology sooner and therefore it is essential we remain at the forefront of these developments. Autonomy will be a consideration for the future in India.</p> <p>“As the UK has already published a code of practice for testing autonomous vehicles safely and legally, it provides the ideal platform to enable us to challenge our self-driving vehicle capabilities. With the support of our much talented team at TMETC, I am delighted to share that the trials done on the HEXA have yielded immensely rewarding insights for us. Going forward, we plan to introduce number of ADAS functionalities in a structured &amp; phased manner,” he continued.</p> <p>Mr. David Hudson, Head of Propulsion, TMETC, said: “This has been a challenging project but has enabled us to safely test our technologies on public roads in a real-world environment. We have to understand the challenging road usage pattern in India and apply that knowledge to this project. Our connectivity vehicle experience has enabled us to safely test and demonstrate numerous features including GLOSA (Green Light Optimal Speed Advisory) and EEBL (Electronic Emergency Brake Light). It has also demonstrated how vehicles and infrastructure will work in tandem for a motoring network in the future. We are pleased by the end of this project we have achieved an appropriate level of self-driving capability to allow us to move forward with our next steps in the mobility revolution.”</p> <p>Tata Motors European Technical Centre (TMETC) is a subsidiary of Tata Motors, created as a UK-based centre of excellence for automotive design and engineering located at the University of Warwick. TMETC provides research and development principally for Tata Motors.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/tata-motors-european-technical-centre-shows-off-latest-mobility-technology-coventry/">Tata Motors&#8217; European Technical Centre shows off its latest Mobility Technology in Coventry</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/tata-motors-european-technical-centre-shows-off-latest-mobility-technology-coventry/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3548</wp:post_id>
		<wp:post_date><![CDATA[2018-10-22 14:49:35]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-22 14:49:35]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[tata-motors-european-technical-centre-shows-off-its-latest-mobility-technology-in-coventry]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/tata-motors-european-technical-centre-shows-off-latest-mobility-technology-coventry/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Denso’s shift in strategy to craft core technologies for Future of Mobility</title>
		<link>https://fifthlevel.ai/archives/3549</link>
		<pubDate>Mon, 22 Oct 2018 13:15:26 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14437</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Denso corporation, the world&#8217;s second largest mobility supplier, has made recent changes that represent one of the largest shifts in business strategy in its 70-year history, expanding into software-based solutions in addition to its hardware expertise, that push the company beyond a vehicle-centric focus in order to help create an new mobility paradigm for society.</p> <p>The global auto industry is undergoing a once-in-a-century shift that will fundamentally reshape transportation. Given the changing cast of competitors that make up the evolving auto industry, Denso has transformed its conventional business model.</p> <p>Denso&#8217;s long-term policy, which was launched last year, guides the company toward its 2030 goal: to create and inspire new value for the future of mobility. Denso has now launched its 2018 Annual Report, which is the first account of company activities following the release of its long-term policy. The policy and report reiterates Denso&#8217;s strategy to achieve vision, including:</p> <ol>
<li>Increasing performance in the fields of electrification and automated driving;</li>
<li>Realigning organizational structure to accelerate business execution, R&amp;D, collaboration, and other efficiencies; and</li>
<li>Focusing on key initiatives both in and outside the auto industry to add value.</li>
</ol> <p>&#8220;Our long-term vision is a future with enhanced mobility, safety and peace of mind, with less impact on the environment. We still have a deep commitment to protecting lives, but we are ready to realise our second founding,&#8221; said CEO <span class="xn-person">Koji Arima</span>. &#8220;This means we must change our own organisation to prevail in the rapidly changing business landscape and provide value to our customers that goes beyond a vehicle-centric focus to enrich society&#8217;s broader needs.&#8221;</p> <p>Denso will continue to increase its performance in the fields of electrification and automated driving to achieve sustainable growth. The company aims to achieve revenues of JPY<span class="xn-money">7 trillion</span> (<span class="xn-money">$62 billion</span>) and an operating profit ratio of 10% by 2026.</p> <p>The $1 billion investment in its <span class="xn-location">Maryville, Tennessee</span>, location significantly advanced <span class="xn-location">the United States&#8217;</span> role in crafting the future of electrification and safety technology and made <span class="xn-location">Maryville</span> a primary manufacturing centre in <span class="xn-location">North America</span> for electrification and safety systems. The investment also expanded multiple production lines to produce advanced safety, connectivity, and electrification products for hybrid and electric vehicles.</p> <p>These new products will radically improve fuel efficiency and preserve electric power by recovering and recycling energy, and by connecting all systems and products inside the vehicles.</p> <p>Denso plans to focus on the areas of electrification, automated driving and connected cars to advance the future of mobility. It will also focus on non-automotive business, particularly on factory automation and agro-industrialisation. Recent investments and partnerships in these key areas include:</p> <ul>
<li><b>Electrification</b>: Aisin seiki and Denso have reached a basic agreement on the establishment of a joint venture company to develop electrified drive modules and are preparing for establishment.</li>
<li><b>Auto</b><b>mated</b><b> Driv</b><b>ing</b>: Aisin seiki, ADVICS, JTEKT and Denso have signed a basic agreement concerning the establishment of a joint venture for the development of integrated ECU software for automatic operation and are preparing for establishment. Additionally, Denso&#8217;s investment in ThinCI speeds the development of semiconductor devices with deep learning capabilities required for next-generation automated driving technology.</li>
<li><b>Connect</b><b>ed Cars</b><b>:</b> The investment in CREATIONLINE upgraded Denso&#8217;s system to develop cloud solutions and open source software. Denso&#8217;s collaboration with Ridecell advances the development of carsharing and ridesharing technologies and services.</li>
</ul>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/autonomous/densos-shift-strategy-craft-core-technologies-future-mobility/">Denso&#8217;s shift in strategy to craft core technologies for Future of Mobility</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/autonomous/densos-shift-strategy-craft-core-technologies-future-mobility/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3549</wp:post_id>
		<wp:post_date><![CDATA[2018-10-22 13:15:26]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-22 13:15:26]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[densos-shift-in-strategy-to-craft-core-technologies-for-future-of-mobility]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/densos-shift-strategy-craft-core-technologies-future-mobility/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>A Monumental Moment: Our Self-Driving Business Development Expands to Washington, D.C.</title>
		<link>https://fifthlevel.ai/archives/3571</link>
		<pubDate>Mon, 22 Oct 2018 18:01:02 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/cbce4320f38f</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>By Sherif Marakby, CEO, Ford Autonomous Vehicles LLC</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*kk-JYciEdhNMj41nSIu5fw.jpeg" /></figure><p>Just like guiding a bill through Congress, establishing a self-driving vehicle business requires a lot of coordination. Not only do self-driving vehicles need to operate safely and reliably, but they also need to work in concert with the businesses, people and cities they serve. On top of all this, they must operate within an ecosystem that supports their operation and maintenance.</p><p>As Ford’s work in all those areas continues in Detroit, Pittsburgh and Miami, we are expanding to become the first company to test autonomous vehicles in Washington, D.C., according to the district, by establishing a self-driving vehicle business — a business that will be responsive to the needs of the city and its residents.</p><p>Both Ford and district officials are committed to exploring how self-driving vehicles can be deployed in an equitable way across the various neighborhoods that make up Washington, D.C., and in a way that promotes job creation.</p><p>The advent of self-driving vehicles promises a chance to make it more affordable and easier for people to get to jobs by filling gaps in access to public transportation, new ways to deliver food and other products, and more. It also means change — and how we prepare for that change will greatly impact people and their communities.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AwNEkSPzEBQc2K0gSuxeKg.jpeg" /></figure><p>That’s why we’ll be working with local officials in Washington, D.C., to ensure that we test our self-driving vehicles in all eight of the district’s wards — and eventually operate business pilot programs in all eight wards as well — as we work toward deployment of a commercial service in 2021. We believe that ensuring widespread access to mobility services enabled by self-driving vehicles is vital, a sentiment that was underscored <a href="https://avworkforce.secureenergy.org/">in a report by Securing America’s Future Energy</a>, which found that autonomous technology could improve people’s access to jobs as well as retail markets.</p><p><a href="https://medium.com/@argoai">Our partners at Argo AI</a>, who are leading development of the self-driving system, already have vehicles on district streets, mapping roads in the first step toward testing in autonomous mode. Over the next year, the fleet will grow as we expand testing areas, including within the city’s downtown core.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*M9dC2MZhwwJM7sMJXusiNg.jpeg" /></figure><p>Meanwhile, we’re also collaborating with the D.C. Infrastructure Academy in Ward 8, a workforce training center launched by Mayor Muriel Bowser earlier this year that prepares residents for a number of jobs. We are looking to train a roster of vehicle operators, who will be responsible for safely operating and monitoring our test vehicles on public roads and on closed courses throughout the development process.</p><p>Additionally, we will work to train residents for auto technician careers that could involve self-driving vehicles in the future. This training will be through courses developed by Excel Automotive in Ward 7 and Ford’s Automotive Career Exploration program with support from local dealers Chesapeake Ford Truck, DARCARS and Sheehy Ford of Marlow Heights.</p><p>With new technology comes new opportunities, after all, and self-driving vehicles won’t be an exception. According to the same SAFE report, a strong workforce development infrastructure can smooth over employment disruptions and “speed the evolution of worker skill requirements that will contribute to full employment and economic growth.” Acting now in preparation for the future is necessary so that we aren’t caught flat-footed as autonomous technology gains mainstream adoption. Part of that means making sure all residents have the chance to learn new skills, and we are committed to helping identify new jobs for communities as self-driving vehicles take to the streets.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2UWrgpbLKSPuv6_8iX6kaA.jpeg" /></figure><p>As we move forward with these initiatives, we’re fortunate to be working with Mayor Bowser. She and her administration have been strong supporters of new mobility initiatives, with a track record of leadership on autonomous technology. Washington, D.C., part of the Bloomberg Aspen Initiative on Cities and Autonomous Vehicles, is the first city to conduct pilots with food delivery bots, and its long-term planning calls for the District to become a test bed for self-driving vehicles and connected technology. <br> <br>To manage our fleet on the ground, we have also established an autonomous vehicle operations terminal in Ward 5. This terminal is where we’ll continue developing our vehicle management process, house our fleet, wash sensors and clean vehicles, as well as conduct routine maintenance and troubleshoot any potential problems. All told, this is a great opportunity to see how we can take what we’ve learned in Miami-Dade County and begin building out operations in a new city.</p><p>Finally, expanding our operations into Washington, D.C., is an enormous opportunity to understand how a comprehensive self-driving business could be utilized. The city is one of the largest markets in the United States, with its population growing significantly during working hours as people commute from the suburbs or take the subway. Outside of commuters, there are millions of visitors every year, major conferences, a popular food scene, and high demand for ride-hailing and delivery services.</p><p>Aside from building a fleet that can handle such a diverse workload, self-driving vehicles will also need to operate within a solid regulatory framework. Earlier this year we shared how we are prioritizing safety, in “<a href="https://media.ford.com/content/dam/fordmedia/pdf/Ford_AV_LLC_FINAL_HR_2.pdf">A Matter of Trust: Ford’s Approach to Developing Self-Driving Vehicles</a>,” but now with our cars on the roads in D.C., it’s important that lawmakers see self-driving vehicles with their own eyes as <a href="https://medium.com/self-driven/securing-our-roads-for-self-driving-vehicles-9b8e68a4632b">we keep pushing for legislation</a> that governs their safe use across the country.</p><p><a href="https://medium.com/self-driven/securing-our-roads-for-self-driving-vehicles-9b8e68a4632b">Securing Our Roads for Self-Driving Vehicles</a></p><p>This technology holds the potential to change our world far beyond our nation’s capital, but D.C. is an ideal place to continue building the foundation for a great service — one that helps free up our roads, ease congestion and improve people’s access to the services they need.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cbce4320f38f" width="1" height="1"><hr><p><a href="https://medium.com/self-driven/a-monumental-moment-our-self-driving-business-development-expands-to-washington-d-c-cbce4320f38f">A Monumental Moment: Our Self-Driving Business Development Expands to Washington, D.C.</a> was originally published in <a href="https://medium.com/self-driven">Self-Driven</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> <p><a href="https://medium.com/self-driven/a-monumental-moment-our-self-driving-business-development-expands-to-washington-d-c-cbce4320f38f?source=rss----7192a82ca09a---4" target="_blank">Read the original article</a></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3571</wp:post_id>
		<wp:post_date><![CDATA[2018-10-22 18:01:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-22 18:01:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[a-monumental-moment-our-self-driving-business-development-expands-to-washington-d-c]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/self-driven/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/self-driven/a-monumental-moment-our-self-driving-business-development-expands-to-washington-d-c-cbce4320f38f?source=rss----7192a82ca09a---4]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Safety Is What Drives Us: Introducing the NVIDIA Self-Driving Safety Report</title>
		<link>https://fifthlevel.ai/archives/3630</link>
		<pubDate>Tue, 23 Oct 2018 07:00:36 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=40957</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Autonomous vehicles promise to reduce traffic accidents by replacing unpredictable human drivers with artificial intelligence. But how do manufacturers ensure these new drivers are truly safe?</p>
<p>To help answer that, NVIDIA has released the <a href="https://www.nvidia.com/content/dam/en-zz/Solutions/self-driving-cars/safety-report/auto-print-safety-report-pdf-v16.5%20(1).pdf">Self-Driving Safety Report</a>.</p>
<p>With this report, we are opening up our development processes. We show how we harness the unprecedented computing power of GPUs to create functionally safe self-driving systems. Achieving the highest levels of compute enables us to incorporate diversity and redundancy into every solution — from sensor types to processors to algorithms — ensuring there is never just one line of defense in the event of a failure.</p>
<p>As a solutions provider to the vast majority of vehicle makers, suppliers, sensor makers, startups and mapping companies in the autonomous driving space, NVIDIA makes safety our first priority. And we have integrated it into every step of the development process.</p>
<p>We believe safety is at the heart of the transition to autonomy. Our report details how compute performance translates to safety at all stages, from initial data collection to public road testing.</p>
<h2><b>The Four Pillars of Safe Autonomous Driving</b></h2>
<p>Safe autonomous driving is built on four fundamental pillars. With high-performance compute at their core, these tenants illustrate NVIDIA’s dedication to safety and ensure a robust self-driving technology development cycle.</p>
<p><i>Pillar 1: Artificially Intelligent Design and Implementation Platform </i></p>
<p>A safe AI driver requires a compute platform that spans the entire range of autonomous driving, from assisted highway driving to robotaxis. It must combine deep learning, sensor fusion and surround vision to enable the car to make split-second decisions based on massive amounts of data.</p>
<p><i>Pillar 2: Development Infrastructure That Supports Deep Learning </i></p>
<p>A single test vehicle can generate petabytes of data annually. Capturing, managing and processing this massive amount of data for not just one car, but a fleet, requires an entirely new computing architecture and infrastructure.</p>
<p><i>Pillar 3: Data Center Solution for Robust Simulation and Testing </i></p>
<p>The ability to test in a realistic simulation environment is essential to providing safe self-driving vehicles. By coupling actual road miles with simulated miles in a high-performance data center solution, manufacturers can comprehensively test and validate their technology.</p>
<p><i>Pillar 4: Best-in-Class, Pervasive Safety Program</i></p>
<p>Self-driving technology development must follow a pervasive safety methodology that emphasizes diversity and redundancy in the design, validation, verification and lifetime support of the entire autonomous system. These programs should follow recommendations from federal and international agencies such as the National Highway Traffic Safety Administration, International Organization for Standardization and the global New Car Assessment Program.</p>
<p>This safety program includes a comprehensive evaluation process before our safety drivers validate NVIDIA technology on public roads. Drivers and co-pilots must complete rigorous training before operating vehicles, which are continuously tested for hardware and software readiness before drives.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-22-at-5.36.32-PM.png"><img class="aligncenter size-large wp-image-40960" src="https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-22-at-5.36.32-PM-672x447.png" alt="" width="672" height="447" srcset="https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-22-at-5.36.32-PM-672x447.png 672w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-22-at-5.36.32-PM-400x266.png 400w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-22-at-5.36.32-PM-768x510.png 768w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-22-at-5.36.32-PM-677x450.png 677w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-22-at-5.36.32-PM-324x215.png 324w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-22-at-5.36.32-PM-150x100.png 150w, https://blogs.nvidia.com/wp-content/uploads/2018/10/Screen-Shot-2018-10-22-at-5.36.32-PM-1280x851.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<h2><b>Working Together</b></h2>
<p>In addition to these four pillars, significant research and development, as well as industry-wide collaboration, is essential to safely deploying autonomous vehicles.</p>
<p>NVIDIA continues to work with our wide and diverse ecosystem as well as regulators to share knowledge and formulate standards for this budding technology.</p>
<p>Read the full report <a href="https://www.nvidia.com/content/dam/en-zz/Solutions/self-driving-cars/safety-report/auto-print-safety-report-pdf-v16.5%20(1).pdf">here</a>.</p>
<p>The post <a rel="nofollow" href="https://blogs.nvidia.com/blog/2018/10/23/introducing-self-driving-safety-report/">Safety Is What Drives Us: Introducing the NVIDIA Self-Driving Safety Report</a> appeared first on <a rel="nofollow" href="https://blogs.nvidia.com">The Official NVIDIA Blog</a>.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/nvidiablog?a=9hlQDpPhkSs:qyMy3U1OqiM:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/nvidiablog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/nvidiablog?a=9hlQDpPhkSs:qyMy3U1OqiM:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/nvidiablog?i=9hlQDpPhkSs:qyMy3U1OqiM:V_sGLiPBpWU" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/nvidiablog/~4/9hlQDpPhkSs" height="1" width="1" alt=""/> <p><b><a href="http://feedproxy.google.com/~r/nvidiablog/~3/9hlQDpPhkSs/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3630</wp:post_id>
		<wp:post_date><![CDATA[2018-10-23 07:00:36]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-23 07:00:36]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[safety-is-what-drives-us-introducing-the-nvidia-self-driving-safety-report]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feeds.feedburner.com/nvidiablog]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feedproxy.google.com/~r/nvidiablog/~3/9hlQDpPhkSs/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Keysight Technologies delivers C-V2X and advanced Automotive Ethernet solutions</title>
		<link>https://fifthlevel.ai/archives/3637</link>
		<pubDate>Tue, 23 Oct 2018 09:07:42 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14455</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Keysight Technologies, Inc., a technology company that helps enterprises, service providers, and governments accelerate innovation in connected solutions, announced a new cellular vehicle-to-everything (C-V2X) and advanced automotive Ethernet solution to address the evolving industry standards and ensure interoperability among components.</p> <p>Vehicle-to-everything communication refers to a car&#8217;s communication system, in which information from sensors, and other sources, travels through high-bandwidth, low-latency, high-reliability links, paving the way to fully autonomous driving. C-V2X communicates to a cellular network for cloud-based services like navigation and infotainment and uses a direct link to connect vehicles to everything, including each other (V2V), to pedestrians (V2P), to infrastructure (V2I), and to the network (V2N).</p> <p>One of the biggest test challenges for C-V2X is keeping up with the latest standard which requires a test solution that is always up-to-date with the latest evolution of the C-V2X requirements, including future releases that involve 5G new radio (5G NR).</p> <p>Keysight&#8217;s C-V2X Toolset is the only solution that can track the evolving C-V2X standard in terms of radio frequency, protocol, and application-layer testing.  Its future proof 5G NR V2X platform based on 3rd Generation Partnership Project (3GPP), Release 16 specifications, protects investments and accelerates deployment of new technologies to enable advanced safety features.</p> <p>The Keysight C-V2X Toolset offers customers:</p> <ul>
<li>Compliance with the 3GPP, Release 14 specifications</li>
<li>An intuitive user interface to simplify C-V2X measurements (protocol and radio frequency)</li>
<li>A global navigation satellite system (GNSS) emulator to generate signals for realistic GNSS</li>
<li>A 5G NR measurement platform to protect initial investments and accelerate deployment of the new technologies that will enable advanced safety features</li>
<li>A foundation for C-V2X conformance test</li>
</ul> <p>&#8220;With autonomous driving on the horizon, advanced LTE and 5G-based wireless technologies are the key technological building-blocks needed to make it a reality,&#8221; said Siegfried Gross, Vice President and General Manager of Keysight&#8217;s Automotive and Energy Solutions business unit. &#8220;Keysight has leveraged the broad acceptance of our 5G capable wireless platform, and collaborations with chipset developers and early technology adopters, to deliver a comprehensive offering that enables C-V2X developers to overcome design and performance verification challenges in their race toward autonomous driving.&#8221;</p> <p>Keysight is fully invested in the creation of complete solutions for automotive Ethernet and offers solutions for compliance testing of receivers, harness and connectors, and transmitters.  Keysight&#8217;s new electronic control unit (ECU) test application covers all four of the standards that govern the automotive Ethernet platform:  BroadR-Reach, IEEE 100BASE-T1, IEEE1000BASE-T1, and the OPEN Alliance (One-Pair Ether-net) ECU specifications.  The new software includes coverage for the media dependent interface (MDI) mode, conversion loss, and common mode emission tests set by the OPEN Alliance automotive Ethernet TC8 ECU test specification.</p> <p>Keysight empowers automotive industry designers and manufacturers with the latest innovations in design and test solutions to help create high-quality and high-performance products while mitigating safety risks. For example, the new Keysight E8740A Automotive Radar Signal Analysis and Generation solution enables radar-based, advanced driver assistance systems to proactively detect and mitigate risks of collisions.</p> <p>Keysight uses this solution in its automotive customer care centres to generate various real-world conditions and address any potential automotive radar interference issues.</p> <p>Additionally, Keysight offers e-mobility solutions that deliver improved battery performance and electric drivetrains.  These solutions also ensure the availability of efficient charging stations, and enhanced power conversion across the entire e-mobility ecosystem to increase the range of hybrid and electric vehicles.</p>
<p>The post <a rel="nofollow" href="http://www.newmobility.global/connected-car/keysight-technologies-delivers-c-v2x-advanced-automotive-ethernet-solutions/">Keysight Technologies delivers C-V2X and advanced Automotive Ethernet solutions</a> appeared first on <a rel="nofollow" href="http://www.newmobility.global">New Mobility</a>.</p> <p><b><a href="http://www.newmobility.global/connected-car/keysight-technologies-delivers-c-v2x-advanced-automotive-ethernet-solutions/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3637</wp:post_id>
		<wp:post_date><![CDATA[2018-10-23 09:07:42]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-23 09:07:42]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[keysight-technologies-delivers-c-v2x-and-advanced-automotive-ethernet-solutions]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/connected-car/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/connected-car/keysight-technologies-delivers-c-v2x-advanced-automotive-ethernet-solutions/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>EyeSight raises $15 million for AI-powered in-car monitoring</title>
		<link>https://fifthlevel.ai/archives/3634</link>
		<pubDate>Tue, 23 Oct 2018 13:00:51 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2407906</guid>
		<description></description>
		<content:encoded><![CDATA[<img width="578" height="319" src="https://venturebeat.com/wp-content/uploads/2018/10/eyesight.png?fit=578%2C319&amp;strip=all" class="attachment-single-feed size-single-feed wp-post-image" alt="EyeSight" /><hr />EyeSight, a Tel Aviv startup specializing in computer vision systems, announced a $15 million round to bring its in-car monitoring tech to mass market.<a href="https://venturebeat.com/2018/10/23/eyesight-raises-15-million-for-ai-powered-in-car-monitoring/" target="_blank">Read More</a> <p><b><a href="https://venturebeat.com/2018/10/23/eyesight-raises-15-million-for-ai-powered-in-car-monitoring/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3634</wp:post_id>
		<wp:post_date><![CDATA[2018-10-23 13:00:51]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-23 13:00:51]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[eyesight-raises-15-million-for-ai-powered-in-car-monitoring]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/10/23/eyesight-raises-15-million-for-ai-powered-in-car-monitoring/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Ridecell granted California Autonomous Vehicle Operations Testing license</title>
		<link>https://fifthlevel.ai/archives/3636</link>
		<pubDate>Tue, 23 Oct 2018 13:55:26 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.newmobility.global/?p=14467</guid>
		<description></description>
		<content:encoded><![CDATA[Ridecell Inc., a platform for new mobility operators, announced that its self-driving vehicle division, Auro, is now licensed to test autonomous vehicles on public roadways in <span class="xn-location">California</span>.

“Receiving a <span class="xn-location">California</span> autonomous testing permit signals an important expansion of our new mobility product offering,” said Aarjav Trivedi, CEO of Ridecell. “Ridecell’s successful ridesharing and carsharing fleet management platform will now expand as our Auro division begins operations testing of self-driving passenger cars and minivans on public streets in cities across <span class="xn-location">California</span>.”

Ridecell began offering self-driving vehicle solutions with its acquisition of Auro in <span class="xn-chron">October 2017</span>. Auro first developed and operated driverless shuttles for private geo-fenced locations, such as corporate parks and university campuses. Auro is now expanding its autonomous product offering to include passenger vehicle models and mini vans, which will operate on public roads alongside existing vehicle traffic.

Auro’s L4 autonomous driving vehicles will initially target low speed urban use cases focused on solving last mile transportation. Designed to improve today’s carsharing operations, these use cases include late night carshare fleet rebalancing to high demand urban locations, as well as fleet movement for parking, cleaning and maintenance.

Combining 3D Lidars, cameras, navigation sensors and high accuracy 3D mapping with proprietary software algorithms, Auro’s autonomous vehicle technology is hardware and vehicle agnostic. The company’s real world trials will start on Ford Fusion vehicle platforms equipped with Auro’s Autonomous Driving System.

“This next step in our corporate development allows Ridecell to serve an even broader segment of the new mobility industry,” explained <span class="xn-person">Nalin Gupta</span>, Director of Business Development, Auro. “Today our broad product offering includes self-driving vehicle technologies and comprehensive cloud-based fleet operations management. Ridecell has become a single source for all the required components that enable our customers to offer either driver-operated or autonomous on-demand vehicle services to their users, all using a single user app and a single cloud-based fleet management platform.”

The post <a href="http://www.newmobility.global/autonomous/ridecell-granted-california-autonomous-vehicle-operations-testing-license/" rel="nofollow">Ridecell granted California Autonomous Vehicle Operations Testing license</a> appeared first on <a href="http://www.newmobility.global" rel="nofollow">New Mobility</a>.

<b><a href="http://www.newmobility.global/autonomous/ridecell-granted-california-autonomous-vehicle-operations-testing-license/" target="_blank" rel="noopener">Read the original article</a></b>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3636</wp:post_id>
		<wp:post_date><![CDATA[2018-10-23 13:55:26]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-23 13:55:26]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[ridecell-granted-california-autonomous-vehicle-operations-testing-license]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/category/autonomous/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://www.newmobility.global/autonomous/ridecell-granted-california-autonomous-vehicle-operations-testing-license/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_advads_ad_settings]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{s:11:"disable_ads";i:0;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[company]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_company]]></wp:meta_key>
			<wp:meta_value><![CDATA[field_5bbaffdedc218]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Intel: 43% of U.S. respondents don’t feel safe around self-driving cars</title>
		<link>https://fifthlevel.ai/archives/3643</link>
		<pubDate>Tue, 23 Oct 2018 16:30:17 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://venturebeat.com/?p=2408066</guid>
		<description></description>
		<content:encoded><![CDATA[<img width="578" height="356" src="https://venturebeat.com/wp-content/uploads/2017/01/intel-car-4.jpg?fit=578%2C356&amp;strip=all" class="attachment-single-feed size-single-feed wp-post-image" alt="" /><hr />A study commissioned by Intel and conducted by PSB Research found that almost half of Americans don't feel safe around self-driving cars.<a href="https://venturebeat.com/2018/10/23/intel-43-of-u-s-respondents-dont-feel-safe-around-self-driving-cars/" target="_blank">Read More</a> <p><b><a href="https://venturebeat.com/2018/10/23/intel-43-of-u-s-respondents-dont-feel-safe-around-self-driving-cars/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3643</wp:post_id>
		<wp:post_date><![CDATA[2018-10-23 16:30:17]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-23 16:30:17]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[intel-43-of-u-s-respondents-dont-feel-safe-around-self-driving-cars]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/category/ai/feed/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://venturebeat.com/2018/10/23/intel-43-of-u-s-respondents-dont-feel-safe-around-self-driving-cars/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Dr.Stallman on Self-driving taxis – A dystopian future?</title>
		<link>https://fifthlevel.ai/archives/3644</link>
		<pubDate>Tue, 23 Oct 2018 16:18:52 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">http://www.theguardian.com/technology/2018/oct/23/self-driving-taxis-a-dystopian-future</guid>
		<description></description>
		<content:encoded><![CDATA[Addison Lee’s future cars | Dishwashers | Waitrose, Lidl and Brexit | Gaby’s Deli<p>Would you like to ask Addison Lee whether the “self-driving” cars it aims to deploy in London by 2021 (<a href="https://www.theguardian.com/technology/2018/oct/22/self-driving-cars-london-addison-lee-oxbotica-huge-leap" title="">Report</a>, 22 October) will in fact have remote control capability? If so, the term “self-driving” would be a misnomer. The remote control would be dangerous for dissidents and whistleblowers – if you ride in a “self-driving” taxi with remote control capability, and the taxi knows who you are, the state could whisk you to a deportation prison, or a black site, at any time.<br><strong>Dr Richard Stallman</strong><br><em>President, Free Software Foundation</em></p><p>• The discussion about dishwashers (<a href="https://www.theguardian.com/environment/2018/oct/18/save-your-energy-use-a-dishwasher" title="">Letters</a>, 19 October) reminds me that I was once expressing a wish for one to a friend after lunch at our house. There was a sudden rattle of crockery and a voice was heard from the kitchen: “What d’you want one of them for when you’ve got me ?” He was also a good Hoover pusher.<br><strong>Hilda Hayden</strong><br><em>Malvern, Worcestershire</em></p> <a href="https://www.theguardian.com/technology/2018/oct/23/self-driving-taxis-a-dystopian-future">Continue reading...</a> <p><b><a href="https://www.theguardian.com/technology/2018/oct/23/self-driving-taxis-a-dystopian-future" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3644</wp:post_id>
		<wp:post_date><![CDATA[2018-10-23 16:18:52]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-23 16:18:52]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[self-driving-taxis-a-dystopian-future-brief-letters]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/self-driving-cars/rss]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://www.theguardian.com/technology/2018/oct/23/self-driving-taxis-a-dystopian-future]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Apple working on self-driving car &#039;peloton&#039; system to share power, increase efficiency</title>
		<link>https://fifthlevel.ai/archives/3647</link>
		<pubDate>Tue, 23 Oct 2018 16:02:07 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://appleinsider.com/articles/18/10/23/apple-working-on-self-driving-car-peloton-system-to-share-power-increase-efficiency</guid>
		<description></description>
		<content:encoded><![CDATA[ <img src="https://photos5.appleinsider.com/gallery/28209-43442-apple-self-driving-car-testbed-l.jpg" alt="Article Image" border="0" /> <br><br> Self-driving cars could potentially work together to minimize energy usage on long journeys, with Apple proposing the use of an autonomous peloton of vehicles that could reduce drag for low-performance vehicles, and even the possibility of cars sharing battery power with each other while on the road. <p><b><a href="https://appleinsider.com/articles/18/10/23/apple-working-on-self-driving-car-peloton-system-to-share-power-increase-efficiency" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3647</wp:post_id>
		<wp:post_date><![CDATA[2018-10-23 16:02:07]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-23 16:02:07]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[apple-working-on-self-driving-car-peloton-system-to-share-power-increase-efficiency]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/rss/topic/project+titan]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://appleinsider.com/articles/18/10/23/apple-working-on-self-driving-car-peloton-system-to-share-power-increase-efficiency]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>NVIDIA, Carnegie Mellon Team Up to Aid First Responders</title>
		<link>https://fifthlevel.ai/archives/3649</link>
		<pubDate>Tue, 23 Oct 2018 17:21:32 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=40975</guid>
		<description></description>
		<content:encoded><![CDATA[<p>When disaster strikes, the clock ticks faster. Civilians need to be evacuated. Supplies need to be moved. First responders need to be deployed. Delays can cost lives.</p>
<p>AI will soon help those on the ground keep up amidst the chaos, NVIDIA vice president for Accelerated Computing Ian Buck explained Tuesday, detailing a partnership between NVIDIA and Carnegie Mellon University.</p>
<p>“This is an area where I think we can make a huge impact saving lives,&#8221; Buck told an audience of more than 2,000 policymakers, technologists, and entrepreneurs gathered in Washington DC for GTC DC.</p>
<p>The goal of the humanitarian aid/disaster relief initiative is to create AI for the fast-paced scenarios where the right information at the right time can save lives, Buck explained.</p>
<p>The effort will bring together the best minds at CMU and NVIDIA with a “build it, break it, then make it better” design cycle to create tools for disaster response.</p>
<p>CMU brings the experience of over 200 faculty working on a broad spectrum of topics within the field of AI. NVIDIA brings thousands of engineers, an international reputation for R&amp;D, and the industry’s leading AI platform.</p>
<h2>Solving “Impossible” Problems</h2>
<p>These teams will bring world-class technical skills not only to solve so-called “impossible” problems, but to develop new methods for problem-solving, Buck explained.</p>
<p>Nothing’s off the table. The effort will begin with basic science, incorporate new and existing algorithms, and make use commercial off the shelf components to jointly deploy for next generation AI solutions.</p>
<p>The broad-based partnership will include work in five key areas:</p>
<ul>
<li style="font-weight: 400;"><b>Autonomy</b> &#8211; The effort will create autonomous, unmanned aerial, aquatic, and terrestrial systems able to operate in the unstructured spaces encountered during disaster scenarios.</li>
<li style="font-weight: 400;"><b>Human-AI Interaction &#8211;</b> The partnership will create new kinds of interfaces, such as virtual, language-based assistants for victims sheltering in place, and assistants able to aid decision makers trying to interpret large amounts of data in real time.</li>
<li style="font-weight: 400;"><b>Sensors, Hardware, Cloud, and Edge Compute &#8211;</b> Meshing sensors and computing power on devices in disaster zones with the cloud promises to help first responders move faster.</li>
<li style="font-weight: 400;"><b>Hardware</b><b> &#8211;</b> CMU will aim to develop an industry leading GPU-based capability to address the most challenging AI and accelerated computing related problems those working on humanitarian assistance and disaster recovery. <b></b></li>
<li style="font-weight: 400;"><b>Developer Training &#8211; </b>NVIDIA’s Deep Learning Institute will help train CMU’s team to apply AI to HA/DR.</li>
</ul>
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="https://blogs.nvidia.com/blog/2018/10/23/carnegie-mellon-first-responders/">NVIDIA, Carnegie Mellon Team Up to Aid First Responders</a> appeared first on <a rel="nofollow" href="https://blogs.nvidia.com">The Official NVIDIA Blog</a>.</p>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/nvidiablog?a=0J4OgGPJE2E:3O1hcO9bAms:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/nvidiablog?d=yIl2AUoC8zA" border="0"></img></a> <a href="http://feeds.feedburner.com/~ff/nvidiablog?a=0J4OgGPJE2E:3O1hcO9bAms:V_sGLiPBpWU"><img src="http://feeds.feedburner.com/~ff/nvidiablog?i=0J4OgGPJE2E:3O1hcO9bAms:V_sGLiPBpWU" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/nvidiablog/~4/0J4OgGPJE2E" height="1" width="1" alt=""/> <p><b><a href="http://feedproxy.google.com/~r/nvidiablog/~3/0J4OgGPJE2E/" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3649</wp:post_id>
		<wp:post_date><![CDATA[2018-10-23 17:21:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-23 17:21:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[nvidia-carnegie-mellon-team-up-to-aid-first-responders]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feeds.feedburner.com/nvidiablog]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[http://feedproxy.google.com/~r/nvidiablog/~3/0J4OgGPJE2E/]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Blue Vision Labs is joining Lyft!</title>
		<link>https://fifthlevel.ai/archives/3733</link>
		<pubDate>Tue, 23 Oct 2018 15:01:52 +0000</pubDate>
		<dc:creator><![CDATA[editor]]></dc:creator>
		<guid isPermaLink="false">https://medium.com/p/657daca89b71</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Joining Forces on Self-driving</p><p><strong>By Peter Ondruska, Co-founder and CEO of Blue Vision Labs</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/740/1*UF3K6srWDKrE3MELBxzRqA.gif" /></figure><p>Today, I am thrilled to announce that Blue Vision Labs is joining forces with Lyft in their mission to revolutionise transportation, via a full technology, team, and product acquisition of Blue Vision Labs. Blue Vision Labs will be the first Lyft office in London, and will become part of <a href="http://www.lyft.com/level5">Lyft’s Level 5</a> division to build technology for Lyft’s self-driving efforts.</p><h3>Building the future of Robotics and AI</h3><p>We started Blue Vision Labs with the mission to advance the capabilities of today’s robotics and AR platforms by solving some of the hardest technological challenges in the field.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*wpm0xPRfKqcJUunsRx21Zw.gif" /><figcaption>Our computer vision technology used to accurately localise and navigate a car.</figcaption></figure><p>In March of this year, we made great progress on this goal by releasing the <a href="https://medium.com/@bluevisionlabs/introducing-blue-vision-ar-cloud-5b9ef21364c9">Blue Vision Labs AR Cloud</a>, a technology that allows developers to create shared AR experiences that were not possible before at unprecedented scale. One potential application of our AR cloud allows us to connect ridesharing passengers with their drivers by overlaying the car’s position onto the rider’s smartphone screen in augmented reality. Joining forces with Lyft means we can develop experiences like this at a much greater scale.</p><p>The technology that we created for our AR cloud is powered using centimeter-accurate 3D maps that cover entire cities, and are built using fleets of vehicles equipped with mobile phones. We believe that gaining a detailed understanding of the world is the key requirement for achieving a safe and scalable self-driving platform. Specifically, this technology helps car know where they are, what is around them, and what should they do next at scale not possible with other technologies. We are very excited to be working with Lyft to contribute to their efforts in enabling the future of autonomous mobility.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*iU2yznuKx96dM1s2gXkGzQ.gif" /><figcaption>Our AR Cloud allows creating shared AR experiences at city-scale that were not possible before.</figcaption></figure><h3>The journey ahead</h3><p>As we begin this next chapter of our story, I would like to thank our investors, particularly Google Ventures, Accel, Horizons Ventures and angel investors for supporting us in our journey so far, and to the thousands of developers, we worked with through our AR beta program. We will be sharing more about the next steps for our beta developers through emails as we embark on this exciting journey.</p><p>We believe we are working on some of the hardest and most important problems in the industry. If this sounds like something you would like to be part of, you can <a href="https://bluevisionlabs.com/careers">join us</a> at Lyft Level 5!</p><p><em>— Peter Ondruska and the Blue Vision Labs team</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=657daca89b71" width="1" height="1"> <p><b><a href="https://medium.com/@bluevisionlabs/blue-vision-labs-is-joining-lyft-657daca89b71?source=rss-733841c73e97------2" target="_blank">Read the original article</a></b></p>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3733</wp:post_id>
		<wp:post_date><![CDATA[2018-10-23 15:01:52]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-23 15:01:52]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[blue-vision-labs-is-joining-lyft]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="autonomous-vehicles"><![CDATA[Autonomous Vehicles]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
			<wp:meta_value><![CDATA[@5thlevelai]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_rss_source]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/feed/@bluevisionlabs/]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[cyberseo_post_link]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://medium.com/@bluevisionlabs/blue-vision-labs-is-joining-lyft-657daca89b71?source=rss-733841c73e97------2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
</channel>
</rss>
